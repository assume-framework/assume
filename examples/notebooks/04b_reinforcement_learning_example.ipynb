{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02bad29e",
   "metadata": {},
   "source": [
    "# 4.2 Designing Adaptive Bidding Strategies in ASSUME using Reinforcement Learning\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "This tutorial introduces the integration of **reinforcement learning (RL)** into the **ASSUME** simulation framework, with a focus on developing and deploying learning-based bidding strategies for electricity market participants.\n",
    "\n",
    "The tutorial is designed to walk you through the essential components required to transform a conventional market participant into an RL agent. Rather than concentrating on the underlying algorithmic infrastructureâ€”such as training loops, buffers, or learning rolesâ€”this tutorial emphasizes **how to define a bidding strategy that interfaces with the ASSUME learning backend**. You will learn how to construct observation spaces, define action mappings, and design reward functions that guide agent behavior.\n",
    "\n",
    "Each core concept is addressed in a dedicated chapter, accompanied by exercises that allow you to apply the material directly. These hands-on tasks culminate in a final integration chapter where you will run a complete simulation and train your first learning agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b93e",
   "metadata": {},
   "source": [
    "**Tutorial Structure**\n",
    "\n",
    "The tutorial is divided into the following chapters:\n",
    "\n",
    "\n",
    "1. **Get ASSUME Running**\n",
    "\n",
    "   Instructions for installing ASSUME and preparing your environment, whether locally or in Google Colab.\n",
    "\n",
    "2. **ASSUME & Learning Basics**\n",
    "\n",
    "   A conceptual overview of RL within the ASSUME framework, including actor-critic architectures, centralized training, and multi-agent design principles.\n",
    "\n",
    "3. **Defining the Observation Space**\n",
    "\n",
    "   Explanation and coding tasks for constructing shared and individual observations used by agents to make decisions.\n",
    "\n",
    "4. **Action Selection and Exploration**\n",
    "\n",
    "   Retrieving the agents actions based on the observed environment and why it is important to explore actions beyond the output values.\n",
    "   \n",
    "5. **From Observation to Action to Bids**\n",
    "\n",
    "   How to convert actor network outputs into economically meaningful bid prices and apply exploration during training.\n",
    "\n",
    "6. **Reward Function Design**\n",
    "\n",
    "   Techniques for shaping agent behavior using profit- and regret-based reward signals. Includes a task to define your own reward logic.\n",
    "\n",
    "7. **Training and Evaluating Your First Learning Agent**\n",
    "\n",
    "   Integration of the previously implemented components into a complete simulation run, demonstrating end-to-end learning behavior in a market setting.\n",
    "\n",
    "8. **Analyzing Strategic Bidding Behavior**\n",
    "\n",
    "   Investigate how a RL agent exploits its market power by learning strategic bidding behavior as an example for more realistic market simulations.\n",
    "\n",
    "9. **Summary and Outlook**\n",
    "\n",
    "   Wraps up the contents of this tutorial. Further ideas which components of the learning process can be additionally tweaked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d221706",
   "metadata": {},
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "By completing this tutorial, you will be able to:\n",
    "\n",
    "* Implement RL-compatible bidding strategies within the ASSUME framework.\n",
    "* Define observation inputs for learning agents.\n",
    "* Map actor outputs to valid market actions and manage exploration.\n",
    "* Construct reward functions that combine economic incentives with strategic signals.\n",
    "* Train and evaluate a basic RL agent in a multi-agent electricity market simulation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cb4ab",
   "metadata": {},
   "source": [
    "## 1. Get ASSUME Running\n",
    "\n",
    "This chapter walks you through setting up the ASSUME framework in your environment and preparing the required input files. At the end, you will confirm that the installation was successful and ready for use.\n",
    "\n",
    "\n",
    "### 1.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608f337",
   "metadata": {},
   "source": [
    "#### In Google Colab\n",
    "\n",
    "Google Colab already includes most scientific computing libraries (e.g., `numpy`, `torch`). You only need to install the ASSUME core framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you are using Google Colab\n",
    "import importlib.util\n",
    "\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install assume-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1014e2",
   "metadata": {},
   "source": [
    "> **Note**: After installation, **Colab may prompt you to restart the session** due to dependency changes.\n",
    "> To do so, click **\"Runtime\" â†’ \"Restart session...\"** in the menu bar, then re-run the cells above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a2d43",
   "metadata": {},
   "source": [
    "#### On Your Local Machine\n",
    "\n",
    "To install ASSUME with all learning-related dependencies, run the following in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install 'assume-framework[learning]'\n",
    "```\n",
    "\n",
    "This will install the simulation framework and the packages required for RL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ca74e",
   "metadata": {},
   "source": [
    "### 1.2 Repository Setup\n",
    "\n",
    "To access predefined simulation scenarios, clone the ASSUME repository (Colab only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you are using Google Colab\n",
    "if IN_COLAB:\n",
    "    !git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6aad1",
   "metadata": {},
   "source": [
    "> Local users may skip this step if input files are already available in the project directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49910de8",
   "metadata": {},
   "source": [
    "### 1.3 Input Path Configuration\n",
    "\n",
    "We define the path to input files depending on whether you're in Colab or working locally. This variable will be used to load configuration and scenario files throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7368b9",
   "metadata": {},
   "source": [
    "### 1.4 Installation Check\n",
    "\n",
    "Use the following cell to ensure the installation was successful and that essential components are available. This test ensures that the simulation engine and RL strategy base class are accessible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6304eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from assume import World\n",
    "    from assume.strategies.learning_strategies import TorchLearningStrategy\n",
    "\n",
    "    print(\"âœ… ASSUME framework is installed and functional.\")\n",
    "except ImportError as e:\n",
    "    print(\"âŒ Failed to import essential components:\", e)\n",
    "    print(\n",
    "        \"Please review the installation instructions and ensure all dependencies are installed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c05cc",
   "metadata": {},
   "source": [
    "### 1.5 Limitations in Colab\n",
    "\n",
    "Colab does not support Docker, so dashboard visualizations included in some ASSUME workflows will not be available. However, simulation runs and RL training can still be fully executed.\n",
    "\n",
    "* In **Colab**: Training and basic plotting are supported.\n",
    "* In **Local environments with Docker**: Full access, including dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcee69",
   "metadata": {},
   "source": [
    "### 1.6 Core Imports\n",
    "\n",
    "In this section, we import the core modules that will be used throughout the tutorial. Each import is explained to clarify its role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import logging  # For logging messages during simulation and debugging\n",
    "import os  # For operating system interactions\n",
    "from datetime import timedelta  # To handle market time resolutions (e.g., hourly steps)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific and data processing libraries\n",
    "import numpy as np  # Numerical operations and array handling\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import yaml  # Parsing YAML configuration files\n",
    "\n",
    "# Database and visualization libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ASSUME framework components\n",
    "from assume import World  # Core simulation container that manages markets and agents\n",
    "from assume.scenario.loader_csv import (  # Functions to load and execute scenarios\n",
    "    load_scenario_folder,\n",
    "    run_learning,\n",
    ")\n",
    "from assume.strategies.learning_strategies import (\n",
    "    MinMaxStrategy,  # Abstract class for powerplant-like strategies\n",
    "    TorchLearningStrategy,  # Abstract base for RL bidding strategies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d63d4b",
   "metadata": {},
   "source": [
    "These imports are used for:\n",
    "\n",
    "* Defining RL bidding strategies.\n",
    "* Managing input/output data.\n",
    "* Executing and analyzing simulations.\n",
    "\n",
    "\n",
    "At this point, you are ready to begin building your RL bidding agent. In the next chapter, we will define how agents perceive the market by constructing their **observation vectors**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a26cb",
   "metadata": {},
   "source": [
    "## 2. ASSUME & Learning Basics\n",
    "\n",
    "### 2.1 The ASSUME Framework\n",
    "\n",
    "ASSUME is a simulation framework designed for researchers, utilities, and planners to model and understand market dynamics in electricity systems. It allows for agent-based modeling of market participants in a modular and configurable environment.\n",
    "\n",
    "The core structure of the framework consists of:\n",
    "\n",
    "* **Markets** (on the left of the architecture diagram): Where electricity products are traded.\n",
    "* **Market Participants / Units** (on the right): Each agent represents a physical or virtual unit bidding into the market.\n",
    "* **Orders**: The main communication channel between units and markets.\n",
    "* **Learning Agents**: Highlighted in yellow in the architecture, these are agents using RL strategies.\n",
    "\n",
    "\n",
    "> The image below illustrates the high-level architecture of ASSUME. Focus on the yellow componentsâ€”these are the parts involved in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "image_path = Path(\"assume-repo/docs/source/img/architecture.svg\")\n",
    "alt_image_path = Path(\"../../docs/source/img/architecture.svg\")\n",
    "\n",
    "if image_path.exists():\n",
    "    display(SVG(image_path))\n",
    "elif alt_image_path.exists():\n",
    "    display(SVG(alt_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79983d44",
   "metadata": {},
   "source": [
    "### 2.2 Introduction to Learning in ASSUME\n",
    "\n",
    "The current implementation of RL in ASSUME models electricity markets as **partially observable Markov games**, allowing multiple agents to operate under individual observations and reward structures.\n",
    "\n",
    "If you are unfamiliar with RL, refer to the following links for background material:\n",
    "\n",
    "* [Reinforcement Learning Overview](https://assume.readthedocs.io/en/latest/learning.html)\n",
    "* [Reinforcement Learning Algorithms](https://assume.readthedocs.io/en/latest/learning_algorithm.html)\n",
    "\n",
    "**Central Concepts:**\n",
    "\n",
    "* **Policy**: The strategy used by an agent to select actions based on observations.\n",
    "* **Actor-Critic Architecture**: A method where the \"actor\" chooses actions and the \"critic\" evaluates them.\n",
    "* **Learning Strategy**: Defines how a unit transforms observations into bids using a trainable model.\n",
    "* **Step Functions**: The typical RL cycle of Observe â†’ Act â†’ Reward â†’ Update is split across several methods in ASSUME, as described in Section 3.\n",
    "\n",
    "### 2.3 Single-Agent RL\n",
    "\n",
    "In a single-agent setup, the agent attempts to maximize its reward over time by learning from interaction with the environment. It does so by making multiple steps in the environment. In RL, each interaction step includes:\n",
    "\n",
    "1. **Observation** of the current state.\n",
    "2. **Action** selection based on policy.\n",
    "3. **Reward** from the environment.\n",
    "4. **Policy Update** to improve behavior.\n",
    "\n",
    "In ASSUME, this step cycle is modularized:\n",
    "\n",
    "| RL Step | Implemented via                                            | Description                                  |\n",
    "| ------- | ---------------------------------------------------------- | -------------------------------------------- |\n",
    "| Step 1  | `create_observation()` and `get_individual_observations()` | Constructs the observation vector.           |\n",
    "| Step 2  | `calculate_bids()` and `get_actions()`                     | Maps observations to bid prices.             |\n",
    "| Step 3  | `calculate_reward()`                                       | Computes the reward signal.                  |\n",
    "| Step 4  | Handled by the learning role                               | Updates model and manages the replay buffer. |\n",
    "\n",
    "\n",
    "**Actor-Critic Structure:**\n",
    "To increase learning stability actor-critic methods are commonly used. They divide the tasks in the following way:\n",
    "\n",
    "* **Actor**: Learns a deterministic policy for choosing actions. Uses policy gradient methods to maximize expected reward.\n",
    "* **Critic**: Learns a value function using Temporal Difference (TD) learning. Provides feedback to the actor based on action quality.\n",
    "\n",
    "### 2.4 Multi-Agent RL\n",
    "\n",
    "Real-world electricity markets involve multiple agents acting simultaneously, which introduces interdependencies and non-stationarity. The latter refers to the fact that the continuous adaptation of other agents makes the environment change and therefore less predictable from the perspective of a single agent. As a result, multi-agent learning requires additional considerations.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "* Actions by one agent influence the environment experienced by others.\n",
    "* The state transitions and rewards become non-stationary.\n",
    "\n",
    "**Solution: Centralized Training with Decentralized Execution (CTDE)**\n",
    "\n",
    "To address these challenges, ASSUME employs the **Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3)** framework with CTDE:\n",
    "\n",
    "* **Centralized Training**: A critic with access to all agents' states and actions is used during training to stabilize learning. Note the critic is only there to update the actor network, so it is only necessary while training. \n",
    "* **Decentralized Execution**: During simulation, the actual actor of each agent relies only on its own observations and learned policy.\n",
    "\n",
    "Each agent trains two critic networks to mitigate overestimation bias, uses target noise for robustness, and relies on deterministic policy gradients for the actor update.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e18a81",
   "metadata": {},
   "source": [
    "## 3. Defining the Observation Space\n",
    "\n",
    "In this chapter, you will define what information your RL agent perceives about the environment and itself at each decision point. This is a critical component of the agentâ€™s behavior, as the observation vector forms the basis for all future actions and learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab1bb3",
   "metadata": {},
   "source": [
    "| RL Step | Implemented via                                            | Description                                  |\n",
    "| ------- | ---------------------------------------------------------- | -------------------------------------------- |\n",
    "| **Step 1**  | `create_observation()` and `get_individual_observations()` | Constructs the observation vector.           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86b3b",
   "metadata": {},
   "source": [
    "### 3.2 Observation Structure in ASSUME\n",
    "\n",
    "Observations are composed of two parts:\n",
    "\n",
    "**1. Global Observations**\n",
    "\n",
    "These are shared across all agents and constructed by the base class method `create_observation()`. They include:\n",
    "\n",
    "* **Forecasted residual load** over the foresight horizon.\n",
    "* **Forecasted market price** over the foresight horizon.\n",
    "* **Historical market price** over a specified window.\n",
    "\n",
    "> These are normalized by maximum demand and maximum bid price for stability.\n",
    "> These values are generated by a forecasting role and made available to all agents before each market cycle.\n",
    "\n",
    "For this tutorial **you do not need to modify this part.** However, if you want to equip new units types with learning or expand the simulation by new concepts, additional global information might be needed. \n",
    "\n",
    "**2. Individual Observations**\n",
    "\n",
    "These are unit-specific and must be implemented by you. The purpose is to provide the agent with private, operational information that may help improve bidding decisions. Each agent appends this information to the end of the shared observation vector.\n",
    "\n",
    "This is done via the method `get_individual_observations(unit, start)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8380f",
   "metadata": {},
   "source": [
    "### 3.3 Defining the Strategy Class and Constructor\n",
    "\n",
    "To enable learning, we define a custom class that extends `TorchLearningStrategy` and initializes key dimensions for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyLearningSingleBidStrategy(TorchLearningStrategy, MinMaxStrategy):\n",
    "    \"\"\"\n",
    "    A simple reinforcement learning bidding strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Forecast horizon (in timesteps) used for market and residual load forecasts\n",
    "        foresight = kwargs.pop(\"foresight\", 12)\n",
    "        act_dim = kwargs.pop(\"act_dim\", 1)  # One action: bid price\n",
    "        unique_obs_dim = kwargs.pop(\"unique_obs_dim\", 2)  # Number of individual obs\n",
    "\n",
    "        super().__init__(\n",
    "            foresight=foresight,\n",
    "            act_dim=act_dim,\n",
    "            unique_obs_dim=unique_obs_dim,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc511c",
   "metadata": {},
   "source": [
    "With your defined foresight range the global observations are defined in the function `create_observation` of the base class. Based on the chosen foresight the observation_space dimension is calculated automatically following `self.obs_dim = num_timeseries_obs_dim * foresight + unique_obs_dim` as defined in the base class. If one wants to change that rational it needs to be overwritten in the `learning_strategy` itself.\n",
    "\n",
    "### 3.4 Exercise 1: Define Individual Observations\n",
    "\n",
    "Now you will implement the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def get_individual_observations(self, unit, start, end):\n",
    "        \"\"\"\n",
    "        Define custom unit-specific observations for the RL agent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        unit : SupportsMinMax\n",
    "            The unit representing the power plant.\n",
    "        start : datetime.datetime\n",
    "            Start time of the market product.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Normalized 1D array of individual observations.\n",
    "        \"\"\"\n",
    "        # Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536464e8",
   "metadata": {},
   "source": [
    "This method must return a NumPy array of length `unique_obs_dim`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e5aa7",
   "metadata": {},
   "source": [
    "**What Should Be in an Individual Observation?**\n",
    "\n",
    "The key principle is to include values that are:\n",
    "\n",
    "* Known **only** to the unit itself.\n",
    "* Relevant for market bidding.\n",
    "* Reflective of the unitâ€™s technical or economic constraints.\n",
    "\n",
    "Here are some good candidate features and how to compute them using ASSUME:\n",
    "\n",
    "| Feature                             | Description                                    | Access via                                            |\n",
    "| ----------------------------------- | ---------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Current output**                  | How much power the unit is currently producing | `unit.get_output_before(start)`                       |\n",
    "| **Marginal cost**                   | Cost to produce current output                 | `unit.calculate_marginal_cost(start, current_volume)` |\n",
    "| **Max capacity**                    | Upper generation limit                         | `unit.max_power`                                      |\n",
    "| **Max bid price**                   | Maximum price at market                        | `self.max_bid_price`               |\n",
    "| **Start-up/shut-down state**        | May be encoded in dispatch history             | infer from `unit.get_output_before(start)`            |\n",
    "| **Ramp limit**                      | Maximum change in output allowed               | `unit.ramp_up`, `unit.ramp_down`                      |\n",
    "| **Efficiency or fuel cost factors** | If applicable                                  | custom attributes per unit model                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e7654",
   "metadata": {},
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b06e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercise 1: Define Individual Observations\n",
    "\n",
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def get_individual_observations(self, unit, start, end):\n",
    "        # --- Current volume & marginal cost ---\n",
    "        current_volume = unit.get_output_before(start)\n",
    "        current_costs = unit.calculate_marginal_cost(start, current_volume)\n",
    "\n",
    "        scaled_total_dispatch = current_volume / unit.max_power\n",
    "        scaled_marginal_cost = current_costs / self.max_bid_price\n",
    "\n",
    "        individual_observations = np.array(\n",
    "            [scaled_total_dispatch, scaled_marginal_cost]\n",
    "        )\n",
    "\n",
    "        return individual_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b440f6d",
   "metadata": {},
   "source": [
    "### 3.5 Summary\n",
    "\n",
    "* Observations in ASSUME combine **shared global forecasts** and **custom individual data**.\n",
    "* The base class handles forecasted residual load and price, as well as historical price signals.\n",
    "* These observations directly affect agent behavior and learning convergenceâ€”thoughtful design matters.\n",
    "\n",
    "\n",
    "In the next chapter, you will define **how the agent selects actions** based on its observations, and how **exploration** is introduced during initial training to populate the learning buffer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6afe4",
   "metadata": {},
   "source": [
    "## 4. Action Selection and Exploration\n",
    "\n",
    "Once an observation is formed, the next step is for the agent to decide how to act. In this context, the **action** determines the **bid price** submitted by the agent to the electricity market.\n",
    "\n",
    "\n",
    "This chapter focuses on how actions are derived from the agentâ€™s policy and how exploration is handledâ€”especially during the early training phase when experience is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bf5a3",
   "metadata": {},
   "source": [
    "### 4.1 Action Selection in RL\n",
    "\n",
    "In RL, the **policy** defines the agentâ€™s behavior: it maps observations to actions. In the actor-critic architecture used by ASSUME, this policy is represented by the **actor neural network**.\n",
    "\n",
    "However, to enable **exploration**, especially in the early stages of training, agents must not always follow the policy exactly. They need to try out a variety of actionsâ€”even suboptimal onesâ€”to collect diverse experiences and learn effectively.\n",
    "\n",
    "This is done by **adding noise** to the actions suggested by the policy network.\n",
    "\n",
    "> Note: The implementation of noise we present here is specific to the used algorithm **MADDPG**. Other Algorithms such as the PPO will use a different mechanism for exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec3eae",
   "metadata": {},
   "source": [
    "### 4.2 Understanding `get_actions()`\n",
    "\n",
    "The method `get_actions(next_observation)` in `TorchLearningStrategy` defines how actions are computed in different modes of operation.\n",
    "\n",
    "Here is a simplified overview of the logic:\n",
    "\n",
    "```python\n",
    "def get_actions(self, next_observation):\n",
    "    if self.learning_mode and not self.evaluation_mode:\n",
    "        if self.collect_initial_experience_mode:\n",
    "            # Initial exploration: use pure noise as action\n",
    "            noise = self.action_noise.noise(...)\n",
    "            curr_action = noise\n",
    "        else:\n",
    "            # Regular exploration: add noise to policy output\n",
    "            curr_action = self.actor(next_observation).detach()\n",
    "            noise = self.action_noise.noise(...)\n",
    "            curr_action += noise\n",
    "    else:\n",
    "        # Evaluation or deterministic policy use\n",
    "        curr_action = self.actor(next_observation).detach()\n",
    "        noise = zeros_like(curr_action)\n",
    "\n",
    "    return curr_action, noise\n",
    "```\n",
    "\n",
    "**Modes of Operation:**\n",
    "\n",
    "* `learning_mode`: Indicates that the agent is being trained (vs. used for evaluation).\n",
    "* `evaluation_mode`: Disables noise; used to assess performance of a learned policy.\n",
    "* `collect_initial_experience_mode`: Special sub-phase during early episodes where we rely heavily on **randomized exploration** to populate the replay buffer with diverse samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3cc34",
   "metadata": {},
   "source": [
    "### 4.3 What Is Initial Experience Collection Mode?\n",
    "\n",
    "The **initial experience collection mode** refers to the first `N` episodes of training where agents fill their learning buffers **purely through exploration**. No learned policy is used at this stage.\n",
    "\n",
    "The purpose is to:\n",
    "\n",
    "* Cover a broad region of the action space.\n",
    "* Enable agents to observe the outcome of many different bidding decisions.\n",
    "\n",
    "By default, the action in this mode is **pure noise**, sampled from a Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f851",
   "metadata": {},
   "source": [
    "### 4.4 Improving Exploration with Prior Knowledge\n",
    "\n",
    "While random actions help explore broadly, we can use **economic and technical knowledge** to make exploration more guided.\n",
    "\n",
    "**What would be a good starting point for a conventional generator?** Exploring in a region around this value is far more productive than exploring arbitrarily.\n",
    "\n",
    "Thus, instead of using random noise alone, we can **shift the noisy action** around a known good starting point so that exploration begins from a plausible economic baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed1518c",
   "metadata": {},
   "source": [
    "### 4.5 Exercise 2: Guided Exploration\n",
    "\n",
    "Your task is to modify the `get_actions()` method to implement a better initial exploration mechanism.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "During the `collect_initial_experience_mode`, instead of using pure noise, base the exploration around a known signal from the observation vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97943adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def get_actions(self, next_observation):\n",
    "        \"\"\"\n",
    "        Compute actions based on the current observation, optionally applying noise for exploration.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        next_observation : torch.Tensor\n",
    "            The current observation, where the last element is assumed to be the marginal cost.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            - Action (with or without noise)\n",
    "            - The applied noise\n",
    "        \"\"\"\n",
    "        # Get the base action and associated noise from the parent implementation\n",
    "        curr_action, noise = super().get_actions(next_observation)\n",
    "\n",
    "        if self.learning_mode and not self.evaluation_mode:\n",
    "            if self.collect_initial_experience_mode:\n",
    "                # TODO: extract a relevant reference value from next_observation\n",
    "                # TODO: shift the noisy action around this value\n",
    "                pass  # replace this with your implementation\n",
    "\n",
    "        return curr_action, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b62bf",
   "metadata": {},
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercise 2: Improve Initial Exploration\n",
    "\n",
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def get_actions(self, next_observation):\n",
    "        # Get the base action and associated noise from the parent implementation\n",
    "        curr_action, noise = super().get_actions(next_observation)\n",
    "\n",
    "        if self.learning_mode and not self.evaluation_mode:\n",
    "            if self.collect_initial_experience_mode:\n",
    "                # Assumes last dimension of the observation corresponds to marginal cost\n",
    "                marginal_cost = next_observation[-1].detach()\n",
    "                # Center the noisy action around marginal cost\n",
    "                curr_action += marginal_cost\n",
    "\n",
    "        return curr_action, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180916d6",
   "metadata": {},
   "source": [
    "This strategy anchors exploration to a meaningful economic quantity, improving the quality of early experiences and accelerating convergence.\n",
    "\n",
    "### 4.6 Summary\n",
    "\n",
    "* The `get_actions()` method controls how agents choose actions under different modes.\n",
    "* During training, actions include noise to enable exploration.\n",
    "* Initial exploration can be enhanced by guiding actions toward domain-relevant baselines (e.g., marginal cost).\n",
    "* You implemented a strategy to **anchor exploration** using part of the observation vector.\n",
    "\n",
    "In the next chapter, we will transform the action values into **actual bids** by applying domain knowledge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b671f",
   "metadata": {},
   "source": [
    "## 5. From Observation to Action to Bids\n",
    "\n",
    "In the previous chapters, we explored how an agent perceives its environment through observations and how it selects actions using its policy, optionally enriched with exploration noise. In this short chapter, we show how these two steps come together inside the `calculate_bids()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9005c",
   "metadata": {},
   "source": [
    "### 5.1 The Role of `calculate_bids()`\n",
    "\n",
    "The `calculate_bids()` method defines how a market participant formulates its bid at each market interval. It brings together two crucial operations:\n",
    "\n",
    "1. **Generating Observations**:\n",
    "   Calls `create_observation()` to construct the full input vector (including both global and individual components).\n",
    "\n",
    "2. **Choosing an Action**:\n",
    "   Passes the observation to `get_actions()`, which invokes the actor network (and optionally adds noise) to return an action vector.\n",
    "\n",
    "This forms the agentâ€™s internal decision pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11607d",
   "metadata": {},
   "source": [
    "### 5.2 Action Normalization and Scaling\n",
    "\n",
    "The neural network policy outputs **normalized actions**â€”typically bounded in the range $[-1, 1]$. To convert these to meaningful bid prices, the raw action is scaled by a predefined constant:\n",
    "\n",
    "```python\n",
    "bid_price = actions[0] * self.max_bid_price\n",
    "```\n",
    "\n",
    "For example, if `self.max_bid_price = 100`, the resulting bid prices will fall between $-100$ and $100$. This reflects a design choice that bounds the agentâ€™s economic behavior in a defined domain.\n",
    "\n",
    "By modifying `max_bid_price` in the learning config, you directly influence the economic aggressiveness of the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a9025",
   "metadata": {},
   "source": [
    "### 5.3 Bid Structure\n",
    "\n",
    "Each bid submitted to the market follows a defined structure, encapsulated as a dictionary:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"start_time\": start,\n",
    "    \"end_time\": end,\n",
    "    \"price\": bid_price,\n",
    "    \"volume\": max_power,\n",
    "    \"node\": unit.node,\n",
    "}\n",
    "```\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* **price**: Determined from the scaled output of the policy.\n",
    "* **volume**: Set to the full technical capacity of the unit.\n",
    "* **node**: Locational identifier (used for zonal/nodal pricing and congestion modeling).\n",
    "\n",
    "Note that `max_power` is **positive**, as this strategy models a generator offering energy. For a **consumer or demand bid**, the volume would be **negative** to reflect load withdrawal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838b5df",
   "metadata": {},
   "source": [
    "### 5.4 Controlling Action Dimensions\n",
    "\n",
    "By changing the `act_dim` in the strategy constructor, you can control the number of outputs returned by the actor network:\n",
    "\n",
    "```python\n",
    "act_dim = kwargs.pop(\"act_dim\", 1)\n",
    "```\n",
    "\n",
    "This allows for richer bidding logic. For instance:\n",
    "\n",
    "* 1 action: Bid price for total capacity.\n",
    "* 2 actions: Bid prices for flexible vs. inflexible portions.\n",
    "* 3 actions: Add directionality or reserve offers.\n",
    "\n",
    "However, it is important to note that **RL performance deteriorates with high-dimensional action spaces**, especially in continuous domains.\n",
    "\n",
    "If you decide to increase `act_dim`, ensure that your `calculate_bids()` method is updated accordingly to interpret and transform all action elements correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6cd5",
   "metadata": {},
   "source": [
    "### 5.5 Full Code Implementation\n",
    "\n",
    "Here is the complete `calculate_bids()` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def calculate_bids(self, unit, market_config, product_tuples, **kwargs):\n",
    "        start = product_tuples[0][0]\n",
    "        end = product_tuples[0][1]\n",
    "\n",
    "        # get technical bounds for the unit output from the unit\n",
    "        _, max_power = unit.calculate_min_max_power(start, end)\n",
    "        max_power = max_power[0]\n",
    "\n",
    "        # =============================================================================\n",
    "        # 1. Get the observations, which are the basis of the action decision\n",
    "        # =============================================================================\n",
    "        next_observation = self.create_observation(\n",
    "            unit=unit, market_id=market_config.market_id, start=start, end=end\n",
    "        )\n",
    "\n",
    "        # =============================================================================\n",
    "        # 2. Get the actions, based on the observations\n",
    "        # =============================================================================\n",
    "        actions, noise = self.get_actions(next_observation)\n",
    "\n",
    "        # =============================================================================\n",
    "        # 3. Transform actions into bids\n",
    "        # =============================================================================\n",
    "        # actions are in the range [-1,1], we need to transform them into actual bids\n",
    "        # we can use our domain knowledge to guide the bid formulation\n",
    "        bid_price = actions[0] * self.max_bid_price\n",
    "\n",
    "        # actually formulate bids in orderbook format\n",
    "        bids = [\n",
    "            {\n",
    "                \"start_time\": start,\n",
    "                \"end_time\": end,\n",
    "                \"price\": bid_price,\n",
    "                \"volume\": max_power,\n",
    "                \"node\": unit.node,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        if self.learning_mode:\n",
    "            self.learning_role.add_actions_to_cache(self.unit_id, start, actions, noise)\n",
    "\n",
    "        return bids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9def27c",
   "metadata": {},
   "source": [
    "In the next chapter, we will define how to compute the **reward** associated with each bid outcome, which completes the agentâ€™s learning cycle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462de3c",
   "metadata": {},
   "source": [
    "## 6. Reward Function Design\n",
    "\n",
    "The reward function is the **central learning signal** in any RL environment. It defines the objective the agent is trying to maximize and serves as the only feedback mechanism from the environment to the agent.\n",
    "\n",
    "In market-based simulations such as ASSUME, designing the reward function is a delicate balance between:\n",
    "\n",
    "* Capturing **realistic economic goals** (e.g., profit maximization),\n",
    "* Enabling **learning stability and convergence**, and\n",
    "* Leaving room for the agent to **discover unexpected, valid strategies**.\n",
    "\n",
    "Itâ€™s tempting to hard-code your preferred behavior into the reward function. However, this often leads to agents that are overly adapted to a specific scenario and perform poorly in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2ad62",
   "metadata": {},
   "source": [
    "### 6.1 When Is the Reward Computed?\n",
    "\n",
    "In ASSUME, the reward is computed **after the market clears**, in the `calculate_reward()` method. At this point, the agent receives information about:\n",
    "\n",
    "* Which portion of its bid was accepted,\n",
    "* At what price,\n",
    "* And what operational costs it incurred.\n",
    "\n",
    "This allows us to calculate realized **profit**, which is the most direct economic reward signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a279e",
   "metadata": {},
   "source": [
    "### 6.2 Exercise 3: Implement Profit-Based Reward\n",
    "\n",
    "Your first task is to implement a profit-based reward. This is **mandatory**.\n",
    "\n",
    "Use the following simplified formula:\n",
    "\n",
    "$$\n",
    "\\pi_{i,t} =\n",
    "\\begin{cases}\n",
    "P^\\text{conf}_{i,t} \\cdot (M_t - mc_{i,t}) \\cdot dt & \\text{if } P^\\text{conf}_{i,t} \\geq 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $P^\\text{conf}$: Confirmed volume (accepted by market),\n",
    "* $M_t$: Market clearing price,\n",
    "* $mc_{i,t}$: Marginal generation cost,\n",
    "* $dt$: Time resolution in hours.\n",
    "\n",
    "You can access these quantities via:\n",
    "\n",
    "```python\n",
    "accepted_volume = order[\"accepted_volume\"]\n",
    "market_clearing_price = order[\"accepted_price\"]\n",
    "marginal_cost = unit.calculate_marginal_cost(start, unit.outputs[marketconfig.product_type].at[start])\n",
    "```\n",
    "\n",
    "Use the duration in hours:\n",
    "\n",
    "```python\n",
    "duration = (end - start) / timedelta(hours=1)\n",
    "```\n",
    "\n",
    "### 6.4 Exercise 3 (optional): Thinking Beyond Profit\n",
    "\n",
    "While profit is a good starting point, agents trained solely on profit may struggle in competitive environments or when there is limited dispatch. In real-world operations, generators also consider **missed opportunities**â€”what could have been earned but wasnâ€™t due to poor bidding or conservative behavior.\n",
    "\n",
    "> What other signal could guide the agent to bid more strategically?\n",
    ">\n",
    "> What do real power plants look at when evaluating their bidding successâ€”even when they were not dispatched?\n",
    "\n",
    "Use your economic intuition or power system experience to answer this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def calculate_reward(self, unit, marketconfig, orderbook):\n",
    "        \"\"\"\n",
    "        Reward function: implement profit and (optionally) opportunity cost.\n",
    "\n",
    "        Instructions:\n",
    "        - Fill in the lines marked as YOUR CODE.\n",
    "        - Compute profit as the primary reward signal.\n",
    "        - Optionally define the opportunity cost as a regret term.\n",
    "        \"\"\"\n",
    "\n",
    "        start = orderbook[0][\"start_time\"]\n",
    "        end = orderbook[0][\"end_time\"]\n",
    "        duration = (end - start) / timedelta(hours=1)\n",
    "        end_excl = end - unit.index.freq\n",
    "\n",
    "        order = orderbook[0]\n",
    "        market_clearing_price = None  # YOUR CODE HERE\n",
    "        accepted_volume = None  # YOUR CODE HERE\n",
    "\n",
    "        marginal_cost = unit.calculate_marginal_cost(\n",
    "            start, unit.outputs[marketconfig.product_type].at[start]\n",
    "        )\n",
    "\n",
    "        # === Required: compute profit ===\n",
    "        order_income = None  # YOUR CODE HERE\n",
    "        order_cost = None  # YOUR CODE HERE\n",
    "        order_profit = None  # YOUR CODE HERE\n",
    "\n",
    "        # === Optional: compute opportunity cost ===\n",
    "        opportunity_cost = None  # YOUR CODE HERE\n",
    "\n",
    "        regret_scale = 0.1 if accepted_volume > unit.min_power else 0.5\n",
    "\n",
    "        # === Normalize reward to ~[-1, 1] ===\n",
    "        scaling = 1 / (self.max_bid_price * unit.max_power)\n",
    "        reward = scaling * (order_profit - regret_scale * opportunity_cost)\n",
    "        regret = regret_scale * opportunity_cost\n",
    "\n",
    "        # Store results in unit outputs\n",
    "        # Note: these are not learning-specific results but stored for all units for analysis\n",
    "        unit.outputs[\"profit\"].loc[start:end_excl] += order_profit\n",
    "        unit.outputs[\"total_costs\"].loc[start:end_excl] += order_cost\n",
    "\n",
    "        # write rl-rewards to buffer\n",
    "        if self.learning_mode:\n",
    "            self.learning_role.add_reward_to_cache(\n",
    "                unit.id, start, reward, regret, order_profit\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad88fc",
   "metadata": {},
   "source": [
    "ðŸ’¡**Hint Optional Extension: Opportunity Cost**\n",
    "\n",
    "The concept of **opportunity cost** captures the **lost profit** from unused capacity. If the market price exceeds marginal cost and the unit wasn't dispatched fully, that represents a missed opportunity.\n",
    "\n",
    "This can be used as a **regret term** to penalize under-utilization of profitable bids.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "cm_{i,t} = \\max\\left[(P^{\\max}_i - P^\\text{conf}_{i,t}) \\cdot (M_t - mc_{i,t}) \\cdot dt, 0\\right]\n",
    "$$\n",
    "\n",
    "A good reward function combines profit and opportunity cost, allowing agents to learn from both actual performance and missed potential.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f8b90",
   "metadata": {},
   "source": [
    "### 6.6 Reward Scaling and Learning Stability\n",
    "\n",
    "Scaling the reward to a **narrow and consistent range** is crucial for stable RL. This is particularly important in continuous-action settings like bidding, where one overly large reward spike can skew the policy updates significantly.\n",
    "\n",
    "**1. Why scale?**\n",
    "\n",
    "* Stabilizes gradients during actor-critic training.\n",
    "* Makes different time steps comparable in magnitude.\n",
    "* Prevents the agent from overfitting to rare but extreme events.\n",
    "\n",
    "**2. What can go wrong?**\n",
    "\n",
    "If your scaling factor is too small:\n",
    "\n",
    "* Rewards become indistinguishable from noise.\n",
    "\n",
    "If your scaling factor is too large:\n",
    "\n",
    "* A single high-reward event (e.g., bidding into a rare price spike) can **dominate learning**,\n",
    "  making the agent try to reproduce that event rather than learn a general policy.\n",
    "\n",
    "> **Tip**: Use conservative scaling based on maximum realistic bid Ã— capacity:\n",
    "\n",
    "```python\n",
    "scaling = 1 / (self.max_bid_price * unit.max_power)\n",
    "```\n",
    "\n",
    "**3 Recommended Practice**\n",
    "\n",
    "Before committing to training:\n",
    "\n",
    "* **Plot the distribution of rewards** across time steps for a few sample runs.\n",
    "* Check for outliers, saturation, or skewness.\n",
    "* If needed, adjust `scaling` or cap outliers in reward postprocessing.\n",
    "\n",
    "This diagnostic step can save hours of failed training runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664884a",
   "metadata": {},
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d52545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercise 3: Implement Reward Function\n",
    "\n",
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class EnergyLearningSingleBidStrategy(EnergyLearningSingleBidStrategy):\n",
    "    def calculate_reward(\n",
    "        self,\n",
    "        unit,\n",
    "        marketconfig,\n",
    "        orderbook,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculates the reward for the unit based on profits, costs, and opportunity costs from market transactions.\n",
    "\n",
    "        The reward is computed by combining the following:\n",
    "        - **Profit**: Income from accepted bids minus marginal and start-up costs.\n",
    "        - **Opportunity Cost**: Penalty for underutilizing capacity, calculated as potential lost income.\n",
    "        - **Regret Term**: A scaled regret term penalizes high opportunity costs to guide effective bidding.\n",
    "\n",
    "        The reward is scaled and stored along with other outputs in the unitâ€™s data to support learning.\n",
    "        \"\"\"\n",
    "\n",
    "        start = orderbook[0][\"start_time\"]\n",
    "        end = orderbook[0][\"end_time\"]\n",
    "        duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "        # `end_excl` marks the last product's start time by subtracting one frequency interval.\n",
    "        end_excl = end - unit.index.freq\n",
    "\n",
    "        order = orderbook[0]  # Assuming a single order for simplicity\n",
    "\n",
    "        market_clearing_price = order[\"accepted_price\"]\n",
    "        accepted_volume = order[\"accepted_volume\"]\n",
    "\n",
    "        # Depending on how the unit calculates marginal costs, retrieve cost values.\n",
    "        marginal_cost = unit.calculate_marginal_cost(\n",
    "            start, unit.outputs[marketconfig.product_type].at[start]\n",
    "        )\n",
    "\n",
    "        # Calculate profit as income minus operational cost for this event.\n",
    "        order_income = market_clearing_price * accepted_volume * duration\n",
    "        order_cost = marginal_cost * accepted_volume * duration\n",
    "\n",
    "        # Accumulate income and operational cost for all orders.\n",
    "        order_profit = order_income - order_cost\n",
    "\n",
    "        # Opportunity cost: The income lost due to not operating at full capacity.\n",
    "        opportunity_cost = (\n",
    "            (market_clearing_price - marginal_cost)\n",
    "            * (unit.max_power - accepted_volume)\n",
    "            * duration\n",
    "        )\n",
    "\n",
    "        # If opportunity cost is negative, no income was lost, so we set it to zero.\n",
    "        opportunity_cost = max(opportunity_cost, 0)\n",
    "\n",
    "        # Dynamic regret scaling:\n",
    "        # - If accepted volume is positive, apply lower regret (0.1) to avoid punishment for being on the edge of the merit order.\n",
    "        # - If no dispatch happens, apply higher regret (0.5) to discourage idle behavior, if it could have been profitable.\n",
    "        regret_scale = 0.1 if accepted_volume > unit.min_power else 0.5\n",
    "\n",
    "        # --------------------\n",
    "        # 4.1 Calculate Reward\n",
    "        # Instead of directly setting reward = profit, we incorporate a regret term (opportunity cost penalty).\n",
    "        # This guides the agent toward strategies that maximize accepted bids while minimizing lost opportunities.\n",
    "\n",
    "        # scaling factor to normalize the reward to the range [-1,1]\n",
    "        scaling = 1 / (self.max_bid_price * unit.max_power)\n",
    "        reward = scaling * (order_profit - regret_scale * opportunity_cost)\n",
    "        regret = regret_scale * opportunity_cost\n",
    "\n",
    "        # Store results in unit outputs\n",
    "        # Note: these are not learning-specific results but stored for all units for analysis\n",
    "        unit.outputs[\"profit\"].loc[start:end_excl] += order_profit\n",
    "        unit.outputs[\"total_costs\"].loc[start:end_excl] += order_cost\n",
    "\n",
    "        # write rl-rewards to buffer\n",
    "        if self.learning_mode:\n",
    "            self.learning_role.add_reward_to_cache(\n",
    "                unit.id, start, reward, regret, order_profit\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04b334",
   "metadata": {},
   "source": [
    "### 6.7 Summary\n",
    "\n",
    "* The reward function is the core signal guiding agent learningâ€”design it carefully.\n",
    "* Start with **profit** as the primary reward.\n",
    "* Consider adding **opportunity cost** as a regret penalty to improve bidding behavior.\n",
    "* Always **normalize** your reward to maintain training stability.\n",
    "* Analyze your reward distribution empirically before training large-scale agents.\n",
    "\n",
    "In the next chapter, we will bring together all the componentsâ€”observation, action, and rewardâ€”and simulate a full training run using your custom learning strategy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f28147",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluating Your First Learning Agent\n",
    "\n",
    "You have now implemented all essential components of a learning bidding strategy in ASSUME:\n",
    "\n",
    "* Observations\n",
    "* Actions and exploration\n",
    "* Reward function\n",
    "\n",
    "In this chapter, you will connect your strategy to a simulation scenario, configure the learning algorithm, and evaluate the agentâ€™s training progress.\n",
    "\n",
    "\n",
    "### 7.1 Load and Inspect the Learning Configuration\n",
    "\n",
    "Each simulation scenario in ASSUME has an associated YAML configuration file. This file contains the **learning configuration**, which determines how the RL algorithm is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"base\"\n",
    "\n",
    "# Read the YAML file\n",
    "with open(f\"{inputs_path}/example_02a/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Print the learning config\n",
    "print(f\"Learning config for scenario '{scenario}':\")\n",
    "display(config[scenario][\"learning_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85338",
   "metadata": {},
   "source": [
    "**Explanation of Learning Configuration Parameters**\n",
    "\n",
    "| Parameter                                     | Description                                                                           |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **learning\\_mode**                            | If `True`, performs the policy updates and evaluates learned policies.                | \n",
    "| **continue\\_learning**                        | If `True`, resumes training from saved policy checkpoints.                            |\n",
    "| **trained\\_policies\\_save\\_path**             | File path where trained policies will be saved.                                       |\n",
    "| **trained\\_policies\\_load\\_path**             | Path to pre-trained policies to load.                                                 |\n",
    "| **max\\_bid\\_price**                           | Used to scale action outputs to economic bid prices.                                  |\n",
    "| **algorithm**                                 | Learning algorithm used (e.g., `matd3` for multi-agent TD3).                          |\n",
    "| **learning\\_rate**                            | Step size for policy and critic updates.                                              |\n",
    "| **training\\_episodes**                        | Number of simulation episodes (repetitions of the time horizon) used for training.    |\n",
    "| **episodes\\_collecting\\_initial\\_experience** | Number of episodes during which agents collect experience using guided exploration.   |\n",
    "| **train\\_freq**                               | Time between training updates, e.g., `'100h'` means update every 100 simulated hours. |\n",
    "| **gradient\\_steps**                           | Number of gradient descent steps per update.                                          |\n",
    "| **batch\\_size**                               | Size of experience batch used for training.                                           |\n",
    "| **gamma**                                     | Discount factor for future rewards ($0 < \\gamma \\leq 1$).                             |\n",
    "| **device**                                    | `\"cpu\"` or `\"cuda\"` depending on hardware.                                            |\n",
    "| **action\\_noise\\_schedule**                   | How the action noise evolves over time (`linear`, `constant`, etc.).                  |\n",
    "| **noise\\_sigma**                              | Standard deviation of exploration noise.                                              |\n",
    "| **noise\\_scale**                              | Global multiplier for noise.                                                          |\n",
    "| **noise\\_dt**                                 | Discretization interval for noise time series.                                        |\n",
    "| **validation\\_episodes\\_interval**            | How often (in episodes) to evaluate the current policy without exploration.           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c181a",
   "metadata": {},
   "source": [
    "### 7.2 Run the Simulation and Train the Agent\n",
    "\n",
    "The simulation environment and learning strategy are connected and executed as follows:\n",
    "\n",
    "> **Hint**: In Google Colab, long-running training sessions may occasionally **crash or disconnect** if the output console is flooded â€” for example, by verbose progress bars or print statements.  \n",
    "> To prevent this, you can suppress output during training using the following approach.\n",
    "\n",
    "1. **Import the required tools:**\n",
    "\n",
    "    ```python\n",
    "    from contextlib import redirect_stdout, redirect_stderr\n",
    "    import os\n",
    "    ```\n",
    "\n",
    "2. **Wrap the training phase with output redirection.**  \n",
    "   Insert the following lines **just before Step 4: _Run the training phase_**:\n",
    "\n",
    "    ```python\n",
    "    # Suppress output for the entire training process\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stdout(devnull), redirect_stderr(devnull):\n",
    "            # Your training function call goes here\n",
    "            train_agents(...)\n",
    "    ```\n",
    "\n",
    "> âœ… This redirects all `stdout` and `stderr` to `/dev/null`, preventing Colab from being overwhelmed by output and improving session stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4251a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "np.random.seed(42)  # Set a random seed for reproducibility\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # 1. Create simulation world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # 2. Register your learning strategy\n",
    "    world.bidding_strategies[\"pp_learning\"] = EnergyLearningSingleBidStrategy\n",
    "\n",
    "    # 3. Load scenario and case\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # 4. Run the training phase\n",
    "    if world.learning_mode:\n",
    "        run_learning(world)\n",
    "\n",
    "    # 5. Execute final evaluation run (no exploration)\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcb328",
   "metadata": {},
   "source": [
    "This script will:\n",
    "\n",
    "* Train the agent using your defined strategy.\n",
    "* Periodically evaluate the agent using a noise-free policy.\n",
    "* Save training data into the database for post-analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc3c74",
   "metadata": {},
   "source": [
    "### **7.3 Analyze Learning Performance**\n",
    "\n",
    "Once training is complete, we can evaluate the learning progress of your RL agent using data from the simulation database. ASSUME stores detailed training metrics in the `rl_params` table, which includes rewards for each time step, grouped by episode, unit, and whether the agent was in evaluation mode.\n",
    "\n",
    "In this case, we are interested in the performance of a specific generator: **`pp_6`**, within the simulation **`example_02a_base`**.\n",
    "\n",
    "Weâ€™ll extract the recorded rewards for this unit, group them by episode, and plot the average reward over time for both training and evaluation phases.\n",
    "\n",
    "> Instead of accessing the training evaluation via the database we also feature a tensorboard integration, which can be accessed in the console `tensorboard --logdir tensorboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the simulation database\n",
    "engine = create_engine(\"sqlite:///local_db/assume_db.db\")\n",
    "\n",
    "# Query rewards for specific simulation and unit\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    datetime,\n",
    "    unit,\n",
    "    reward,\n",
    "    simulation,\n",
    "    evaluation_mode,\n",
    "    episode\n",
    "FROM rl_params\n",
    "WHERE simulation = 'example_02a_base'\n",
    "  AND unit = 'pp_6'\n",
    "ORDER BY datetime\n",
    "\"\"\"\n",
    "\n",
    "# Load query results\n",
    "rewards_df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Rename column for consistency\n",
    "rewards_df.rename(columns={\"evaluation_mode\": \"evaluation\"}, inplace=True)\n",
    "\n",
    "# --- Separate plots for training and evaluation ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# Plot training rewards (evaluation == 0)\n",
    "train_df = rewards_df[rewards_df[\"evaluation\"] == 0]\n",
    "train_grouped = train_df.groupby(\"episode\")[\"reward\"].mean()\n",
    "\n",
    "axes[0].plot(train_grouped.index, train_grouped.values, color=\"tab:blue\")\n",
    "axes[0].set_title(\"Training Reward per Episode (Unit: pp_6)\")\n",
    "axes[0].set_ylabel(\"Average Reward\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot evaluation rewards (evaluation == 1)\n",
    "eval_df = rewards_df[rewards_df[\"evaluation\"] == 1]\n",
    "eval_grouped = eval_df.groupby(\"episode\")[\"reward\"].mean()\n",
    "\n",
    "axes[1].plot(eval_grouped.index, eval_grouped.values, color=\"tab:green\")\n",
    "axes[1].set_title(\"Evaluation Reward per Episode (Unit: pp_6)\")\n",
    "axes[1].set_xlabel(\"Episode\")\n",
    "axes[1].set_ylabel(\"Average Reward\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ab867",
   "metadata": {},
   "source": [
    "**What This Shows**\n",
    "\n",
    "* **Training curve**: Captures learning progress with exploration noise.\n",
    "* **Evaluation curve**: Tracks the performance of the evaluation/validation run without noise, which is performed every `validation_episodes_interval` steps, as defined in the `learning_config`. \n",
    "\n",
    "This plot provides insight into:\n",
    "\n",
    "* How well the agent is improving over time.\n",
    "* Whether learning has converged or stagnated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf740050",
   "metadata": {},
   "source": [
    "### 7.4 Summary\n",
    "\n",
    "* You have now run your **first complete training loop** in ASSUME.\n",
    "* The learning configuration defines all key training parametersâ€”review them carefully.\n",
    "* After training, rewards from `rl_params` allow you to inspect and validate agent behavior.\n",
    "* The separation of **training** and **evaluation** rewards is key to understanding generalization.\n",
    "\n",
    "\n",
    "In the next chapter, you may proceed to analyze simulation outcomes in greater detail (e.g., market prices, total costs, capacity dispatch), or compare different agent configurations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510ddc5",
   "metadata": {},
   "source": [
    "## 8. Analyzing Strategic Bidding Behavior\n",
    "\n",
    "Now that your agent has completed training, we shift our focus to a critical and more insightful question:\n",
    "\n",
    "> **What did the agent actually learn?**\n",
    "\n",
    "This chapter analyzes the **actual bids submitted by the agent** and evaluates whether the agent developed a **strategic bidding behavior**â€”especially in the context of market power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a365517",
   "metadata": {},
   "source": [
    "### 8.1. Background: Market Setup\n",
    "\n",
    "This simulation is based on **example case 1** from the following study:\n",
    "\n",
    "**\\[1]** Harder, N.; Qussous, R.; Weidlich, A.\n",
    "*Fit for purpose: Modeling wholesale electricity markets realistically with multi-agent deep reinforcement learning*.\n",
    "Energy and AI, 2023, 14:100295.\n",
    "[https://doi.org/10.1016/j.egyai.2023.100295](https://doi.org/10.1016/j.egyai.2023.100295)\n",
    "\n",
    "In this case:\n",
    "\n",
    "* The market contains **one large RL agent**: `pp_6`.\n",
    "* The agent has enough capacity to influence the market clearing price.\n",
    "* It is allowed to bid freely to maximize its own reward (profit, adjusted by regret).\n",
    "\n",
    "**Marginal Cost Structure:**\n",
    "\n",
    "| Unit      | Marginal Cost (â‚¬/MWh) |\n",
    "| --------- | --------------------- |\n",
    "| pp\\_6     | **55.7**              |\n",
    "| Next unit | **85.7**              |\n",
    "\n",
    "A profit-maximizing agent **with market power** would learn to bid just below the next most expensive unitâ€”in this case, somewhere just below **85.7 â‚¬/MWh**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2e0c2",
   "metadata": {},
   "source": [
    "### 8.2. Extract and Plot the Agent's Bids\n",
    "\n",
    "We will extract the bids submitted by `pp_6` from the `market_orders` table and plot them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "engine = create_engine(\"sqlite:///local_db/assume_db.db\")\n",
    "\n",
    "# Query bids from pp_6 in simulation example_02a_base and market EOM\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    start_time AS time,\n",
    "    price,\n",
    "    accepted_price,\n",
    "    unit_id,\n",
    "    simulation\n",
    "FROM market_orders\n",
    "WHERE simulation = 'example_02a_base'\n",
    "  AND unit_id = 'pp_6'\n",
    "  AND market_id = 'EOM'\n",
    "ORDER BY start_time\n",
    "\"\"\"\n",
    "\n",
    "# Load results into DataFrame\n",
    "bids_df = pd.read_sql(sql, engine)\n",
    "bids_df[\"time\"] = pd.to_datetime(bids_df[\"time\"])\n",
    "\n",
    "# Define marginal cost boundaries\n",
    "mc_pp6 = 55.7\n",
    "mc_next = 85.7\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(bids_df[\"time\"], bids_df[\"price\"], label=\"pp_6 Bid Price\", color=\"tab:blue\")\n",
    "\n",
    "# Reference lines for marginal cost and competitive threshold\n",
    "plt.axhline(\n",
    "    mc_pp6,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"pp_6 Marginal Cost (55.7 â‚¬)\",\n",
    ")\n",
    "plt.axhline(\n",
    "    mc_next,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Next Unit's Marginal Cost (85.7 â‚¬)\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    bids_df[\"time\"],\n",
    "    bids_df[\"accepted_price\"],\n",
    "    label=\"Accepted Price\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "\n",
    "plt.title(\"Bidding Behavior of RL Agent (pp_6)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Bid Price (â‚¬/MWh)\")\n",
    "plt.legend()\n",
    "plt.ylim(30, 100)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fdcd2",
   "metadata": {},
   "source": [
    "### 8.4. What Does This Show?\n",
    "\n",
    "The plot typically reveals:\n",
    "\n",
    "* The agent **almost never bids at its own marginal cost**.\n",
    "* Instead, its bid prices **cluster below 85.7 â‚¬/MWh**, indicating that it has learned to:\n",
    "\n",
    "  * Underbid the next unit to secure dispatch.\n",
    "  * **Exploit its market position** to maximize profit rather than behave as a price-taker.\n",
    "* This is consistent with **strategic bidding behavior** in oligopolistic market settings.\n",
    "\n",
    "This outcome aligns with the findings from \\[1], confirming that **deep RL agents can learn to exercise market power** when not explicitly restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d45dc",
   "metadata": {},
   "source": [
    "### 8.5. Summary\n",
    "\n",
    "* The RL agent did not simply mimic marginal cost biddingâ€”it learned to **optimize strategically**.\n",
    "* The bid curve confirms that **market power was exercised** by bidding just under the next marginal unit.\n",
    "* This is a core feature of realistic market modeling, and shows the value of RL in economic simulations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b67630",
   "metadata": {},
   "source": [
    "## 9. Summary and Outlook\n",
    "\n",
    "### 9.1. What You Built\n",
    "\n",
    "Over the course of this tutorial, you developed a complete **RL bidding strategy** for an electricity market agent in the **ASSUME** framework. You constructed and trained a fully functional learning agent that can:\n",
    "\n",
    "* Observe the market and its own internal state.\n",
    "* Make strategic bidding decisions based on learned policy.\n",
    "* Receive reward signals and adapt its behavior accordingly.\n",
    "* Exploit market dynamics, including market power, when permitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3e528",
   "metadata": {},
   "source": [
    "### 9.2 What You Learned\n",
    "\n",
    "Throughout the tutorial, you explored the **full learning pipeline** in a realistic electricity market context:\n",
    "\n",
    "* How to construct **observations** from both global forecasts and unit-specific state.\n",
    "* How to define **actions** and handle **exploration**, including guided exploration around meaningful economic baselines.\n",
    "* How to design and normalize a **reward function** that balances realized profit with opportunity cost.\n",
    "* How to run a simulation using **multi-agent DRL** and analyze its outcomes.\n",
    "* How to evaluate **bidding behavior** and interpret **economic strategies** emerging from the agentâ€™s learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc118c",
   "metadata": {},
   "source": [
    "### 9.3 What You Can Try Next\n",
    "\n",
    "Your implementation is modular and extensible. Here are several directions you can explore on your own:\n",
    "\n",
    "1. **Adjust Learning Parameters**\n",
    "\n",
    "Experiment with:\n",
    "\n",
    "* `learning_rate`, `gamma`, `noise_sigma`, `episodes_collecting_initial_experience`\n",
    "* `validation_episodes_interval`, `train_freq`, or `gradient_steps`\n",
    "\n",
    "Observe how these changes affect convergence, stability, and bidding behavior.\n",
    "\n",
    "2. **Try Different Scenarios**\n",
    "\n",
    "* Run **`example_02b`** or **`example_02c`**:\n",
    "\n",
    "  * `02b`: Introduces moderate competition with several learning agents.\n",
    "  * `02c`: Contains many learning agents, simulating a highly competitive environment.\n",
    "* Compare bidding behavior and reward dynamics across settings.\n",
    "\n",
    "3. **Dive into Other Tutorials**\n",
    "\n",
    "* If you are interested in the general algorithm behind the MADDPG and how it is integrated into ASSUME look into [04a_RL_algorithm_example](./04a_reinforcement_learning_algorithm_example.ipynb) \n",
    "* In the small example we could see what the a good bidding behavior of the agent might be and, hence, can judge learning easily, but what if we model many agents in new simulations? We provide explainable RL mechanisms in another tutorial for you to dive into [09_example_Sim_and_xRL](./09_example_Sim_and_xRL.ipynb) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
