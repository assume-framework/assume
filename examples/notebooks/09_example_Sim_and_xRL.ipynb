{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efad297",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "# 9. Explainable Reinforcement Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db7561",
   "metadata": {
    "id": "fb3aa803"
   },
   "source": [
    "Welcome to this tutorial on **Explainable Reinforcement Learning (XRL)**! In this guide, we will explore how to interpret and explain the decisions made by reinforcement learning agents using the SHAP (SHapley Additive exPlanations) library. Through a practical example involving a simulation in a reinforcement learning setting, we'll demonstrate how to compute and visualize feature attributions for the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a32c0",
   "metadata": {
    "id": "0d793362"
   },
   "source": [
    "**Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c2689",
   "metadata": {
    "id": "87bdf688"
   },
   "source": [
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "    1.1. [Multi-Agent Deep Reinforcement Learning with Market Splitting](#1.1.-running-a-madrl-simulation)\n",
    "    \n",
    "\n",
    "2. [Explainable AI and SHAP Values](#2.-explainable-ai-and-shap-values)\n",
    "\n",
    "    2.1 [Understanding Explainable AI](#2.1.-understanding-explainable-ai)\n",
    "\n",
    "    2.2 [Introduction to SHAP Values](#2.2.-introduction-to-shap-values)\n",
    "\n",
    "3. [Calculating SHAP Values](#3.-calculating-shap-values)\n",
    "\n",
    "    3.1. [Loading and Preparing Data](#3.1.-loading-and-preparing-data)\n",
    "\n",
    "    3.2. [Creating a SHAP Explainer](#3.2.-creating-a-shap-explainer)\n",
    "\n",
    "4. [Visualizing SHAP Values](#4.-visualizing-shap-values)\n",
    "5. [Conclusion](#5.-conclusion)\n",
    "6. [Additional Resources](#6.-additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5d28",
   "metadata": {
    "id": "5e8c7fec"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ef10",
   "metadata": {
    "id": "06e91420"
   },
   "source": [
    "Reinforcement Learning (RL) has achieved remarkable success in various domains, such as game playing, robotics, and autonomous systems. However, RL models, particularly those using deep neural networks, are often seen as **black boxes** due to their complex architectures and non-linear computations. This opacity makes it challenging to understand and trust the decisions made by RL agents, especially in critical applications where transparency is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fcbef6",
   "metadata": {
    "id": "47b1e7ab"
   },
   "source": [
    "**Explainable Reinforcement Learning (XRL)** aims to bridge this gap by providing insights into an agent's decision-making process. By leveraging explainability techniques, we can:\n",
    "- Interpret the actions of an RL agent.\n",
    "- Understand the influence of input features on decisions.\n",
    "- Potentially improve the model's performance, fairness, and transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3235",
   "metadata": {
    "id": "ec0717c1"
   },
   "source": [
    "In this tutorial, we will demonstrate how to apply SHAP values to a trained actor neural network in an RL framework to explain the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01746",
   "metadata": {
    "id": "0d59bb0a",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.1. Running a MADRL Simulation\n",
    "\n",
    "In this tutorial, we will simulate RL agents using a Multi-Agent Deep Reinforcement Learning (MADRL) approach. The agents operate in a market-splitting environment where they interact and learn optimal strategies over time. Here’s a breakdown of the key components:\n",
    "\n",
    "- **Observations**: Each agent receives observations, including market forecasts, unit-specific information, and past actions.\n",
    "- **Actions**: The agents decide on bidding strategies, such as bid prices for both inflexible and flexible capacities.\n",
    "- **Rewards**: The agents are rewarded based on profits and opportunity costs, helping them learn optimal bidding strategies.\n",
    "- **Algorithm**: We utilize a multi-agent version of the TD3 (Twin Delayed Deep Deterministic Policy Gradient) algorithm, which ensures stable learning even in non-stationary environments.\n",
    "\n",
    "For a more detailed explanation of the RL configurations, refer to the two DRL tutorials on the [learning algorithm](04a_reinforcement_learning_algorithm_example.ipynb) and [RL bidding strategies](04b_reinforcement_learning_example.ipynb).\n",
    "\n",
    "### Key Aspects of the Simulation\n",
    "\n",
    "Agents require **observations** to make informed decisions, which include:\n",
    "\n",
    "- **Residual Load Forecast**: Forecasted net demand (electricity demand minus renewable generation) over the next 12 hours.\n",
    "- **Price Forecast**: Forecasted market prices over the next 12 hours.\n",
    "- **Historical Market Prices**: Actual market price outcomes from the last 12 hours.\n",
    "- **Marginal Cost**: The current marginal cost of operating the agent's power-generating unit.\n",
    "- **Previous Output**: The agent’s dispatched capacity (energy production) from the previous time step.\n",
    "\n",
    "### Agent Actions\n",
    "\n",
    "The action space for the agents is two-dimensional and consists of:\n",
    "\n",
    "- **Bid Price for Inflexible Capacity (p_inflex)**: The price at which the agent offers its minimum power output (must-run capacity) to the market.\n",
    "- **Bid Price for Flexible Capacity (p_flex)**: The price for the additional capacity above the minimum output that the agent can flexibly adjust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b17e49",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "#### 1.1.1 Install ASSUME and Required Packages\n",
    "\n",
    "In this section, we will install the necessary packages to run the **ASSUME framework** along with other dependencies.\n",
    "The process is similar to the other tutorials on ASSUME.\n",
    "\n",
    "The following commands will install ASSUME and its dependencies for reinforcement learning, along with additional libraries such as Plotly for visualization.\n",
    "Make sure to install these before running the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647079b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ee220130",
    "outputId": "ffd98b47-2b07-41cd-dfe4-ff0381571825"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check if 'google.colab' is available\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install 'assume-framework[learning]'\n",
    "    !git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0\n",
    "!pip install plotly\n",
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28add86",
   "metadata": {},
   "source": [
    "Define paths to differentiate between Colab or local usage.\n",
    "If you're running this on Google Colab, the paths might differ slightly from your local environment.\n",
    "You can configure the paths accordingly based on where you're executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c91474",
   "metadata": {
    "id": "e62e00c9"
   },
   "outputs": [],
   "source": [
    "# import other necessary libraries\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# import os for file operations\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import plotly for visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# import yaml for reading and writing YAML files\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95724ea",
   "metadata": {
    "id": "636ea9ae"
   },
   "source": [
    "#### 1.1.2 Create and Load Example Files from Market Splitting Tutorial\n",
    "\n",
    "To define the RL Agent, we need to obtain the results from the **Market Zone Splitting** tutorial.\n",
    "This tutorial provides essential data that the RL agent will use for decision-making.\n",
    "\n",
    "If you are working in **Google Colab**, execute the following cells to download and run the necessary notebook automatically. \n",
    "If you are working on your **local machine**, simply open the respective [tutorial notebook](08_market_zone_coupling.ipynb) and execute it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdfe19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # For execution in Google Colab:\n",
    "    %cd assume-repo/examples/notebooks/\n",
    "\n",
    "    # Execute the Market Zone Splitting tutorial:\n",
    "    !jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=60 --output output.ipynb 08_market_zone_coupling.ipynb\n",
    "\n",
    "    # Return to content folder (for Colab):\n",
    "    %cd /content\n",
    "\n",
    "    # Copy inputs directory to the working folder (for Colab):\n",
    "    !mkdir -p inputs/tutorial_09\n",
    "    !cp -r assume-repo/examples/notebooks/inputs/tutorial_08/* inputs/tutorial_09/ #TODO: copy to input_dir folder/rename\n",
    "\n",
    "    input_dir = os.path.join(\"inputs\", \"tutorial_09\")\n",
    "else:\n",
    "    # Define the input directory\n",
    "    tutorial_08_dir = os.path.join(\"inputs\", \"tutorial_08\")\n",
    "    input_dir = os.path.join(\"inputs\", \"tutorial_09\")\n",
    "    shutil.copytree(tutorial_08_dir, input_dir, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DataFrames from CSV files\n",
    "powerplant_units = pd.read_csv(os.path.join(input_dir, \"powerplant_units.csv\"))\n",
    "demand_df = pd.read_csv(os.path.join(input_dir, \"demand_df.csv\"))\n",
    "\n",
    "print(\"Input CSV files have been read from 'inputs/tutorial_09'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc51a8",
   "metadata": {
    "id": "6985289b"
   },
   "source": [
    "#### 1.1.3 Transform the Scenario into a Learning Example\n",
    "\n",
    "The following cells show how we can convert any pre-configured scenario in ASSUME into a learning example.\n",
    "\n",
    "**Define a Learning Power Plant**\n",
    "\n",
    "In this example, we place a learning nuclear power plant in the southern zone. This plant has five times the maximum power of a typical plant, which allows us to create a scenario where its actions have a noticeable impact on market prices. Additionally, we remove multiple units in the southern zone so that scarcity is caused and demand cannot be fulfilled during some periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4153fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "b205256f",
    "outputId": "b9bb887b-f534-4a50-dd5b-229be1012600"
   },
   "outputs": [],
   "source": [
    "# Create scarcity in southern Germany by limiting the number of power plants\n",
    "powerplant_units = powerplant_units[:20]\n",
    "\n",
    "# Assign the RL-controlled power plant and give it market power\n",
    "powerplant_units.loc[19, \"bidding_zonal\"] = \"powerplant_energy_learning\"\n",
    "powerplant_units.loc[19, \"max_power\"] = 5000  # Set maximum power to 5000 MW\n",
    "\n",
    "# Assign a specific RL unit operator to the plant\n",
    "powerplant_units.loc[19, \"unit_operator\"] = \"Operator-RL\"\n",
    "\n",
    "# Set the 'name' column as the index\n",
    "powerplant_units.set_index(\"name\", inplace=True, drop=True)\n",
    "\n",
    "# Save the updated power plant units to a CSV file\n",
    "powerplant_units.to_csv(input_dir + \"/powerplant_units.csv\")\n",
    "\n",
    "# Show the last 10 entries\n",
    "powerplant_units.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a22a61",
   "metadata": {
    "id": "cce0e8b4"
   },
   "source": [
    "**Configure Learning Hyperparameters in YAML**\n",
    "\n",
    "The following YAML configuration contains the learning-specific hyperparameters that will guide the RL agent's training process. Below is a brief description of these hyperparameters:\n",
    "\n",
    "- **continue_learning** (`False`): \n",
    "  - Whether to continue training from a previously saved state or start fresh.\n",
    "\n",
    "- **max_bid_price** (`100`): \n",
    "  - The maximum allowable bid price for the agent, used to scale the actor's output.\n",
    "\n",
    "- **algorithm** (`\"matd3\"`): \n",
    "  - The learning algorithm to be used, in this case `MATD3` (Multi-Agent Twin Delayed Deep Deterministic Policy Gradient).\n",
    "\n",
    "- **learning_rate** (`0.001`): \n",
    "  - The rate at which the model’s parameters are updated during training.\n",
    "\n",
    "- **training_episodes** (`50`): \n",
    "  - The total number of episodes for training the agent.\n",
    "\n",
    "- **episodes_collecting_initial_experience** (`3`): \n",
    "  - Number of episodes dedicated to collecting initial experience before actual training begins, during which the agent follows a random policy.\n",
    "\n",
    "- **train_freq** (`\"4h\"`): \n",
    "  - Frequency of model training, in this case, every 4 hours.\n",
    "\n",
    "- **gradient_steps** (`4`): \n",
    "  - The number of gradient updates to perform at each training step.\n",
    "\n",
    "- **batch_size** (`256`): \n",
    "  - The size of the mini-batch used for training.\n",
    "\n",
    "- **gamma** (`0.99`): \n",
    "  - The discount factor for future rewards, balancing short-term vs. long-term reward importance.\n",
    "\n",
    "- **device** (`\"cpu\"`): \n",
    "  - The computational device for training. In this case, the CPU is used.\n",
    "\n",
    "- **noise_sigma** (`0.1`): \n",
    "  - The standard deviation of the exploration noise added to actions.\n",
    "\n",
    "- **noise_scale** (`1`) and **noise_dt** (`1`): \n",
    "  - Parameters controlling the scale and time step of the exploration noise. Since both are set to 1, no decay is applied.\n",
    "\n",
    "- **validation_episodes_interval** (`3`): \n",
    "  - The interval (in episodes) at which validation is performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c64dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c555ce9",
    "outputId": "473126ae-3c3e-4698-e3a5-347cc00e5108"
   },
   "outputs": [],
   "source": [
    "# YAML configuration for the RL training\n",
    "config = {\n",
    "    \"zonal_rl_case\": {\n",
    "        \"start_date\": \"2019-01-01 00:00\",\n",
    "        \"end_date\": \"2019-01-01 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 4,\n",
    "        \"markets_config\": {\n",
    "            \"zonal\": {\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 1, \"first_delivery\": \"1h\"}],\n",
    "                \"opening_frequency\": \"1h\",\n",
    "                \"opening_duration\": \"1h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"complex_clearing\",\n",
    "                \"additional_fields\": [\"bid_type\", \"node\"],\n",
    "                \"param_dict\": {\"network_path\": \".\", \"zones_identifier\": \"zone_id\"},\n",
    "            }\n",
    "        },\n",
    "        \"learning_config\": {\n",
    "            \"learning_mode\": True,\n",
    "            \"continue_learning\": False,\n",
    "            \"max_bid_price\": 100,\n",
    "            \"algorithm\": \"matd3\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"training_episodes\": 15,\n",
    "            \"episodes_collecting_initial_experience\": 3,\n",
    "            \"train_freq\": \"4h\",\n",
    "            \"gradient_steps\": 4,\n",
    "            \"batch_size\": 256,\n",
    "            \"gamma\": 0.99,\n",
    "            \"device\": \"cpu\",\n",
    "            \"noise_sigma\": 0.1,\n",
    "            \"noise_scale\": 1,\n",
    "            \"noise_dt\": 1,\n",
    "            \"validation_episodes_interval\": 3,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the configuration file\n",
    "config_path = os.path.join(input_dir, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cbdb4",
   "metadata": {
    "id": "3f0f38fb"
   },
   "source": [
    "In order to make this setup compatible with XRL, we need to enhance the logging of the learning process. \n",
    "ASSUME does not have this feature natively, so we will override some functions to enable this logging for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01977d5",
   "metadata": {
    "cellView": "form",
    "id": "201251c6"
   },
   "outputs": [],
   "source": [
    "# @title Overwrite run_learning function with enhanced logging\n",
    "# import required ASSUME modules\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.scenario.loader_csv import setup_world\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def run_learning(\n",
    "    world: World,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "    from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"assume.scenario.loader_csv\").setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    world.learning_role.rl_algorithm.initialize_policy()\n",
    "\n",
    "    # check if we already stored policies for this simulation\n",
    "    save_path = world.learning_role.learning_config.trained_policies_save_path\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        if world.learning_role.learning_config.continue_learning:\n",
    "            logger.warning(\n",
    "                f\"Save path '{save_path}' exists.\\n\"\n",
    "                \"You are in continue learning mode. New strategies may overwrite previous ones.\\n\"\n",
    "                \"It is recommended to use a different save path to avoid unintended overwrites.\\n\"\n",
    "                \"You can set 'trained_policies_save_path' in the config.\"\n",
    "            )\n",
    "            proceed = input(\n",
    "                \"Do you still want to proceed with the existing save path? (y/N) \"\n",
    "            )\n",
    "            if not proceed.lower().startswith(\"y\"):\n",
    "                raise AssumeException(\n",
    "                    \"Simulation aborted by user to avoid overwriting previous learned strategies. \"\n",
    "                    \"Consider setting a new 'simulation_id' or 'trained_policies_save_path' in the config.\"\n",
    "                )\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Save path '{save_path}' exists. Previous training data will be deleted to start fresh.\"\n",
    "            )\n",
    "            accept = input(\"Do you want to overwrite and start fresh? (y/N) \")\n",
    "            if accept.lower().startswith(\"y\"):\n",
    "                shutil.rmtree(save_path, ignore_errors=True)\n",
    "                logger.info(\n",
    "                    f\"Previous strategies at '{save_path}' deleted. Starting fresh training.\"\n",
    "                )\n",
    "            else:\n",
    "                raise AssumeException(\n",
    "                    \"Simulation aborted by user not to overwrite existing learned strategies. \"\n",
    "                    \"You can set a different 'simulation_id' or 'trained_policies_save_path' in the config.\"\n",
    "                )\n",
    "\n",
    "    # also remove tensorboard logs\n",
    "    tensorboard_path = f\"tensorboard/{world.scenario_data['simulation_id']}\"\n",
    "    if os.path.exists(tensorboard_path):\n",
    "        shutil.rmtree(tensorboard_path, ignore_errors=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=world.learning_role.learning_config.replay_buffer_size,\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "    }\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.learning_config.training_episodes,\n",
    "        world.learning_role.learning_config.validation_episodes_interval,\n",
    "    )\n",
    "\n",
    "    # Ensure training episodes exceed the sum of initial experience and one evaluation interval\n",
    "    min_required_episodes = (\n",
    "        world.learning_role.learning_config.episodes_collecting_initial_experience\n",
    "        + validation_interval\n",
    "    )\n",
    "\n",
    "    if world.learning_role.learning_config.training_episodes < min_required_episodes:\n",
    "        raise ValueError(\n",
    "            f\"Training episodes ({world.learning_role.training_episodes}) must be greater than the sum of initial experience episodes ({world.learning_role.episodes_collecting_initial_experience}) and evaluation interval ({validation_interval}).\"\n",
    "        )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.learning_config.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initialized learning role the needed information across episodes\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                episode=episode,\n",
    "            )\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store the entire buffer for xAI workflow\n",
    "        if episode == world.learning_role.learning_config.training_episodes:\n",
    "            export = inter_episodic_data[\"buffer\"].observations.tolist()\n",
    "\n",
    "            with open(\n",
    "                os.path.join(\n",
    "                    world.learning_role.learning_config.trained_policies_save_path,\n",
    "                    \"buffer_obs.json\",\n",
    "                ),\n",
    "                \"w\",\n",
    "            ) as f:\n",
    "                json.dump(export, f)\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.learning_config.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                evaluation_mode=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward(eval_episode)\n",
    "\n",
    "            if len(total_rewards) == 0:\n",
    "                raise AssumeException(\"No rewards were collected during evaluation run\")\n",
    "\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # save the policies after each episode in case the simulation is stopped or crashes\n",
    "        if (\n",
    "            episode\n",
    "            >= world.learning_role.learning_config.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.learning_config.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "    # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # Set 'trained_policies_load_path' to None in order to load the most recent policies,\n",
    "    # especially if previous strategies were loaded from an external source.\n",
    "    # This is useful when continuing from a previous learning session.\n",
    "    world.scenario_data[\"config\"][\"learning_config\"][\"trained_policies_load_path\"] = (\n",
    "        f\"{world.learning_role.learning_config.trained_policies_save_path}/avg_reward_eval_policies\"\n",
    "    )\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        terminate_learning=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa6b82",
   "metadata": {
    "id": "dcacfe26"
   },
   "source": [
    "**Run the Example Case**\n",
    "\n",
    "Now we run the example case as done previously in the market zone tutorial. \n",
    "The main difference here is that we call the `run_learning()` function, which iterates multiple times over the simulation horizon for reinforcement learning.\n",
    "\n",
    "> **Hint**: In Google Colab, long-running training sessions may occasionally **crash or disconnect** if the output console is flooded — for example, by verbose progress bars or print statements.  \n",
    "> To prevent this, you can suppress output during training using the following approach.\n",
    "\n",
    "1. **Import the required tools:**\n",
    "\n",
    "    ```python\n",
    "    from contextlib import redirect_stdout, redirect_stderr\n",
    "    import os\n",
    "    ```\n",
    "\n",
    "2. **Wrap the training phase with output redirection.**  \n",
    "   Insert the following lines **just before Step 4: _Run the training phase_**:\n",
    "\n",
    "    ```python\n",
    "    # Suppress output for the entire training process\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stdout(devnull), redirect_stderr(devnull):\n",
    "            # Your training function call goes here\n",
    "            train_agents(...)\n",
    "    ```\n",
    "\n",
    "> ✅ This redirects all `stdout` and `stderr` to `/dev/null`, preventing Colab from being overwhelmed by output and improving session stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c9334",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bfadf522",
    "outputId": "7c91ab13-a3c2-4e89-d8ac-d20be95391f6"
   },
   "outputs": [],
   "source": [
    "# Import necessary classes and functions from the ASSUME framework\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "\n",
    "# Define paths for input and output data\n",
    "csv_path = \"outputs\"\n",
    "\n",
    "# Define the data format and database URI for storing results\n",
    "# Use \"local_db\" for SQLite or \"timescale\" for TimescaleDB\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "data_format = \"local_db\"  # Options: \"local_db\" (SQLite) or \"timescale\" (TimescaleDB)\n",
    "\n",
    "np.random.seed(42)  # Set a random seed for reproducibility\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance with the specified database\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario configuration\n",
    "# - world: World instance\n",
    "# - inputs_path: Folder containing input data\n",
    "# - scenario: Scenario subfolder in inputs\n",
    "# - study_case: Which configuration (case) to use for the simulation\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=\"inputs\",\n",
    "    scenario=\"tutorial_09\",\n",
    "    study_case=\"zonal_rl_case\",\n",
    ")\n",
    "\n",
    "# If learning mode is enabled, run the reinforcement learning loop\n",
    "if world.learning_mode:\n",
    "    run_learning(world)\n",
    "\n",
    "# Run the simulation\n",
    "world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec3968",
   "metadata": {
    "id": "2194f71b"
   },
   "source": [
    "**Compare the Results**\n",
    "\n",
    "Next, we use the same code from the market zone tutorial to generate a Plotly graph displaying market clearing prices over time for each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762acdfe",
   "metadata": {
    "id": "bdb21cbe",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define the path to the simulation output directory\n",
    "output_dir = \"outputs/tutorial_09_zonal_rl_case\"\n",
    "market_meta_path = os.path.join(output_dir, \"market_meta.csv\")\n",
    "\n",
    "# Load the market metadata from the CSV file\n",
    "market_meta = pd.read_csv(market_meta_path, index_col=\"time\", parse_dates=True)\n",
    "market_meta = market_meta.drop(\n",
    "    columns=market_meta.columns[0]\n",
    ")  # Drop the first unnamed column\n",
    "\n",
    "# Extract unique zones from the \"node\" column\n",
    "zones = market_meta[\"node\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store clearing prices for each zone\n",
    "clearing_prices_df = pd.DataFrame()\n",
    "\n",
    "# Populate the DataFrame with clearing prices for each zone\n",
    "for zone in zones:\n",
    "    zone_data = market_meta[market_meta[\"node\"] == zone][[\"price\"]]\n",
    "    zone_data = zone_data.rename(columns={\"price\": f\"{zone}_price\"})\n",
    "    clearing_prices_df = (\n",
    "        pd.merge(\n",
    "            clearing_prices_df,\n",
    "            zone_data,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"outer\",\n",
    "        )\n",
    "        if not clearing_prices_df.empty\n",
    "        else zone_data\n",
    "    )\n",
    "\n",
    "# Sort the DataFrame by time\n",
    "clearing_prices_df = clearing_prices_df.sort_index()\n",
    "\n",
    "# Initialize the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot clearing prices for each zone\n",
    "for zone in zones:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=clearing_prices_df.index,\n",
    "            y=clearing_prices_df[f\"{zone}_price\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"{zone} - Simulation\",\n",
    "            line=dict(width=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the layout for better aesthetics and interaction\n",
    "fig.update_layout(\n",
    "    title=\"Clearing Prices per Zone Over Time: Simulation Results\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Clearing Price (EUR/MWh)\",\n",
    "    legend_title=\"Market Zones\",\n",
    "    xaxis=dict(\n",
    "        tickangle=45,  # Rotate x-axis labels for readability\n",
    "        type=\"date\",  # Ensure x-axis is treated as dates\n",
    "    ),\n",
    "    hovermode=\"x unified\",  # Unified hover to compare values across zones at the same time\n",
    "    template=\"plotly_white\",  # Use a clean white background\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930f3e4",
   "metadata": {},
   "source": [
    "Two **main observations** can be made:\n",
    "\n",
    "1. Compared to the previous tutorial 08, the learning unit acts as a price making unit and exploits market power during the first 9 hours of the day. It drives the market clearing price up to almost 100 €/MWh (which is the maximum bid price in the learning configuration). Before, it was always below 50 €/MWh. \n",
    "\n",
    "2. During the second period, the situations of scarcity appear and demand cannot be fulfilled. The maximum price of demand units of 3.000 €/MWh is reached.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed563ab7",
   "metadata": {
    "id": "e77c41cf"
   },
   "source": [
    "## 2. Explainable AI and SHAP Values <a name=\"explainable-ai-and-shap-values\"></a>\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee9432",
   "metadata": {
    "id": "cbf18570"
   },
   "source": [
    "To follow along with this tutorial, we need some additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a11a6",
   "metadata": {
    "id": "a5ac592c"
   },
   "source": [
    "- `matplotlib`\n",
    "- `shap`\n",
    "- `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e097c0",
   "metadata": {
    "id": "ae266ecb",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install shap==0.50.0\n",
    "!pip install scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3463ee2",
   "metadata": {
    "id": "5069ff93"
   },
   "source": [
    "### 2.1. Understanding Explainable AI\n",
    "Explainable AI (XAI) refers to techniques and methods that make the behavior and decisions of AI systems understandable to humans. In the context of complex models like deep neural networks, XAI helps to:\n",
    "- **Increase Transparency**: Providing insights into how models make decisions.\n",
    "- **Build Trust**: Users and stakeholders can trust AI systems if they understand them.\n",
    "- **Ensure Compliance**: Regulatory requirements often demand explainability.\n",
    "- **Improve Models**: Identifying weaknesses or biases in models.\n",
    "\n",
    "\n",
    "### 2.2. Introduction to SHAP Values\n",
    "Shapley values are a method from cooperative game theory used to explain the contribution of each feature to the prediction of a machine learning model, such as a neural network. They provide an interpretability technique by distributing the \"payout\" (the prediction) among the input features, attributing the importance of each feature to the prediction.\n",
    "\n",
    "For a given prediction, the Shapley value of a feature represents the average contribution of that feature to the prediction, considering all possible combinations of other features.\n",
    "\n",
    "1. **Marginal Contribution**:\n",
    "   The marginal contribution of a feature is the difference between the prediction with and without that feature.\n",
    "\n",
    "2. **Average over all Subsets**:\n",
    "   The Shapley value is calculated by averaging the marginal contributions over all possible subsets of features.\n",
    "\n",
    "The formula for the Shapley value of feature $i$ is:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!} \\cdot \\left( f(S \\cup \\{i\\}) - f(S) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the set of all features.\n",
    "- $S$ is a subset of features.\n",
    "- $f(S)$ is the model’s prediction when using only the features in subset $S$.\n",
    "\n",
    "\n",
    "The `shap` library is a popular tool for computing Shapley values for machine learning models, including neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Why Use SHAP in RL?\n",
    "- Model-Agnostic: Applicable to any machine learning model, including neural networks.\n",
    "- Local Explanations: Provides explanations for individual predictions (actions).\n",
    "- Consistency: Ensures that features contributing more to the prediction have higher Shapley values.\n",
    "\n",
    "\n",
    "Properties of SHAP:\n",
    "1. Local Accuracy: The sum of Shapley values equals the difference between the model output and the expected output.\n",
    "2. Missingness: Features not present in the model have zero Shapley value.\n",
    "3. Consistency: If a model changes so that a feature contributes more to the prediction, the Shapley value of that feature should not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade4682",
   "metadata": {
    "id": "21d49573"
   },
   "source": [
    "## 3. Calculating SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3758646",
   "metadata": {
    "id": "d16712fc"
   },
   "source": [
    "We will work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5898",
   "metadata": {
    "id": "2c0c1262"
   },
   "source": [
    "- **Observations (`input_data`)**: These are the inputs to our actor neural network, representing the state of the environment.\n",
    "- **Trained Actor Model**: A neural network representing the decision making of one RL power plant that outputs actions based on the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77094d",
   "metadata": {
    "id": "2fb6fc14"
   },
   "source": [
    "Our goal is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2fbc9",
   "metadata": {
    "id": "e0b69db6"
   },
   "source": [
    "1. Load the observations and the trained actor model.\n",
    "2. Use the model to predict actions.\n",
    "3. Apply SHAP to explain the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e0416",
   "metadata": {
    "id": "f870b3e8"
   },
   "source": [
    "### 3.1. Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5965",
   "metadata": {
    "id": "aaa7c3d3"
   },
   "source": [
    "First, let's load the necessary libraries and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6f33b",
   "metadata": {
    "id": "b6ee4f28"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch as th\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "th.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f095c",
   "metadata": {
    "id": "ddfe95d9"
   },
   "source": [
    "We define a utility function to load observations and input data from a specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89d972",
   "metadata": {
    "id": "44862f06"
   },
   "outputs": [],
   "source": [
    "# @title Load observations function\n",
    "\n",
    "\n",
    "def load_observations(path, feature_names):\n",
    "    # Load observations\n",
    "    obs_path = f\"{path}/buffer_obs.json\"\n",
    "\n",
    "    with open(obs_path) as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Convert the list of lists into a 2D numpy array\n",
    "    input_data = np.array(json_data)\n",
    "    input_data = np.squeeze(input_data)\n",
    "\n",
    "    # filter out arrays where all value are 0\n",
    "    input_data = input_data[~np.all(input_data == 0, axis=1)]\n",
    "\n",
    "    return pd.DataFrame(input_data, columns=feature_names), input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f6681",
   "metadata": {
    "id": "6c5f1986"
   },
   "source": [
    "**Load Observations and Input Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b084bb",
   "metadata": {
    "id": "36ae8f9e"
   },
   "source": [
    "Load the observations and input data using the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d778759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to extra logged observation values\n",
    "path = input_dir + \"/learned_strategies/tutorial_09_zonal_rl_case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e142be2",
   "metadata": {
    "id": "d522969d"
   },
   "outputs": [],
   "source": [
    "# Define feature names (replace with actual feature names)\n",
    "# make columns names\n",
    "names_1 = [\"residual load forecast t+\" + str(x) for x in range(1, 13)]\n",
    "names_2 = [\"price forecast t+\" + str(x) for x in range(1, 13)]\n",
    "names_3 = [\"historical prices t+\" + str(x) for x in range(1, 13)]\n",
    "feature_names = (\n",
    "    names_1 + names_2 + names_3 + [\"total capacity t-1\"] + [\"marginal costs t-1\"]\n",
    ")\n",
    "\n",
    "df_obs, input_data = load_observations(path, feature_names)\n",
    "\n",
    "df_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add0715",
   "metadata": {
    "id": "5d8b9dcf"
   },
   "source": [
    "**Load the Trained Actor Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e67d",
   "metadata": {
    "id": "b1b50488"
   },
   "source": [
    "We initialize and load the trained actor neural network. Therefore, we define the actor neural network class that will be used to predict actions based on observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca85e13",
   "metadata": {
    "id": "4da4de57"
   },
   "outputs": [],
   "source": [
    "from assume.reinforcement_learning.neural_network_architecture import MLPActor\n",
    "\n",
    "# Initialize the model\n",
    "obs_dim = len(feature_names)\n",
    "act_dim = 2  # Adjust if your model outputs a different number of actions\n",
    "model = MLPActor(obs_dim=obs_dim, act_dim=act_dim, float_type=th.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3b7e6",
   "metadata": {
    "id": "37adecfa"
   },
   "outputs": [],
   "source": [
    "# which actor is the RL actor\n",
    "ACTOR_NUM = len(powerplant_units)  # 20\n",
    "\n",
    "# Path to actor we want to analyse\n",
    "actor_path = os.path.join(\n",
    "    input_dir,\n",
    "    f\"learned_strategies/tutorial_09_zonal_rl_case/avg_reward_eval_policies/actors/actor_Unit {ACTOR_NUM}.pt\",\n",
    ")\n",
    "\n",
    "# Load the trained model parameters\n",
    "model_state = th.load(actor_path, map_location=th.device(\"cpu\"))\n",
    "model.load_state_dict(model_state[\"actor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d43a1b",
   "metadata": {
    "id": "d4a63712"
   },
   "source": [
    "Get the actions base on observation tensor we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507d331",
   "metadata": {
    "id": "e6460cfb"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "for obs in input_data:\n",
    "    obs_tensor = th.tensor(obs, dtype=th.float)\n",
    "    action = model(obs_tensor)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579665bc",
   "metadata": {
    "id": "ddd1ab1e"
   },
   "source": [
    "## 3.2. Creating a SHAP Explainer\n",
    "\n",
    "In the next step, we create the Shap explainer. In this example, we facilitate the Kernel Shap method. You can easily switch it out for Deep Shap. The SHAP Kernel Explainer is a model-agnostic method for computing SHAP values, which can be applied to any machine learning model, including black-box models like neural networks, decision trees, or ensemble models. It uses a simplified linear approximation based on the Kernel SHAP method to estimate the SHAP values, allowing you to interpret how each feature contributes to a particular model’s prediction. Basically the SHAP Kernel Explainer builds a weighted linear regression model around each prediction, using different combinations (coalitions) of input features to simulate their presence or absence. This results in SHAP values that represent the marginal contribution of each feature.\n",
    "\n",
    "As we fit a linear regression, we split the observatoin and action data into test and train data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0758eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Title Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_data, actions, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to tensors\n",
    "y_train = th.stack(y_train)\n",
    "y_test = th.stack(y_test)\n",
    "\n",
    "X_train_tensor = th.tensor(X_train, dtype=th.float32)\n",
    "y_train_tensor = th.tensor(y_train, dtype=th.float32)\n",
    "X_test_tensor = th.tensor(X_test, dtype=th.float32)\n",
    "y_test_tensor = th.tensor(y_test, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa45f",
   "metadata": {
    "id": "ae7b108b",
    "lines_to_next_cell": 2
   },
   "source": [
    "We define a prediction function compatible with SHAP and create a Kernel SHAP explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e12192",
   "metadata": {
    "id": "6d9be211"
   },
   "outputs": [],
   "source": [
    "# @ Title Define a prediction function for generating actions for SHAP Explainer\n",
    "def model_predict(X):\n",
    "    X_tensor = th.tensor(X, dtype=th.float32)\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        return model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a32f41",
   "metadata": {
    "id": "84bb96cf"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP Kernel Explainer\n",
    "explainer = shap.KernelExplainer(model_predict, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279910b",
   "metadata": {
    "id": "2a7929e4"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ca286",
   "metadata": {
    "id": "c1f3d550"
   },
   "source": [
    "## 4. Visualizing SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cecbfe",
   "metadata": {
    "id": "3a0f0cbe"
   },
   "source": [
    "We generate summary plots to visualize feature importance for each output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3d533",
   "metadata": {
    "id": "2e318a5b"
   },
   "outputs": [],
   "source": [
    "# Summary plot for the first output dimension\n",
    "shap.summary_plot(shap_values[:, :, 0], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 0, p_inflex\")\n",
    "plt.show()\n",
    "\n",
    "# Summary plot for the second output dimension\n",
    "shap.summary_plot(shap_values[:, :, 1], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 1, p_flex\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[:, :, 0],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 0\",\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[:, :, 1],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ebfb",
   "metadata": {
    "id": "9a888f8b"
   },
   "source": [
    "The SHAP summary plots show the impact of each feature on the model's predictions for each output dimension (action). Features with larger absolute SHAP values have a more significant influence on the decision-making process of the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555afa84",
   "metadata": {
    "id": "c6c4ce8c"
   },
   "source": [
    "- **Positive SHAP Value**: Indicates that the feature contributes positively to the predicted action value.\n",
    "- **Negative SHAP Value**: Indicates that the feature contributes negatively to the predicted action value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc307ed",
   "metadata": {
    "id": "86545200"
   },
   "source": [
    "By analyzing these plots, we can identify which features are most influential and understand how changes in feature values affect the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967edf9",
   "metadata": {},
   "source": [
    "**Possible interpretations and underlying relationships**\n",
    "\n",
    "For example, a different bidding behavior for inflexible and flexible bids is shown. \n",
    "- The **price forecast of the next hour** is important for both output action values.\n",
    "- For the **inflexible** bid price, a **positive relationship** is observed: higher price forecast values at t+1 lead to higher inflex bid prices, lower price forecasts at t+1 lead to lower bid prices.\n",
    "- The **opposite** is apparent **for** the **flexible** bid price: higher price forecasts for t+1 lead to lower flex bid prices, and the other way round. \n",
    "\n",
    "(One could speculate that the must run capacity under competition should get sold at any cost. Whereas during periods of scarcity, the confidence of being in the money leads to higher inflex bid prices. For the bid price of flexible capacity, acting as price making units is actively changing the market clearing price and, thus, profits - possibly driving the price up even more in periods of lower forecasted prices. When the price forecast is high - indicating scarcity -, then the own bidding price does not have a large impact.)\n",
    "\n",
    "Additionally, the importance of forecasted prices further in the future (t+7 - t+12) seems to be high. Isn't this counterintuitive?\n",
    "\n",
    "Have fun exploring, detective!🔍 \n",
    "(Hint: Take a look again at the observations in [Section 3.1.](#3.1.-loading-and-preparing-data) May this just be a modeling artefact?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b57e2",
   "metadata": {
    "id": "06f3977c"
   },
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79634a",
   "metadata": {
    "id": "dadd0a0c"
   },
   "source": [
    "In this tutorial, we've demonstrated how to apply SHAP to a reinforcement learning agent to explain its decision-making process. By interpreting the SHAP values, we gain valuable insights into which features influence the agent's actions, enhancing transparency and trust in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0035820",
   "metadata": {
    "id": "37633c16"
   },
   "source": [
    "Explainability is crucial, especially when deploying RL agents in real-world applications where understanding the rationale behind decisions is essential for safety, fairness, and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c10373",
   "metadata": {
    "id": "8735d66f"
   },
   "source": [
    "## 6. Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80754bea",
   "metadata": {
    "id": "d6b0332f"
   },
   "source": [
    "- **SHAP Documentation**: [https://shap.readthedocs.io/en/latest/](https://shap.readthedocs.io/en/latest/)\n",
    "- **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "- **Reinforcement Learning Introduction**: [Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- **Interpretable Machine Learning Book**: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873097",
   "metadata": {
    "id": "a8cdea5f"
   },
   "source": [
    "**Feel free to experiment with the code and explore different explainability techniques. Happy learning!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,outputId,colab,cellView,id,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
