{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efad297",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "# 9. Explainable Reinforcement Learning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db7561",
   "metadata": {
    "id": "fb3aa803"
   },
   "source": [
    "Welcome to this tutorial on **Explainable Reinforcement Learning (XRL)**! In this guide, we will explore how to interpret and explain the decisions made by reinforcement learning agents using the SHAP (SHapley Additive exPlanations) library. Through a practical example involving a simulation in a reinforcement learning setting, we'll demonstrate how to compute and visualize feature attributions for the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a32c0",
   "metadata": {
    "id": "0d793362"
   },
   "source": [
    "**Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c2689",
   "metadata": {
    "id": "87bdf688"
   },
   "source": [
    "1. [Introduction](#1-Introduction)\n",
    "\n",
    "    1.1. [Multi-Agent Deep Reinforcement Learning with Market Splitting](#11-multi-agent-deep-reinforcement-learning)\n",
    "\n",
    "2. [Explainable AI and SHAP Values](#2-explainable-ai-and-shap-values)\n",
    "\n",
    "    2.1 Understanding Explainable AI\n",
    "\n",
    "    2.2 Introduction to SHAP Values\n",
    "\n",
    "3. [Calculating SHAP Values](#3-calculating-shap-values)\n",
    "\n",
    "    3.1. [Loading and Preparing Data](#loading-and-preparing-data)\n",
    "\n",
    "    3.2. [Creating a SHAP Explainer](#32-creating-a-shap-explainer)\n",
    "\n",
    "4. [Visualizing SHAP Values](#visualizing-shap-values)\n",
    "5. [Conclusion](#conclusion)\n",
    "6. [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5d28",
   "metadata": {
    "id": "5e8c7fec"
   },
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ef10",
   "metadata": {
    "id": "06e91420"
   },
   "source": [
    "Reinforcement Learning (RL) has achieved remarkable success in various domains, such as game playing, robotics, and autonomous systems. However, RL models, particularly those using deep neural networks, are often seen as **black boxes** due to their complex architectures and non-linear computations. This opacity makes it challenging to understand and trust the decisions made by RL agents, especially in critical applications where transparency is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fcbef6",
   "metadata": {
    "id": "47b1e7ab"
   },
   "source": [
    "**Explainable Reinforcement Learning (XRL)** aims to bridge this gap by providing insights into an agent's decision-making process. By leveraging explainability techniques, we can:\n",
    "- Interpret the actions of an RL agent.\n",
    "- Understand the influence of input features on decisions.\n",
    "- Potentially improve the model's performance, fairness, and transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3235",
   "metadata": {
    "id": "ec0717c1"
   },
   "source": [
    "In this tutorial, we will demonstrate how to apply SHAP values to a trained actor neural network in an RL framework to explain the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01746",
   "metadata": {
    "id": "0d59bb0a",
    "lines_to_next_cell": 0
   },
   "source": [
    "### 1.1 Running a MADRL Simulation <a name=\"MARL\"></a>\n",
    "\n",
    "In this tutorial, we will simulate RL agents using a Multi-Agent Deep Reinforcement Learning (MADRL) approach. The agents operate in a market-splitting environment where they interact and learn optimal strategies over time. Here’s a breakdown of the key components:\n",
    "\n",
    "- **Observations**: Each agent receives observations, including market forecasts, unit-specific information, and past actions.\n",
    "- **Actions**: The agents decide on bidding strategies, such as bid prices for both inflexible and flexible capacities.\n",
    "- **Rewards**: The agents are rewarded based on profits and opportunity costs, helping them learn optimal bidding strategies.\n",
    "- **Algorithm**: We utilize a multi-agent version of the TD3 (Twin Delayed Deep Deterministic Policy Gradient) algorithm, which ensures stable learning even in non-stationary environments.\n",
    "\n",
    "For a more detailed explanation of the RL configurations, refer to the [Deep Reinforcement Learning Tutorial](https://example.com/deep-rl-tutorial).\n",
    "\n",
    "### Key Aspects of the Simulation\n",
    "\n",
    "Agents require **observations** to make informed decisions, which include:\n",
    "\n",
    "- **Residual Load Forecast**: Forecasted net demand (electricity demand minus renewable generation) over the next 24 hours.\n",
    "- **Price Forecast**: Forecasted market prices over the next 24 hours.\n",
    "- **Marginal Cost**: The current marginal cost of operating the agent's power-generating unit.\n",
    "- **Previous Output**: The agent’s dispatched capacity (energy production) from the previous time step.\n",
    "\n",
    "### Agent Actions\n",
    "\n",
    "The action space for the agents is two-dimensional and consists of:\n",
    "\n",
    "- **Bid Price for Inflexible Capacity (p_inflex)**: The price at which the agent offers its minimum power output (must-run capacity) to the market.\n",
    "- **Bid Price for Flexible Capacity (p_flex)**: The price for the additional capacity above the minimum output that the agent can flexibly adjust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b17e49",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "#### 1.1.1 Install Assume and Required Packages\n",
    "\n",
    "In this section, we will install the necessary packages to run the **Assume framework** along with other dependencies.\n",
    "The process is similar to the other tutorial on Assume.\n",
    "\n",
    "The following commands will install Assume and its dependencies for reinforcement learning, along with additional libraries such as Plotly for visualization.\n",
    "Make sure to install these before running the main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647079b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ee220130",
    "outputId": "ffd98b47-2b07-41cd-dfe4-ff0381571825",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install 'assume-framework[learning]'\n",
    "!pip install plotly\n",
    "!git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca95d2",
   "metadata": {
    "id": "e62e00c9"
   },
   "source": [
    "You will also need to install additional optimization libraries like Pyomo and GLPK. These libraries are crucial for modeling and solving optimization problems in the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f0fb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn_DvrqR7oK2",
    "outputId": "391046fd-90d9-4967-b919-52c49f13a9f8",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyomo\n",
    "!apt-get install -y -qq glpk-utils\n",
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28add86",
   "metadata": {},
   "source": [
    "Define paths to differentiate between Colab or local usage.\n",
    "If you're running this on Google Colab, the paths might differ slightly from your local environment.\n",
    "You can configure the paths accordingly based on where you're executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c91474",
   "metadata": {
    "id": "e62e00c9"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import plotly for visualization\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# import yaml for reading and writing YAML files\n",
    "import yaml\n",
    "\n",
    "# Check if 'google.colab' is available\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "colab_inputs_path = \"inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path\n",
    "\n",
    "print(inputs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95724ea",
   "metadata": {
    "id": "636ea9ae"
   },
   "source": [
    "#### 1.1.2 Create and Load Example Files from Market Splitting Tutorial\n",
    "\n",
    "To define the RL Agent, we need to obtain the results from the **Market Zone Splitting** tutorial.\n",
    "This tutorial provides essential data that the RL agent will use for decision-making.\n",
    "\n",
    "If you are working in **Google Colab**, execute the following cells to download and run the necessary notebook automatically. \n",
    "If you are working on your **local machine**, simply open the respective tutorial notebook and execute it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdfe19",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# For local execution:\n",
    "%cd assume/examples/notebooks/\n",
    "\n",
    "# For execution in Google Colab:\n",
    "%cd assume-repo/examples/notebooks/\n",
    "\n",
    "# Execute the Market Zone Splitting tutorial:\n",
    "!jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=60 --output output.ipynb 08_market_zone_coupling.ipynb\n",
    "\n",
    "# Return to content folder (for Colab):\n",
    "%cd /content\n",
    "\n",
    "# Copy inputs directory to the working folder (for Colab):\n",
    "!cp -r assume-repo/examples/notebooks/inputs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7eab9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "233f315b",
    "outputId": "f98da7d4-0080-4546-c642-838f722965b0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the input directory\n",
    "input_dir = os.path.join(inputs_path, \"tutorial_08\")\n",
    "\n",
    "\n",
    "# Read the DataFrames from CSV files\n",
    "powerplant_units = pd.read_csv(os.path.join(input_dir, \"powerplant_units.csv\"))\n",
    "demand_df = pd.read_csv(os.path.join(input_dir, \"demand_df.csv\"))\n",
    "\n",
    "print(\"Input CSV files have been read from 'inputs/tutorial_08'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc51a8",
   "metadata": {
    "id": "6985289b"
   },
   "source": [
    "#### 1.1.3 Transform the Scenario into a Learning Example\n",
    "\n",
    "The following cells show how we can convert any pre-configured scenario in Assume into a learning example.\n",
    "\n",
    "**Define a Learning Power Plant**\n",
    "\n",
    "In this example, we place a learning nuclear power plant in the southern zone. This plant has five times the maximum power of a typical plant, which allows us to create a scenario where its actions have a noticeable impact on market prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4153fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "b205256f",
    "outputId": "b9bb887b-f534-4a50-dd5b-229be1012600"
   },
   "outputs": [],
   "source": [
    "# Create scarcity in southern Germany by limiting the number of power plants\n",
    "powerplant_units = powerplant_units[:20]\n",
    "\n",
    "# Assign the RL-controlled power plant and give it market power\n",
    "powerplant_units.loc[19, \"bidding_zonal\"] = \"pp_learning\"\n",
    "powerplant_units.loc[19, \"max_power\"] = 5000  # Set maximum power to 5000 MW\n",
    "\n",
    "# Assign a specific RL unit operator to the plant\n",
    "powerplant_units.loc[19, \"unit_operator\"] = \"Operator-RL\"\n",
    "\n",
    "# Set the 'name' column as the index\n",
    "powerplant_units.set_index(\"name\", inplace=True, drop=True)\n",
    "\n",
    "# Save the updated power plant units to a CSV file\n",
    "powerplant_units.to_csv(input_dir + \"/powerplant_units.csv\")\n",
    "\n",
    "# Show the last 10 entries\n",
    "powerplant_units.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a22a61",
   "metadata": {
    "id": "cce0e8b4"
   },
   "source": [
    "**Configure Learning Hyperparameters in YAML**\n",
    "\n",
    "The following YAML configuration contains the learning-specific hyperparameters that will guide the RL agent's training process. Below is a brief description of these hyperparameters:\n",
    "\n",
    "- **continue_learning** (`False`): \n",
    "  - Whether to continue training from a previously saved state or start fresh.\n",
    "\n",
    "- **max_bid_price** (`100`): \n",
    "  - The maximum allowable bid price for the agent, used to scale the actor's output.\n",
    "\n",
    "- **algorithm** (`\"matd3\"`): \n",
    "  - The learning algorithm to be used, in this case `MATD3` (Multi-Agent Twin Delayed Deep Deterministic Policy Gradient).\n",
    "\n",
    "- **learning_rate** (`0.001`): \n",
    "  - The rate at which the model’s parameters are updated during training.\n",
    "\n",
    "- **training_episodes** (`50`): \n",
    "  - The total number of episodes for training the agent.\n",
    "\n",
    "- **episodes_collecting_initial_experience** (`3`): \n",
    "  - Number of episodes dedicated to collecting initial experience before actual training begins, during which the agent follows a random policy.\n",
    "\n",
    "- **train_freq** (`\"4h\"`): \n",
    "  - Frequency of model training, in this case, every 4 hours.\n",
    "\n",
    "- **gradient_steps** (`-1`): \n",
    "  - The number of gradient updates to perform at each training step. A value of `-1` typically means that all collected experience will be used for training.\n",
    "\n",
    "- **batch_size** (`256`): \n",
    "  - The size of the mini-batch used for training.\n",
    "\n",
    "- **gamma** (`0.99`): \n",
    "  - The discount factor for future rewards, balancing short-term vs. long-term reward importance.\n",
    "\n",
    "- **device** (`\"cpu\"`): \n",
    "  - The computational device for training. In this case, the CPU is used.\n",
    "\n",
    "- **noise_sigma** (`0.1`): \n",
    "  - The standard deviation of the exploration noise added to actions.\n",
    "\n",
    "- **noise_scale** (`1`) and **noise_dt** (`1`): \n",
    "  - Parameters controlling the scale and time step of the exploration noise. Since both are set to 1, no decay is applied.\n",
    "\n",
    "- **validation_episodes_interval** (`3`): \n",
    "  - The interval (in episodes) at which validation is performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c64dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c555ce9",
    "outputId": "473126ae-3c3e-4698-e3a5-347cc00e5108"
   },
   "outputs": [],
   "source": [
    "# YAML configuration for the RL training\n",
    "config = {\n",
    "    \"zonal_case\": {\n",
    "        \"start_date\": \"2019-01-01 00:00\",\n",
    "        \"end_date\": \"2019-01-01 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 4,\n",
    "        \"learning_mode\": \"True\",\n",
    "        \"markets_config\": {\n",
    "            \"zonal\": {\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 1, \"first_delivery\": \"1h\"}],\n",
    "                \"opening_frequency\": \"1h\",\n",
    "                \"opening_duration\": \"1h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"pay_as_clear_complex\",\n",
    "                \"additional_fields\": [\"bid_type\", \"node\"],\n",
    "                \"param_dict\": {\"network_path\": \".\", \"zones_identifier\": \"zone_id\"},\n",
    "            }\n",
    "        },\n",
    "        \"learning_config\": {\n",
    "            \"continue_learning\": False,\n",
    "            \"max_bid_price\": 100,\n",
    "            \"algorithm\": \"matd3\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"training_episodes\": 15,\n",
    "            \"episodes_collecting_initial_experience\": 3,\n",
    "            \"train_freq\": \"4h\",\n",
    "            \"gradient_steps\": -1,\n",
    "            \"batch_size\": 256,\n",
    "            \"gamma\": 0.99,\n",
    "            \"device\": \"cpu\",\n",
    "            \"noise_sigma\": 0.1,\n",
    "            \"noise_scale\": 1,\n",
    "            \"noise_dt\": 1,\n",
    "            \"validation_episodes_interval\": 3,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the configuration file\n",
    "config_path = os.path.join(input_dir, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cbdb4",
   "metadata": {
    "id": "3f0f38fb"
   },
   "source": [
    "In order to make this setup compatible with XRL, we need to enhance the logging of the learning process. \n",
    "ASSUME does not have this feature natively, so we will override some functions to enable this logging for the purpose of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01977d5",
   "metadata": {
    "cellView": "form",
    "id": "201251c6"
   },
   "outputs": [],
   "source": [
    "# @title Overwrite run_learning function with enhanced logging\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_config_and_create_forecaster,\n",
    "    setup_world,\n",
    ")\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def run_learning(\n",
    "    world: World,\n",
    "    inputs_path: str,\n",
    "    scenario: str,\n",
    "    study_case: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "    from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    actors_and_critics = None\n",
    "    world.learning_role.initialize_policy(actors_and_critics=actors_and_critics)\n",
    "    world.output_role.del_similar_runs()\n",
    "\n",
    "    # check if we already stored policies for this simualtion\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir():\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learnings? (y/N) \"\n",
    "        )\n",
    "        if not accept.lower().startswith(\"y\"):\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\"don't overwrite existing strategies\")\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Load scenario data to reuse across episodes\n",
    "    scenario_data = load_config_and_create_forecaster(inputs_path, scenario, study_case)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "        \"noise_scale\": world.learning_config.get(\"noise_scale\", 1.0),\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # TODO normally, loading twice should not create issues, somehow a scheduling issue is raised currently\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                episode=episode,\n",
    "            )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initliazed learning role the needed information across episodes\n",
    "        world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                scenario_data=scenario_data,\n",
    "                study_case=study_case,\n",
    "                perform_evaluation=True,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward()\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # if at end of simulation save last policies\n",
    "        if episode == (world.learning_role.training_episodes):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "            # export buffer_obs.json in the last training episode to get observations later\n",
    "            export = inter_episodic_data[\"buffer\"].observations.tolist()\n",
    "            path = f\"{world.learning_role.trained_policies_save_path}/buffer_obs\"\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(os.path.join(path, \"buffer_obs.json\"), \"w\") as f:\n",
    "                json.dump(export, f)\n",
    "\n",
    "        # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        scenario_data=scenario_data,\n",
    "        study_case=study_case,\n",
    "        terminate_learning=True,\n",
    "    )\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa6b82",
   "metadata": {
    "id": "dcacfe26"
   },
   "source": [
    "**Run the Example Case**\n",
    "\n",
    "Now we run the example case as done previously in the market zone tutorial. \n",
    "The main difference here is that we call the `run_learning()` function, which iterates multiple times over the simulation horizon for reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c9334",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bfadf522",
    "outputId": "7c91ab13-a3c2-4e89-d8ac-d20be95391f6"
   },
   "outputs": [],
   "source": [
    "# Import necessary classes and functions from the Assume framework\n",
    "from assume import World\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "\n",
    "# Define paths for input and output data\n",
    "csv_path = \"outputs\"\n",
    "\n",
    "# Define the data format and database URI for storing results\n",
    "# Use \"local_db\" for SQLite or \"timescale\" for TimescaleDB\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "data_format = \"local_db\"  # Options: \"local_db\" (SQLite) or \"timescale\" (TimescaleDB)\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance with the specified database\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario configuration\n",
    "# - world: World instance\n",
    "# - inputs_path: Folder containing input data\n",
    "# - scenario: Scenario subfolder in inputs\n",
    "# - study_case: Which configuration (case) to use for the simulation\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=inputs_path,\n",
    "    scenario=\"tutorial_08\",\n",
    "    study_case=\"zonal_case\",\n",
    ")\n",
    "\n",
    "# If learning mode is enabled, run the reinforcement learning loop\n",
    "if world.learning_config.get(\"learning_mode\", False):\n",
    "    run_learning(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=\"tutorial_08\",\n",
    "        study_case=\"zonal_case\",\n",
    "    )\n",
    "\n",
    "# Run the simulation\n",
    "world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec3968",
   "metadata": {
    "id": "2194f71b"
   },
   "source": [
    "**Compare the Results**\n",
    "\n",
    "Next, we use the same code from the market zone tutorial to generate a Plotly graph displaying market clearing prices over time for each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762acdfe",
   "metadata": {
    "id": "bdb21cbe",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import Plotly for creating interactive visualizations\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the path to the simulation output directory\n",
    "output_dir = \"outputs/tutorial_08_zonal_case\"\n",
    "market_meta_path = os.path.join(output_dir, \"market_meta.csv\")\n",
    "\n",
    "# Load the market metadata from the CSV file\n",
    "market_meta = pd.read_csv(market_meta_path, index_col=\"time\", parse_dates=True)\n",
    "market_meta = market_meta.drop(\n",
    "    columns=market_meta.columns[0]\n",
    ")  # Drop the first unnamed column\n",
    "\n",
    "# Extract unique zones from the \"node\" column\n",
    "zones = market_meta[\"node\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store clearing prices for each zone\n",
    "clearing_prices_df = pd.DataFrame()\n",
    "\n",
    "# Populate the DataFrame with clearing prices for each zone\n",
    "for zone in zones:\n",
    "    zone_data = market_meta[market_meta[\"node\"] == zone][[\"price\"]]\n",
    "    zone_data = zone_data.rename(columns={\"price\": f\"{zone}_price\"})\n",
    "    clearing_prices_df = (\n",
    "        pd.merge(\n",
    "            clearing_prices_df,\n",
    "            zone_data,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"outer\",\n",
    "        )\n",
    "        if not clearing_prices_df.empty\n",
    "        else zone_data\n",
    "    )\n",
    "\n",
    "# Sort the DataFrame by time\n",
    "clearing_prices_df = clearing_prices_df.sort_index()\n",
    "\n",
    "# Initialize the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot clearing prices for each zone\n",
    "for zone in zones:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=clearing_prices_df.index,\n",
    "            y=clearing_prices_df[f\"{zone}_price\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"{zone} - Simulation\",\n",
    "            line=dict(width=2),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize the layout for better aesthetics and interaction\n",
    "fig.update_layout(\n",
    "    title=\"Clearing Prices per Zone Over Time: Simulation Results\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Clearing Price (EUR/MWh)\",\n",
    "    legend_title=\"Market Zones\",\n",
    "    xaxis=dict(\n",
    "        tickangle=45,  # Rotate x-axis labels for readability\n",
    "        type=\"date\",  # Ensure x-axis is treated as dates\n",
    "    ),\n",
    "    hovermode=\"x unified\",  # Unified hover to compare values across zones at the same time\n",
    "    template=\"plotly_white\",  # Use a clean white background\n",
    "    width=1000,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed563ab7",
   "metadata": {
    "id": "e77c41cf"
   },
   "source": [
    "## 2. Explainable AI and SHAP Values <a name=\"explainable-ai-and-shap-values\"></a>\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee9432",
   "metadata": {
    "id": "cbf18570"
   },
   "source": [
    "To follow along with this tutorial, we need some additional libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a11a6",
   "metadata": {
    "id": "a5ac592c"
   },
   "source": [
    "- `matplotlib`\n",
    "- `shap`\n",
    "- `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e097c0",
   "metadata": {
    "id": "ae266ecb",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install shap==0.42.1\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3463ee2",
   "metadata": {
    "id": "5069ff93"
   },
   "source": [
    "### 2.1 Understanding Explainable AI\n",
    "Explainable AI (XAI) refers to techniques and methods that make the behavior and decisions of AI systems understandable to humans. In the context of complex models like deep neural networks, XAI helps to:\n",
    "- Increase Transparency: Providing insights into how models make decisions.\n",
    "- Build Trust: Users and stakeholders can trust AI systems if they understand them.\n",
    "- Ensure Compliance: Regulatory requirements often demand explainability.\n",
    "- Improve Models: Identifying weaknesses or biases in models.\n",
    "\n",
    "\n",
    "### 2.2 Introduction to SHAP Values\n",
    "Shapley values are a method from cooperative game theory used to explain the contribution of each feature to the prediction of a machine learning model, such as a neural network. They provide an interpretability technique by distributing the \"payout\" (the prediction) among the input features, attributing the importance of each feature to the prediction.\n",
    "\n",
    "For a given prediction, the Shapley value of a feature represents the average contribution of that feature to the prediction, considering all possible combinations of other features.\n",
    "\n",
    "1. **Marginal Contribution**:\n",
    "   The marginal contribution of a feature is the difference between the prediction with and without that feature.\n",
    "\n",
    "2. **Average over all subsets**:\n",
    "   The Shapley value is calculated by averaging the marginal contributions over all possible subsets of features.\n",
    "\n",
    "The formula for the Shapley value of feature $i$ is:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N| - |S| - 1)!}{|N|!} \\cdot \\left( f(S \\cup \\{i\\}) - f(S) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the set of all features.\n",
    "- $S$ is a subset of features.\n",
    "- $f(S)$ is the model’s prediction when using only the features in subset $S$.\n",
    "\n",
    "\n",
    "The `shap` library is a popular tool for computing Shapley values for machine learning models, including neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Why Use SHAP in RL?\n",
    "- Model-Agnostic: Applicable to any machine learning model, including neural networks.\n",
    "- Local Explanations: Provides explanations for individual predictions (actions).\n",
    "- Consistency: Ensures that features contributing more to the prediction have higher Shapley values.\n",
    "\n",
    "\n",
    "Properties of SHAP:\n",
    "1. Local Accuracy: The sum of Shapley values equals the difference between the model output and the expected output.\n",
    "2. Missingness: Features not present in the model have zero Shapley value.\n",
    "3. Consistency: If a model changes so that a feature contributes more to the prediction, the Shapley value of that feature should not decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade4682",
   "metadata": {
    "id": "21d49573"
   },
   "source": [
    "## 3. Calculating SHAP values <a name=\"calculating-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3758646",
   "metadata": {
    "id": "d16712fc"
   },
   "source": [
    "We will work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d5898",
   "metadata": {
    "id": "2c0c1262"
   },
   "source": [
    "- **Observations (`input_data`)**: These are the inputs to our actor neural network, representing the state of the environment.\n",
    "- **Trained Actor Model**: A neural network representing the decision making of one RL power plant that outputs actions based on the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77094d",
   "metadata": {
    "id": "2fb6fc14"
   },
   "source": [
    "Our goal is to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e2fbc9",
   "metadata": {
    "id": "e0b69db6"
   },
   "source": [
    "1. Load the observations and the trained actor model.\n",
    "2. Use the model to predict actions.\n",
    "3. Apply SHAP to explain the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e0416",
   "metadata": {
    "id": "f870b3e8"
   },
   "source": [
    "### 3.1. Loading and Preparing Data <a name=\"loading-and-preparing-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5965",
   "metadata": {
    "id": "aaa7c3d3"
   },
   "source": [
    "First, let's load the necessary libraries and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6f33b",
   "metadata": {
    "id": "b6ee4f28"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch as th\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f095c",
   "metadata": {
    "id": "ddfe95d9"
   },
   "source": [
    "We define a utility function to load observations and input data from a specified path. Analyzing the shap values for all observations and all parameters would make this notebook quite lengthy, so we’re filtering the observation data frame to include only 700 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89d972",
   "metadata": {
    "id": "44862f06"
   },
   "outputs": [],
   "source": [
    "# @title Load observations function\n",
    "\n",
    "\n",
    "def load_observations(path, feature_names):\n",
    "    # Load observations\n",
    "    obs_path = f\"{path}/buffer_obs.json\"\n",
    "\n",
    "    print(obs_path)\n",
    "\n",
    "    with open(obs_path) as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Convert the list of lists into a 2D numpy array\n",
    "    input_data = np.array(json_data)\n",
    "    input_data = np.squeeze(input_data)\n",
    "\n",
    "    print(len(input_data))\n",
    "    # filter out arrays where all value are 0\n",
    "    input_data = input_data[~np.all(input_data == 0, axis=1)]\n",
    "\n",
    "    print(len(input_data))\n",
    "    # filter only first 700 observations\n",
    "    input_data = input_data[:300]\n",
    "\n",
    "    return pd.DataFrame(input_data, columns=feature_names), input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f6681",
   "metadata": {
    "id": "6c5f1986"
   },
   "source": [
    "**Load Observations and Input Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b084bb",
   "metadata": {
    "id": "36ae8f9e"
   },
   "source": [
    "Load the observations and input data using the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e142be2",
   "metadata": {
    "id": "d522969d"
   },
   "outputs": [],
   "source": [
    "# path to extra loggedobservation values\n",
    "path = input_dir + \"/learned_strategies/zonal_case/buffer_obs\"\n",
    "\n",
    "# Define feature names (replace with actual feature names)\n",
    "# make columns names\n",
    "names_1 = [\"price forecast t+\" + str(x) for x in range(1, 25)]\n",
    "names_2 = [\"residual load forecast t+\" + str(x) for x in range(1, 25)]\n",
    "feature_names = names_1 + names_2 + [\"total capacity t-1\"] + [\"marginal costs t-1\"]\n",
    "\n",
    "df_obs, input_data = load_observations(path, feature_names)\n",
    "\n",
    "df_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add0715",
   "metadata": {
    "id": "5d8b9dcf"
   },
   "source": [
    "**Load the Trained Actor Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e67d",
   "metadata": {
    "id": "b1b50488"
   },
   "source": [
    "We initialize and load the trained actor neural network. Therefore, we define the actor neural network class that will be used to predict actions based on observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca85e13",
   "metadata": {
    "id": "4da4de57"
   },
   "outputs": [],
   "source": [
    "from assume.reinforcement_learning.neural_network_architecture import MLPActor\n",
    "\n",
    "# Initialize the model\n",
    "obs_dim = len(feature_names)\n",
    "act_dim = 2  # Adjust if your model outputs a different number of actions\n",
    "model = MLPActor(obs_dim=obs_dim, act_dim=act_dim, float_type=th.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3b7e6",
   "metadata": {
    "id": "37adecfa"
   },
   "outputs": [],
   "source": [
    "# which actor is the RL actor\n",
    "ACTOR_NUM = len(powerplant_units)  # 20\n",
    "\n",
    "# Path to actor we want to analyse\n",
    "actor_path = os.path.join(\n",
    "    input_dir,\n",
    "    f\"learned_strategies/zonal_case/avg_reward_eval_policies/actors/actor_Unit {ACTOR_NUM}.pt\",\n",
    ")\n",
    "\n",
    "# Load the trained model parameters\n",
    "model_state = th.load(actor_path, map_location=th.device(\"cpu\"))\n",
    "model.load_state_dict(model_state[\"actor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d43a1b",
   "metadata": {
    "id": "d4a63712"
   },
   "source": [
    "Get the actions base on observation tensor we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507d331",
   "metadata": {
    "id": "e6460cfb"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "for obs in input_data:\n",
    "    obs_tensor = th.tensor(obs, dtype=th.float)\n",
    "    action = model(obs_tensor)\n",
    "    actions.append(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579665bc",
   "metadata": {
    "id": "ddd1ab1e"
   },
   "source": [
    "## 3.2. Creating a SHAP Explainer <a name=\"creating-a-shap-explainer\"></a>\n",
    "\n",
    "In the next step we create the Shap explainer. In this example we facilitat the Kenrel Shap method. You can easly swithc it out for Deep Shap. The SHAP Kernel Explainer is a model-agnostic method for computing SHAP values, which can be applied to any machine learning model, including black-box models like neural networks, decision trees, or ensemble models. It uses a simplified linear approximation based on the Kernel SHAP method to estimate the SHAP values, allowing you to interpret how each feature contributes to a particular model’s prediction. Basically the SHAP Kernel Explainer builds a weighted linear regression model around each prediction, using different combinations (coalitions) of input features to simulate their presence or absence. This results in SHAP values that represent the marginal contribution of each feature.\n",
    "\n",
    "As we fit a linear regression, we split the observatoin and action data into test and train data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0758eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Title Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_data, actions, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to tensors\n",
    "y_train = th.stack(y_train)\n",
    "y_test = th.stack(y_test)\n",
    "\n",
    "X_train_tensor = th.tensor(X_train, dtype=th.float32)\n",
    "y_train_tensor = th.tensor(y_train, dtype=th.float32)\n",
    "X_test_tensor = th.tensor(X_test, dtype=th.float32)\n",
    "y_test_tensor = th.tensor(y_test, dtype=th.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaa45f",
   "metadata": {
    "id": "ae7b108b",
    "lines_to_next_cell": 2
   },
   "source": [
    "We define a prediction function compatible with SHAP and create a Kernel SHAP explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e12192",
   "metadata": {
    "id": "6d9be211"
   },
   "outputs": [],
   "source": [
    "# @ Title Define a prediction function for generating actions for SHAP Explainer\n",
    "def model_predict(X):\n",
    "    X_tensor = th.tensor(X, dtype=th.float32)\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        return model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a32f41",
   "metadata": {
    "id": "84bb96cf"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP Kernel Explainer\n",
    "explainer = shap.KernelExplainer(model_predict, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279910b",
   "metadata": {
    "id": "2a7929e4"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ca286",
   "metadata": {
    "id": "c1f3d550"
   },
   "source": [
    "## 4. Visualizing SHAP Values <a name=\"visualizing-shap-values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cecbfe",
   "metadata": {
    "id": "3a0f0cbe"
   },
   "source": [
    "We generate summary plots to visualize feature importance for each output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3d533",
   "metadata": {
    "id": "2e318a5b"
   },
   "outputs": [],
   "source": [
    "# Summary plot for the first output dimension\n",
    "shap.summary_plot(shap_values[0], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 0, p_inflex\")\n",
    "plt.show()\n",
    "\n",
    "# Summary plot for the second output dimension\n",
    "shap.summary_plot(shap_values[1], X_test, feature_names=feature_names, show=False)\n",
    "plt.title(\"Summary Plot for Output Dimension 1, p_flex\")\n",
    "plt.show()\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[0],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 0\",\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values[1],\n",
    "    X_test,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    title=\"Summary Bar Plot for Output Dimension 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ebfb",
   "metadata": {
    "id": "9a888f8b"
   },
   "source": [
    "The SHAP summary plots show the impact of each feature on the model's predictions for each output dimension (action). Features with larger absolute SHAP values have a more significant influence on the decision-making process of the RL agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555afa84",
   "metadata": {
    "id": "c6c4ce8c"
   },
   "source": [
    "- **Positive SHAP Value**: Indicates that the feature contributes positively to the predicted action value.\n",
    "- **Negative SHAP Value**: Indicates that the feature contributes negatively to the predicted action value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc307ed",
   "metadata": {
    "id": "86545200"
   },
   "source": [
    "By analyzing these plots, we can identify which features are most influential and understand how changes in feature values affect the agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b57e2",
   "metadata": {
    "id": "06f3977c"
   },
   "source": [
    "## 5. Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79634a",
   "metadata": {
    "id": "dadd0a0c"
   },
   "source": [
    "In this tutorial, we've demonstrated how to apply SHAP to a reinforcement learning agent to explain its decision-making process. By interpreting the SHAP values, we gain valuable insights into which features influence the agent's actions, enhancing transparency and trust in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0035820",
   "metadata": {
    "id": "37633c16"
   },
   "source": [
    "Explainability is crucial, especially when deploying RL agents in real-world applications where understanding the rationale behind decisions is essential for safety, fairness, and compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c10373",
   "metadata": {
    "id": "8735d66f"
   },
   "source": [
    "## 6. Additional Resources <a name=\"additional-resources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80754bea",
   "metadata": {
    "id": "d6b0332f"
   },
   "source": [
    "- **SHAP Documentation**: [https://shap.readthedocs.io/en/latest/](https://shap.readthedocs.io/en/latest/)\n",
    "- **PyTorch Documentation**: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "- **Reinforcement Learning Introduction**: [Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "- **Interpretable Machine Learning Book**: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873097",
   "metadata": {
    "id": "a8cdea5f"
   },
   "source": [
    "**Feel free to experiment with the code and explore different explainability techniques. Happy learning!**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "vscode,outputId,colab,cellView,id,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
