{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f627ddd",
   "metadata": {
    "id": "4JeBorbE6FYr"
   },
   "source": [
    "# 4.2 Reinforcement learning tutorial\n",
    "\n",
    "This tutorial will introduce users into ASSUME and its ways of using reinforcement learning (RL). The main objective of this tutorial is to ensure participants grasp the steps required to equip a new unit with RL strategies or modify the action dimensions.\n",
    "Our emphasis lies in the bidding strategy, with less focus on the algorithm and role. The latter are usable as a plug-and-play solution in the framework. The following coding tasks will highlight the key aspects to be adjusted, as already outlined in the learning_strategies.py file.\n",
    "\n",
    "The outline of this tutorial is as follows. We will start with a basic summary of the implementation of reinforcement learning (RL) in ASSUME and its architecture (1. ASSUME & Learning Basics) . If you need a refresher on RL in general, please visit our readthedocs (https://ASSUME.readthedocs.io/en/latest/). Afterwards, we install ASSUME in this Google Colab (2. Get ASSUME running) and then we dive into the learning_strategies.py file and explain how we need to adjust conventional bidding strategies. to incorporate RL (3. Make ASSUME learn).\n",
    "\n",
    "**As a whole, this tutorial covers the following coding tasks:**\n",
    "\n",
    "1. How to define a step function in the ASSUME framework.\n",
    "\n",
    "2. How do we get observations from the simulation framework.\n",
    "\n",
    "3. How do we define actions based on the output of the actor neural network considering necessary exploration?\n",
    "\n",
    "4. How do we define the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762568ca",
   "metadata": {
    "id": "bj2C4ElILNNv"
   },
   "source": [
    "## 1. ASSUME & Learning Basics\n",
    "\n",
    "ASSUME in general is intended for researchers, planners, utilities and everyone searching to understand market dynamics of energy markets. It provides an easy-to-use tool-box as a free software that can be tailored to the specific use case of the user.\n",
    "\n",
    "In the following figure the architecture of the framework is depicted. It can be roughly devided into two parts. On the left side of the world class the markets are located and on the right side the market participants, which are here named units. Both world are connected via the orders that market participants place on the markets. The learning capability is sketched out with the yellow classes on the right side, namely the units side.\n",
    "\n",
    "\n",
    "\n",
    "![architecture.svg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa1c37",
   "metadata": {
    "id": "dDn1blWvPM7Z"
   },
   "source": [
    "Let's focus on the bright yellow part of the architecture, namely the learning algorithm, the actor and the critic. We start with some **reinforcement learning backround**. In the current implementation of ASSUME, we model the electricity market as a partially observable Markov game, which is an extension of MDPs for multi-agent setups.\n",
    "\n",
    "**Multi-agent DRL** is understood as the simultaneous learning of multiple agents interacting in the same environment. The Markov game for $N$ agents consists of a set of states $S$, a set of actions $A_1, ..., A_N$, a set of observations $O_1, ..., O_N$, and a state transition function $P: S \\times A_1 \\times ... \\times A_N \\rightarrow \\mathcal{P}(S)$ dependent on the state and actions of all agents. After taking action $a_i \\in A_i$ in state $s_i \\in S$ according to a policy $\\pi_i:O_i\\rightarrow A_i$, every agent $i$ is transitioned into the new state $s'_i \\in S$. Each agent receives a reward $r_i$ according to the individual reward function $R_i$ and a private observation correlated with the state $o_i:S \\rightarrow O_i$. Like MDP, each agent $i$ learns an optimal policy $\\pi_i^*(s)$ that maximizes its expected reward.\n",
    "\n",
    "To enable multi-agent learning some adjustments are needed within the learning algorithm to get from the TD3 to an MATD3 algorithm. Other authors used similar tweaks to improve the TD3 into the MADDPG algorithm and derive the MA-TD3 algorithm. We'll start explaining the learning by focusing on a single agent and then extend it to multi-agent learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fa058",
   "metadata": {},
   "source": [
    "### Single-Agent Learning\n",
    "\n",
    "We use the actor-critic approach to train the learning agent. The actor-critic approach is a popular RL algorithm that uses two neural networks: an actor network and a critic network. The actor network is responsible for selecting actions, while the critic network evaluates the quality of the actions taken by the actor.\n",
    "\n",
    "The actor and critic networks are trained simultaneously using the actor-critic algorithm, which updates the weights of both networks at each time step. The actor-critic algorithm is a form of policy iteration, where the policy is updated based on the estimated value function, and the value function is updated based on the.\n",
    "\n",
    "**Actor**\n",
    "The actor network is trained using the policy gradient method, which updates the weights of the actor network in the direction of the gradient of the expected reward with respect to the network parameters:\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = E[\\nabla_{\\theta} log \\pi_{\\theta}(a_t|s_t) * Q^{\\pi}(s_t, a_t)]$\n",
    "\n",
    "where $J(\\theta)$ is the expected reward, $\\theta$ are the weights of the actor network, $\\pi_{\\theta}(a_t|s_t)$ is the probability of selecting action a_t given state $s_t$, and $Q^{\\pi}(s_t, a_t)$ is the expected reward of taking action $a_t$ in state $s_t$ under policy $\\pi$.\n",
    "\n",
    "**Critic**\n",
    "The critic network is trained using the temporal difference (TD) learning method, which updates the weights of the critic network based on the difference between the estimated value of the current state and the estimated value of the next state:\n",
    "\n",
    "$\\delta_t = r_t + \\gamma * V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "where $\\delta_t$ is the TD error, $r_t$ is the reward obtained at time step $t$, $\\gamma$ is the discount factor, $V(s_t)$ is the estimated value of state $s_t$, and $V(s_{t+1})$ is the estimated value of the next state $s_{t+1}$.\n",
    "\n",
    "The weights of the critic network are updated in the direction of the gradient of the mean squared TD error:\n",
    "\n",
    "$\\nabla_{\\theta} L = E[(\\delta_t)^2]$\n",
    "\n",
    "where L is the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f0839",
   "metadata": {
    "id": "OMvIl2xLVi1l"
   },
   "source": [
    "### Multi-Agent Learning\n",
    "\n",
    "While in a single-agent setup, the state transition and respective reward depend only on the actions of a single agent, the state transitions and rewards depend on the actions of all learning agents in a multi-agent setup. This makes the environment non-stationary for a single agent, which violates the Markov property. Hence, the convergence guarantees of single-agent RL algorithms are no longer valid. Therefore, we utilize the framework of centralized training and decentralized execution and expand upon the MADDPG algorithm. The main idea of this approach is to use a centralized critic during the training phase, which has access to the entire state $\\textbf{S}$, and all actions $a_1, ..., a_N$, thus resolving the issue of non-stationarity, as changes in state transitions and rewards can be explained by the actions of other agents. Meanwhile, during both training and execution, the actor has access only to its local observations $o_i$ derived from the entire state $\\textbf{S}$.\n",
    "\n",
    "For each agent $i$, we train two centralized critics $Q_{i,θ_1,2}(S, a_1, ..., a_N)$  together with two target critic networks. Similar to TD3, the smaller value of the two critics and target action noise $a_i$,$k~$ is used to calculate the target $y_i,k$:\n",
    "\n",
    "$y_i,k = r_i,k + γ * min_j=1,2 Q_i,θ′_j(S′_k, a_1,k, ..., a_N,k, π′(o_i,k))$\n",
    "\n",
    "where $r_i,k$ is the reward obtained by agent $i$ at time step $k$, $γ$ is the discount factor, $S′_k$ is the next state of the environment, and $π′(o_i,k)$ is the target policy of agent $i$.\n",
    "\n",
    "The critics are trained using the mean squared Bellman error (MSBE) loss:\n",
    "\n",
    "$L(Q_i,θ_j) = E[(y_i,k - Q_i,θ_j(S_k, a_1,k, ..., a_N,k))^2]$\n",
    "\n",
    "The actor policy of each agent is updated using the deterministic policy gradient (DPG) algorithm:\n",
    "\n",
    "$∇_a Q_i,θ_j(S_k, a_1,k, ..., a_N,k, π(o_i,k))|a_i,k=π(o_i,k) * ∇_θ π(o_i,k)$\n",
    "\n",
    "The actor is updated similarly using only one critic network $Q_{θ1}$. These changes to the original DDPG algorithm allow increased stability and convergence of the TD3 algorithm. This is especially relevant when approaching a multi-agent RL setup, as discussed in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e905e",
   "metadata": {
    "id": "OeeZDtIFmmhn"
   },
   "source": [
    "## 2. Get ASSUME running\n",
    "Here we just install the ASSUME core package via pip. In general the instructions for an installation can be found here: https://ASSUME.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary.\n",
    "\n",
    "As we will be working with learning agents, we need to install ASSUME with all learning dependencies such as torch. For this, we use the [learning] attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0770442",
   "metadata": {},
   "source": [
    "**You don't need to execute the following code cell if you already have the ASSUME framework installed including learning dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8459b9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0DaRwFA7VgW",
    "outputId": "5655adad-5b7a-4fe3-9067-6b502a06136b"
   },
   "outputs": [],
   "source": [
    "!pip install 'assume-framework[learning]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53bcf9",
   "metadata": {
    "id": "IIw_QIE3pY34"
   },
   "source": [
    "And easy like this we have ASSUME installed. Now we can let it run. Please note though that we cannot use the functionalities tied to docker and, hence, cannot access the predefined dashboards in colab. For this please install docker and ASSUME on your personal machine.\n",
    "\n",
    "Further we would like to access the predefined scenarios in ASSUME which are stored on the git repository. Hence, we clone the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09764f6",
   "metadata": {},
   "source": [
    "**You don't need to execute the following code cell if you already have the ASSUME repository cloned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00f9b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hB0uDisSsg",
    "outputId": "1241881f-e090-4f26-9b02-560adfcb3a3e"
   },
   "outputs": [],
   "source": [
    "!git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32169f9f",
   "metadata": {
    "id": "Fg7DyNjLuvSb"
   },
   "source": [
    "**Let the magic happen.** Now you can run your first ever simulation in ASSUME. The following code navigates to the respective ASSUME folder and starts the simulation example example_01b using the local database here in colab.\n",
    "\n",
    "When running locally, you can also just run `ASSUME -s example_01b -db \"sqlite:///./examples/local_db/ASSUME_db_example_01b.db\"` in a shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31a7b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eVM60Qx8SC0",
    "outputId": "20434515-6e65-4d34-d44d-8c4529a46ece"
   },
   "outputs": [],
   "source": [
    "!cd assume-repo && assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7f32a",
   "metadata": {},
   "source": [
    "**Select input files path**:\n",
    "\n",
    "We also need to differentiate between the input file paths when using this tutorial in Google Colab and a local environment. The code snippets will include both options for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3dd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check if 'google.colab' is available\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e527b5",
   "metadata": {
    "id": "zMyZhaNM7NRP"
   },
   "source": [
    "## 3. Make your agents learn\n",
    "\n",
    "Now it is time to get your hands dirty and actually dive into coding in ASSUME. The main objective of this session is to ensure participants grasp the steps required to equip a new unit with RL strategies or modify the action dimensions. Our emphasis lies in the bidding strategy, with less focus on the algorithm and role. Coding tasks will highlight the key aspects to be a djusted, as already outlined in the learning_strategies.py file. Subsequent\n",
    "sections will present the tasks and provide the correct answers for the coding exercises.\n",
    "\n",
    "We start by initializing the class of our Learning Strategy. This is very cloesly related to the general strucutre of a bidding strategy.\n",
    "\n",
    "\n",
    "**But first some imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d17e03",
   "metadata": {
    "id": "xUsbeZdPJ_2Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "\n",
    "from assume import World\n",
    "from assume.common.base import LearningStrategy, SupportsMinMax\n",
    "from assume.common.market_objects import MarketConfig, Orderbook, Product\n",
    "from assume.reinforcement_learning.algorithms import actor_architecture_aliases\n",
    "from assume.reinforcement_learning.learning_utils import NormalActionNoise\n",
    "from assume.scenario.loader_csv import load_scenario_folder, run_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383bbbfd",
   "metadata": {
    "id": "UXYSesx4Ifp5"
   },
   "outputs": [],
   "source": [
    "class RLStrategy(LearningStrategy):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Strategy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(obs_dim=50, act_dim=2, unique_obs_dim=2, *args, **kwargs)\n",
    "\n",
    "        self.unit_id = kwargs[\"unit_id\"]\n",
    "\n",
    "        # defines bounds of actions space\n",
    "        self.max_bid_price = kwargs.get(\"max_bid_price\", 100)\n",
    "        self.max_demand = kwargs.get(\"max_demand\", 10e3)\n",
    "\n",
    "        # tells us whether we are training the agents or just executing per-learnind stategies\n",
    "        self.learning_mode = kwargs.get(\"learning_mode\", False)\n",
    "        self.perform_evaluation = kwargs.get(\"perform_evaluation\", False)\n",
    "\n",
    "        # based on learning config define algorithm configuration\n",
    "        self.algorithm = kwargs.get(\"algorithm\", \"matd3\")\n",
    "        actor_architecture = kwargs.get(\"actor_architecture\", \"mlp\")\n",
    "\n",
    "        # define the architecture of the actor neural network\n",
    "        # if you use many time series niputs you might wantto use the LSTM instead of teh MLP for example\n",
    "        if actor_architecture in actor_architecture_aliases.keys():\n",
    "            self.actor_architecture_class = actor_architecture_aliases[\n",
    "                actor_architecture\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Policy '{actor_architecture}' unknown. Supported architectures are {list(actor_architecture_aliases.keys())}\"\n",
    "            )\n",
    "\n",
    "        # sets the devide of the actor network\n",
    "        device = kwargs.get(\"device\", \"cpu\")\n",
    "        self.device = th.device(device if th.cuda.is_available() else \"cpu\")\n",
    "        if not self.learning_mode:\n",
    "            self.device = th.device(\"cpu\")\n",
    "\n",
    "        # future: add option to choose between float16 and float32\n",
    "        # float_type = kwargs.get(\"float_type\", \"float32\")\n",
    "        self.float_type = th.float\n",
    "\n",
    "        # for definition of observation space\n",
    "        self.foresight = kwargs.get(\"foresight\", 24)\n",
    "\n",
    "        if self.learning_mode:\n",
    "            self.learning_role = None\n",
    "            self.collect_initial_experience_mode = kwargs.get(\n",
    "                \"episodes_collecting_initial_experience\", True\n",
    "            )\n",
    "\n",
    "            self.action_noise = NormalActionNoise(\n",
    "                mu=0.0,\n",
    "                sigma=kwargs.get(\"noise_sigma\", 0.1),\n",
    "                action_dimension=self.act_dim,\n",
    "                scale=kwargs.get(\"noise_scale\", 1.0),\n",
    "                dt=kwargs.get(\"noise_dt\", 1.0),\n",
    "            )\n",
    "\n",
    "        elif Path(load_path=kwargs[\"trained_policies_save_path\"]).is_dir():\n",
    "            self.load_actor_params(load_path=kwargs[\"trained_policies_save_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c7ad1",
   "metadata": {
    "id": "8UM1QPZrIdqK"
   },
   "source": [
    "### 3.1 The \"Step Function\"\n",
    "\n",
    "The key function in an RL problem is the step that is taken in the so called environment. It consist the following parts:\n",
    "\n",
    "1. Get an observation\n",
    "2. Choose an action\n",
    "3. Get a reward\n",
    "4. Update your policy\n",
    "\n",
    "In ASSUME we do not have such a straight forward step function. The steps 1 & 2 are combined in the calculate_bids() function which is called as soon as an offer on the market is placed. The step 3, however, can only happen after we get the market feedback from the simulation run and, hence, is in the calculate_reward() function. Step 4 is solely handeled by the learning_role as it shedules the policy update manages the buffer and what not. Hence, it is actually not included in this notebook, since we only focus on transforming the bidding strategy into a learning one.\n",
    "\n",
    "**Step 1-3 will be implemented in the following sections 3.2 to 3.4. If there is a coding task for you it will be marked accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e33c9e",
   "metadata": {
    "id": "iApbQsg5x_u2"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def calculate_bids(\n",
    "        self,\n",
    "        unit: SupportsMinMax,\n",
    "        market_config: MarketConfig,\n",
    "        product_tuples: list[Product],\n",
    "        **kwargs,\n",
    "    ) -> Orderbook:\n",
    "        \"\"\"\n",
    "        Calculate bids for a unit -> STEP 1 & 2\n",
    "        \"\"\"\n",
    "\n",
    "        start = product_tuples[0][0]\n",
    "        end = product_tuples[0][1]\n",
    "        # get technical bounds for the unit output from the unit\n",
    "        min_power, max_power = unit.calculate_min_max_power(start, end)\n",
    "        min_power = min_power[start]\n",
    "        max_power = max_power[start]\n",
    "\n",
    "        # =============================================================================\n",
    "        # 1. Get the Observations, which are the basis of the action decision\n",
    "        # =============================================================================\n",
    "        next_observation = self.create_observation(\n",
    "            unit=unit,\n",
    "            market_id=market_config.market_id,\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "        # =============================================================================\n",
    "        # 2. Get the Actions, based on the observations\n",
    "        # =============================================================================\n",
    "        actions, noise = self.get_actions(next_observation)\n",
    "\n",
    "        bids = actions\n",
    "\n",
    "        bids = self.remove_empty_bids(bids)\n",
    "\n",
    "        return bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2907489",
   "metadata": {
    "id": "_4cJ8Y8uvMgV"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def calculate_reward(\n",
    "        self,\n",
    "        unit,\n",
    "        marketconfig: MarketConfig,\n",
    "        orderbook: Orderbook,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate reward\n",
    "        \"\"\"\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91134e29",
   "metadata": {
    "id": "Jgjx14997Y9s"
   },
   "source": [
    "### 3.2 Get an observation\n",
    "\n",
    "The decision about the observations received by each agent plays a crucial role when designing a multi-agent RL setup. The following describes the task of learning agents representing profit-maximizing electricity market participants who either sell a generating unit's output or optimize a storage unit's operation. They are represented through their plants' techno-economic parameters, such as minimal operational capacity $P^{min}$, start-up $c^{su}$, and shut-down $c^{sd}$ costs. This information is all know by the unit istself and, hence, also accessible in the bidding strategy.\n",
    "\n",
    "During the training phase, the centralized critic receives observations from all agents, resulting in an input size that grows linearly with the number of agents. This can lead to unstable training behavior of the critic networks, which limits the maximal number of agents in the simulation. This effect is known as the dimensionality curse, which likely contributed to the small number of learning agents in existing approaches. To address the dimensionality curse, we use a single observation that is the same for all agents and added a small size of unique observations for each agent to improve their performance. This modification allows the use of only one observation in the centralized critic, decoupled from the number of learning agents, significantly reducing the observation size and enabling simultaneous training of hundreds of learning agents with stable training behavior. The only limiting factor is the available working memory.\n",
    "\n",
    "At time-step $t$, agent $i$ receives the observation $o_{i,t}$ consisting of vectors $[L_{\\mathrm{h},t}, L_{\\mathrm{f},t}, M_{\\mathrm{h},t}, M_{\\mathrm{f},t}, mc_{i,t}]$. Here $L_{\\mathrm{h},t}, L_{\\mathrm{f},t}$ and $M_{\\mathrm{h},t}, M_{\\mathrm{f},t}$ are the past and the forecast residual loads and market prices, respectively. These information stems from the world, where a overall forecasting role generates them. The price forecast is calculated ahead of the simulation run using a simple merit order model based on the residual load forecast and the marginal cost of power plants. This part of the observation is the same for all agents. In addition, each agent receives its current marginal cost $mc_{i,t}$. Information about the marginal cost is shared with a centralized critic during the training phase. Still, it is not shared with other agents during the execution phase. All the inputs are normalized to improve the performance of the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade95c0",
   "metadata": {
    "id": "PngYyvs72UxB"
   },
   "source": [
    "#### **Task 1**\n",
    "**Goal**: With the help of the *unit*, the *starttime* and the *endtime* we want to create the Observations for the unit.\n",
    "\n",
    "There are 4 different observations:\n",
    "- residual load forecast\n",
    "- price forecast\n",
    "- total capacity of the unit\n",
    "- marginal costs of the unit\n",
    "\n",
    "For all observations we need scaling factors. Why do you think it is important to scale the input? How would you define the scaling factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7be3eb",
   "metadata": {
    "id": "0ww-L9fABnw3"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def create_observation(\n",
    "        self,\n",
    "        unit: SupportsMinMax,\n",
    "        market_id: str,\n",
    "        start: datetime,\n",
    "        end: datetime,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create observation\n",
    "        \"\"\"\n",
    "\n",
    "        end_excl = end - unit.index.freq\n",
    "\n",
    "        # get the forecast length depending on the tme unit considered in the modelled unit\n",
    "        forecast_len = pd.Timedelta((self.foresight - 1) * unit.index.freq)\n",
    "\n",
    "        # =============================================================================\n",
    "        # 1.1 Get the Observations, which are the basis of the action decision\n",
    "        # =============================================================================\n",
    "        # residual load forecast\n",
    "        # residual load forecast\n",
    "        scaling_factor_res_load = self.max_demand\n",
    "\n",
    "        # price forecast\n",
    "        scaling_factor_price = self.max_bid_price\n",
    "\n",
    "        # total capacity\n",
    "        scaling_factor_total_capacity = unit.max_power\n",
    "\n",
    "        # marginal cost\n",
    "        scaling_factor_marginal_cost = self.max_bid_price\n",
    "\n",
    "        # checks if we are at end of simulation horizon, since we need to change the forecast then\n",
    "        # for residual load and price forecast and scale them\n",
    "        if (\n",
    "            end_excl + forecast_len\n",
    "            > unit.forecaster[f\"residual_load_{market_id}\"].index[-1]\n",
    "        ):\n",
    "            scaled_res_load_forecast = (\n",
    "                unit.forecaster[f\"residual_load_{market_id}\"].loc[start:].values\n",
    "                / scaling_factor_res_load\n",
    "            )\n",
    "            scaled_res_load_forecast = np.concatenate(\n",
    "                [\n",
    "                    scaled_res_load_forecast,\n",
    "                    unit.forecaster[f\"residual_load_{market_id}\"].iloc[\n",
    "                        : self.foresight - len(scaled_res_load_forecast)\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            scaled_res_load_forecast = (\n",
    "                unit.forecaster[f\"residual_load_{market_id}\"]\n",
    "                .loc[start : end_excl + forecast_len]\n",
    "                .values\n",
    "                / scaling_factor_res_load\n",
    "            )\n",
    "\n",
    "        if end_excl + forecast_len > unit.forecaster[f\"price_{market_id}\"].index[-1]:\n",
    "            scaled_price_forecast = (\n",
    "                unit.forecaster[f\"price_{market_id}\"].loc[start:].values\n",
    "                / scaling_factor_price\n",
    "            )\n",
    "            scaled_price_forecast = np.concatenate(\n",
    "                [\n",
    "                    scaled_price_forecast,\n",
    "                    unit.forecaster[f\"price_{market_id}\"].iloc[\n",
    "                        : self.foresight - len(scaled_price_forecast)\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            scaled_price_forecast = (\n",
    "                unit.forecaster[f\"price_{market_id}\"]\n",
    "                .loc[start : end_excl + forecast_len]\n",
    "                .values\n",
    "                / scaling_factor_price\n",
    "            )\n",
    "\n",
    "        # get last accapted bid volume and the current marginal costs of the unit\n",
    "        current_volume = unit.get_output_before(start)\n",
    "        current_costs = unit.calc_marginal_cost_with_partial_eff(current_volume, start)\n",
    "\n",
    "        # scale unit outpus\n",
    "        scaled_total_capacity = current_volume / scaling_factor_total_capacity\n",
    "        scaled_marginal_cost = current_costs / scaling_factor_marginal_cost\n",
    "\n",
    "        # concat all obsverations into one array\n",
    "        observation = np.concatenate(\n",
    "            [\n",
    "                scaled_res_load_forecast,\n",
    "                scaled_price_forecast,\n",
    "                np.array([scaled_total_capacity, scaled_marginal_cost]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # transfer arry to GPU for NN processing\n",
    "        observation = (\n",
    "            th.tensor(observation, dtype=self.float_type)\n",
    "            .to(self.device, non_blocking=True)\n",
    "            .view(-1)\n",
    "        )\n",
    "\n",
    "        return observation.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c091f0",
   "metadata": {
    "id": "kDYKZGERKJ6V"
   },
   "source": [
    "#### **Solution 1**\n",
    "\n",
    "First why do we scale?\n",
    "\n",
    "Scaling observations is a crucial preprocessing step in machine learning, including reinforcement learning. It involves transforming the features so that they all fall within a similar numerical range. This is important for several reasons. Firstly, it aids in numerical stability during training. Large input values can lead to numerical precision issues, potentially causing the algorithm to perform poorly or even fail to converge. By scaling the features, we mitigate this risk, ensuring a more stable and reliable learning process.\n",
    "\n",
    "Additionally, scaling promotes uniformity in the learning process. Many optimization algorithms, like gradient descent, adjust model parameters based on the magnitude of gradients. When features have vastly different scales, some may dominate the learning process, while others receive less attention. This imbalance can hinder convergence and result in a suboptimal model. Scaling addresses this issue, allowing the algorithm to treat all features equally and progress more efficiently towards an optimal solution. This not only expedites the learning process but also enhances the model's ability to generalize to new, unseen data. In essence, scaling observations is a fundamental practice that enhances the performance and robustness of machine learning models across a wide array of applications.\n",
    "\n",
    "According to this the scaling should ensure a similar range for all input parameteres. You can achieve that by chosing the following scaling factors. If you add new observations, choose your scaling factors wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e1548",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "PYoI3ncSKJSX",
    "outputId": "4b4341d7-5a21-49c4-ee25-b8c55f693cd1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#scaling factors for all observations\n",
    "#residual load forecast\n",
    "scaling_factor_res_load = self.max_demand\n",
    "\n",
    "# price forecast\n",
    "scaling_factor_price = self.max_bid_price\n",
    "\n",
    "# total capacity\n",
    "scaling_factor_total_capacity = unit.max_power\n",
    "\n",
    "# marginal cost\n",
    "scaling_factor_marginal_cost = self.max_bid_price\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79379f1e",
   "metadata": {
    "id": "rW_1op6fCTV-"
   },
   "source": [
    "### 3.3 Choose an action\n",
    "\n",
    "To differentiate between the inflexible and flexible parts of a plant's generation capacity, we split the bids into two parts. The first bid part allows agents to bid a very low or even negative price for the inflexible capacity; this reflects the agent's motivation to stay infra-marginal during periods of very low net load (e.g., in periods of high solar and wind power generation) to avoid the cost of a shut-down and subsequent start-up of the plant. The flexible part of the capacity can be offered at a higher price to provide chances for higher profits. The actions of agent $i$ at time-step $t$ are defined as $a_{i,t} = [ep^\\mathrm{inflex}_{i,t}, ep^\\mathrm{flex}_{i,t}] \\in [ep^{min},ep^{max}]$, where $ep^\\mathrm{inflex}_{i,t}$ and $ep^\\mathrm{flex}_{i,t}$ are bid prices for the inflexible and flexible capacities, and $ep^{min},ep^{max}$ are minimal and maximal bid prices, respectively.\n",
    "\n",
    "How do we learn, how to make good decisions? Basically by try and error, also know as **exploration**. Exploration is a fundamental concept in reinforcement learning, representing the strategy by which an agent interacts with its environment to gather information about the consequences of its actions. This is crucial because without exploration, the agent might settle for suboptimal policies based on its initial knowledge, limiting its ability to discover more rewarding states or actions.\n",
    "\n",
    "In the initial stages of training, also often called initial exploration, it's imperative to employ almost random actions. This means having the agent take actions purely by chance. This seemingly counterintuitive approach serves a critical purpose. Initially, the agent lacks any meaningful information about the environment, making it impossible to make informed decisions. By taking random actions, it can quickly gather a broad range of experiences, allowing it to grasp the fundamental structure of the environment. These random actions serve as a kind of \"baseline exploration,\" providing a starting point from which the agent can refine its policy through learning. With our domain knowledge we can even guide the initial exploration process, to enhance learning capabilities.\n",
    "\n",
    "\n",
    "Following up on these concepts the following tasks will:\n",
    "1. obtain the action values from the neurnal net in the bidding staretgy and\n",
    "2. then transform theses values into the actual bids of an order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2ee77",
   "metadata": {
    "id": "Cho84Pqs2N2G"
   },
   "source": [
    "#### **Task 2.1**\n",
    "**Goal**: With the observations and noise we generate actions\n",
    "\n",
    "In the following task we define the actions for the initial exploration mode. As described before we can guide it by not letting it choose random actions but defining a base-bid on which we add a good amount of noise. In this way the initial strategy starts from a solution that we know works somewhat well. Define the respective base bid in the followin code. Remeber we are defining bids for a conventional power plant bidding in an Energy-Only-Market with a uniform pricing auction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2f135a",
   "metadata": {
    "id": "8ehlm5Z9CbRw"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def get_actions(self, next_observation):\n",
    "        \"\"\"\n",
    "        Get actions\n",
    "        \"\"\"\n",
    "\n",
    "        # distinction whetere we are in learning mode or not to handle exploration realised with noise\n",
    "        if self.learning_mode:\n",
    "            # if we are in learning mode the first x episodes we want to explore the entire action space\n",
    "            # to get a good initial experience, in the area around the costs of the agent\n",
    "            if self.collect_initial_experience_mode:\n",
    "                # define current action as soley noise\n",
    "                noise = (\n",
    "                    th.normal(\n",
    "                        mean=0.0, std=0.2, size=(1, self.act_dim), dtype=self.float_type\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .squeeze()\n",
    "                )\n",
    "\n",
    "                # =============================================================================\n",
    "                # 2.1 Get Actions and handle exploration\n",
    "                # =============================================================================\n",
    "                # ==> YOUR CODE HERE\n",
    "                base_bid = None  # TODO\n",
    "                # add niose to the last dimension of the observation\n",
    "                # needs to be adjusted if observation space is changed, because only makes sense\n",
    "                # if the last dimension of the observation space are the marginal cost\n",
    "                curr_action = noise + base_bid.clone().detach()\n",
    "\n",
    "            else:\n",
    "                # if we are not in the initial exploration phase we chose the action with the actor neuronal net\n",
    "                # and add noise to the action\n",
    "                curr_action = self.actor(next_observation).detach()\n",
    "                noise = th.tensor(\n",
    "                    self.action_noise.noise(), device=self.device, dtype=self.float_type\n",
    "                )\n",
    "                curr_action += noise\n",
    "        else:\n",
    "            # if we are not in learning mode we just use the actor neuronal net to get the action without adding noise\n",
    "\n",
    "            curr_action = self.actor(next_observation).detach()\n",
    "            noise = tuple(0 for _ in range(self.act_dim))\n",
    "\n",
    "        curr_action = curr_action.clamp(-1, 1)\n",
    "\n",
    "        return curr_action, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c792a7b",
   "metadata": {
    "id": "OTaqkwV3xcf6"
   },
   "source": [
    "#### **Solution 2.1**\n",
    "\n",
    "So how do we define the base bid?\n",
    "\n",
    "Assuming the described auction is a efficient market with full information and competition, we know that bidding the marginal costs of the power plant is the economically best bid. With the RL strategy we can recreate the abuse of market power and incomplete information, which enables us to model different market settings. Yet, starting of with the theoretically styleized optimal solution guides our RL agents porperly. As the marginal costs of the power plant are part of the oberservations we can define the base bid in the following way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d6dba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rfXJBGOKxbk7",
    "outputId": "06f76c52-e215-4998-8f61-f7492b880e4d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#base_bid = marginal costs\n",
    "base_bid = next_observation[-1] # = marginal_costs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb00a28",
   "metadata": {
    "id": "B5Hgh88Vz0wD"
   },
   "source": [
    "#### **Task 2.2**\n",
    "**Goal: Define the actual bids with the outputs of the actors**\n",
    "\n",
    "Similarly to every other output of a neuronal network, the actions are in the range of 0-1. These values need to be translated into the actual bids $a_{i,t} = [ep^\\mathrm{inflex}_{i,t}, ep^\\mathrm{flex}_{i,t}] \\in [ep^{min},ep^{max}]$. This can be done in a way that further helps the RL agent to learn, if we put some thought into.\n",
    "\n",
    "For this we go back into the calculate_bids() function and instead of just defining bids=actions, which was just a place holder, we actually make them into bids. Think about a smart way to transform them and fill the gaps in the following code. Remember:\n",
    "\n",
    "  - *bid_quantity_inflex* represent the inflexible part of the bid. This represents the minimum run capacity of the unit.\n",
    "  - *bid_quantity_flex* represent the flexible part of the bid. This represents the flexible capacity of the unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337833a5",
   "metadata": {
    "id": "Y81HzlkjNHJ0"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def calculate_bids(\n",
    "        self,\n",
    "        unit: SupportsMinMax,\n",
    "        market_config: MarketConfig,\n",
    "        product_tuples: list[Product],\n",
    "        **kwargs,\n",
    "    ) -> Orderbook:\n",
    "        \"\"\"\n",
    "        Calculate bids for a unit\n",
    "        \"\"\"\n",
    "\n",
    "        bid_quantity_inflex, bid_price_inflex = 0, 0\n",
    "        bid_quantity_flex, bid_price_flex = 0, 0\n",
    "\n",
    "        start = product_tuples[0][0]\n",
    "        end = product_tuples[0][1]\n",
    "        # get technical bounds for the unit output from the unit\n",
    "        min_power, max_power = unit.calculate_min_max_power(start, end)\n",
    "        min_power = min_power[start]\n",
    "        max_power = max_power[start]\n",
    "\n",
    "        # =============================================================================\n",
    "        # 1. Get the Observations, which are the basis of the action decision\n",
    "        # =============================================================================\n",
    "        next_observation = self.create_observation(\n",
    "            unit=unit,\n",
    "            market_id=market_config.market_id,\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "\n",
    "        # =============================================================================\n",
    "        # 2. Get the Actions, based on the observations\n",
    "        # =============================================================================\n",
    "        actions, noise = self.get_actions(next_observation)\n",
    "\n",
    "        bids = actions\n",
    "\n",
    "        # =============================================================================\n",
    "        # 3.2 Transform Actions into bids\n",
    "        # =============================================================================\n",
    "        # ==> YOUR CODE HERE\n",
    "        # actions are in the range [0,1], we need to transform them into actual bids\n",
    "        # we can use our domain knowledge to guide the bid formulation\n",
    "\n",
    "        bid_prices = None  # TODO\n",
    "\n",
    "        # calculate inflexible part of the bid\n",
    "        bid_quantity_inflex = None  # TODO\n",
    "        bid_price_inflex = None  # TODO\n",
    "\n",
    "        # calculate flexible part of the bid\n",
    "        bid_quantity_flex = None  # TODO\n",
    "        bid_price_flex = None  # TODO\n",
    "\n",
    "        # actually formulate bids in orderbook format\n",
    "        bids = [\n",
    "            {\n",
    "                \"start_time\": start,\n",
    "                \"end_time\": end,\n",
    "                \"only_hours\": None,\n",
    "                \"price\": bid_price_inflex,\n",
    "                \"volume\": bid_quantity_inflex,\n",
    "            },\n",
    "            {\n",
    "                \"start_time\": start,\n",
    "                \"end_time\": end,\n",
    "                \"only_hours\": None,\n",
    "                \"price\": bid_price_flex,\n",
    "                \"volume\": bid_quantity_flex,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # store results in unit outputs as lists to be written to the buffer for learning\n",
    "        unit.outputs[\"rl_observations\"].append(next_observation)\n",
    "        unit.outputs[\"rl_actions\"].append(actions)\n",
    "\n",
    "        # store results in unit outputs as series to be written to the database by the unit operator\n",
    "        unit.outputs[\"actions\"][start] = actions\n",
    "        unit.outputs[\"exploration_noise\"][start] = noise\n",
    "\n",
    "        bids = self.remove_empty_bids(bids)\n",
    "\n",
    "        return bids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9cd3b",
   "metadata": {
    "id": "3n-kJeOFCfRB"
   },
   "source": [
    "#### **Solution 2.2**\n",
    "\n",
    "So how do we define the actual bid from the action?\n",
    "\n",
    "We have the bid price for the minimum power (inflex) and the rest of the power. As the power plant needs to run at minimal the minum power in order to offer generation in general, it makes sense to offer this generation at a lower price than the rest of the power. Hence, we can allocate the actions to the bid prices in the following way. In addition, the actions need to be rescaled of course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84f013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "wB7X-pFkCje3",
    "outputId": "ff905a9d-e3f2-4487-9e8a-9dbf4e855ab7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#calculate actual bids\n",
    "#rescale actions to actual prices\n",
    "bid_prices = actions * self.max_bid_price\n",
    "\n",
    "#calculate inflexible part of the bid\n",
    "bid_quantity_inflex = min_power\n",
    "bid_price_inflex = min(bid_prices)\n",
    "\n",
    "#calculate flexible part of the bid\n",
    "bid_quantity_flex = max_power - bid_quantity_inflex\n",
    "bid_price_flex = max(bid_prices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452178f",
   "metadata": {
    "id": "hr15xKuGCkbn"
   },
   "source": [
    "### 3.4 Get a reward\n",
    "This step is done in the *calculate_reward*()-function, which is called after the market is cleared and we get the market feedback, so we can calculate the profit. In RL, the design of a reward function is as important as the choice of the correct algorithm. During the initial phase of the work, pure economic reward in the form of the agent's profit was used. Typically, electricity market models consider only a single restart cost. Still, in the case of using RL, the split into shut-down and start-up costs allow the agents to better differentiate between these two events and learn a better policy.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{i,t} =\n",
    "\\begin{cases}\n",
    "P^\\text{conf}_{i,t} (M_t - mc_{i,t}) dt - c^{su}_i & \\text{if $P^\\text{conf}_{i,t}$ $\\geq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $= 0$} \\\\\n",
    "P^\\text{conf}_{i,t} (M_t - mc_{i,t}) dt & \\text{if $P^\\text{conf}_{i,t}$ $\\geq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $\\neq 0$} \\\\\n",
    "- c^{sd}_i & \\text{if $P^\\text{conf}_{i,t}$ $\\leq  P^{min}_i$} \\\\\n",
    "& \\text{and $P_{i,t-1}$ $\\neq 0$} \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In this equation, the variables are:\n",
    "* $P^\\text{conf}$ the confirmed capacity on the market\n",
    "* $P^{min}$ the minimal stable capacity\n",
    "* $M$ the market clearing price\n",
    "* $mc$ the marginal generation cost\n",
    "* $dt$ the market time resolution\n",
    "* $c^{su}, c^{sd}$ the start-up and shut-down costs, respectively\n",
    "\n",
    "The profit-driven reward function was sufficient for a few agents, but the learning performance decreased significantly with more agents. Therefore, we add an additional regret term $cm$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3afb42",
   "metadata": {
    "id": "aGyaOUgo3Y8Q"
   },
   "source": [
    "#### **Task 3**\n",
    "**Goal**: Define the reward guiding the learning process of the agent.\n",
    "\n",
    "As the reward plays such a crucial role in the learning think of ways how to integrate further signals exceeding the monetary profit. One example could be integrating a regret term, namely the opportunity costs. Your task is to define the rewrad using the opportunity costs and to scale it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e813e2",
   "metadata": {
    "id": "U9HX41mODuBU"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def calculate_reward(\n",
    "        self,\n",
    "        unit,\n",
    "        marketconfig: MarketConfig,\n",
    "        orderbook: Orderbook,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate reward\n",
    "        \"\"\"\n",
    "\n",
    "        # =============================================================================\n",
    "        # 3. Calculate Reward\n",
    "        # =============================================================================\n",
    "        # function is called after the market is cleared and we get the market feedback,\n",
    "        # so we can calculate the profit\n",
    "\n",
    "        product_type = marketconfig.product_type\n",
    "\n",
    "        profit = 0\n",
    "        reward = 0\n",
    "        opportunity_cost = 0\n",
    "\n",
    "        # iterate over all orders in the orderbook, to calculate order specific profit\n",
    "        for order in orderbook:\n",
    "            start = order[\"start_time\"]\n",
    "            end = order[\"end_time\"]\n",
    "            end_excl = end - unit.index.freq\n",
    "\n",
    "            # depending on way the unit calaculates marginal costs we take costs\n",
    "            if unit.marginal_cost is not None:\n",
    "                marginal_cost = (\n",
    "                    unit.marginal_cost[start]\n",
    "                    if len(unit.marginal_cost) > 1\n",
    "                    else unit.marginal_cost\n",
    "                )\n",
    "            else:\n",
    "                marginal_cost = unit.calc_marginal_cost_with_partial_eff(\n",
    "                    power_output=unit.outputs[product_type].loc[start:end_excl],\n",
    "                    timestep=start,\n",
    "                )\n",
    "\n",
    "            duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "            # calculate profit as income - running_cost from this event\n",
    "            price_difference = order[\"accepted_price\"] - marginal_cost\n",
    "            order_profit = price_difference * order[\"accepted_volume\"] * duration\n",
    "\n",
    "            # calculate opportunity cost\n",
    "            # as the loss of income we have because we are not running at full power\n",
    "            order_opportunity_cost = (\n",
    "                price_difference\n",
    "                * (\n",
    "                    unit.max_power - unit.outputs[product_type].loc[start:end_excl]\n",
    "                ).sum()\n",
    "                * duration\n",
    "            )\n",
    "\n",
    "            # if our opportunity costs are negative, we did not miss an opportunity to earn money and we set them to 0\n",
    "            order_opportunity_cost = max(order_opportunity_cost, 0)\n",
    "\n",
    "            # collect profit and opportunity cost for all orders\n",
    "            opportunity_cost += order_opportunity_cost\n",
    "            profit += order_profit\n",
    "\n",
    "        # consideration of start-up costs, which are evenly divided between the\n",
    "        # upward and downward regulation events\n",
    "        if (\n",
    "            unit.outputs[product_type].loc[start] != 0\n",
    "            and unit.outputs[product_type].loc[start - unit.index.freq] == 0\n",
    "        ):\n",
    "            profit = profit - unit.hot_start_cost / 2\n",
    "        elif (\n",
    "            unit.outputs[product_type].loc[start] == 0\n",
    "            and unit.outputs[product_type].loc[start - unit.index.freq] != 0\n",
    "        ):\n",
    "            profit = profit - unit.hot_start_cost / 2\n",
    "\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "        # ==> YOUR CODE HERE\n",
    "        # The straight forward implemntation would be reward = profit, yet we would like to give the agent more guidance\n",
    "        # in the learning process, so we add a regret term to the reward, which is the opportunity cost\n",
    "        # define the reward and scale it\n",
    "\n",
    "        scaling = 0.1 / unit.max_power\n",
    "        regret_scale = None  # TODO\n",
    "        reward = None  # TODO\n",
    "\n",
    "        # store results in unit outputs which are written to database by unit operator\n",
    "        unit.outputs[\"profit\"].loc[start:end_excl] += profit\n",
    "        unit.outputs[\"reward\"].loc[start:end_excl] = reward\n",
    "        unit.outputs[\"regret\"].loc[start:end_excl] = opportunity_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e38ee",
   "metadata": {
    "id": "gWF7D4QA2-kz"
   },
   "source": [
    "#### **Solution 3**\n",
    "\n",
    "So how do we define the actual reward?\n",
    "\n",
    "We use the opportunity costs for further guidance, which quantify the expected contribution margin, as defined by the following equation, with $P^{max}$ as the maximal available capacity.\n",
    "\n",
    "\\begin{equation}\n",
    "cm_{i,t} = \\max[(P^{max}_i - P^\\text{conf}_{i,t}) (M_t - mc_{i,t}) dt, 0]\n",
    "\\end{equation}\n",
    "\n",
    "The regret term gives a negative signal to the agent when there is opportunity cost due to the unsold capacity, thus correcting the agent's actions. This term also introduces an increased influence of the competition between agents in learning. By minimizing the regret, the agents drive the bid prices closer to the marginal generation cost, which drives the market price down.\n",
    "\n",
    "The reward of agent $i$ at time-step $t$ is defined by the equation below.\n",
    "\n",
    "\\begin{equation}\n",
    "R_{i,t}  = \\pi_{i,t} + \\beta cm_{i,t}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\beta$ is the regret scaling factor to adjust the ratio between profit-maximizing and regret-minimizing learning.\n",
    "\n",
    "The described reward function has proven to perform well even with many agents and to accelerate learning convergence. This is because minimizing the regret term drives the overall system to equilibrium. At a point close to the equilibrium point, the average reward of all agents would converge to a constant value since further policy changes would not lead to an additional reduction in regrets or an increase in profits. Therefore, the average reward value can also be a good indicator of learning performance and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8a015",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "e1XdVXPSCo_k",
    "outputId": "585d94a5-7475-4e96-d0a1-5e82b711c6a5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scaling = 0.1 / unit.max_power\n",
    "regret_scale = 0.2\n",
    "reward = float(profit - regret_scale * opportunity_cost) * scaling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72845b",
   "metadata": {
    "id": "L3flH5iY4x7Z"
   },
   "source": [
    "### 3.5 Start the simulation\n",
    "\n",
    "We are almost done with all the changes to actually be able to make ASSUME learn here in google colab. If you would rather like to load our pretrained strategies, we need a function for loading parameters, which can be found below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa54f30",
   "metadata": {
    "id": "ZwVtpK3B5gR6"
   },
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class RLStrategy(RLStrategy):\n",
    "    def load_actor_params(self, load_path):\n",
    "        \"\"\"\n",
    "        Load actor parameters\n",
    "        \"\"\"\n",
    "        directory = f\"{load_path}/actors/actor_{self.unit_id}.pt\"\n",
    "\n",
    "        params = th.load(directory, map_location=self.device)\n",
    "\n",
    "        self.actor = self.actor_architecture_class(\n",
    "            obs_dim=self.obs_dim,\n",
    "            act_dim=self.act_dim,\n",
    "            float_type=self.float_type,\n",
    "            unique_obs_dim=self.unique_obs_dim,\n",
    "            num_timeseries_obs_dim=self.num_timeseries_obs_dim,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.actor.load_state_dict(params[\"actor\"])\n",
    "\n",
    "        if self.learning_mode:\n",
    "            self.actor_target = self.actor_architecture_class(\n",
    "                obs_dim=self.obs_dim,\n",
    "                act_dim=self.act_dim,\n",
    "                float_type=self.float_type,\n",
    "                unique_obs_dim=self.unique_obs_dim,\n",
    "                num_timeseries_obs_dim=self.num_timeseries_obs_dim,\n",
    "            ).to(self.device)\n",
    "            self.actor_target.load_state_dict(params[\"actor_target\"])\n",
    "            self.actor_target.eval()\n",
    "            self.actor.optimizer.load_state_dict(params[\"actor_optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ec7f3",
   "metadata": {
    "id": "cTlqMouufKyo"
   },
   "source": [
    "To control the learning process, the config file determines the parameters of the learning algorithm. As we want to temper with these values in the notebook we will overwrite the learning config in the next cell and then load it into our world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b299f8",
   "metadata": {
    "id": "moZ_UD7FfkOh"
   },
   "outputs": [],
   "source": [
    "learning_config = {\n",
    "    \"continue_learning\": False,\n",
    "    \"trained_policies_save_path\": \"null\",\n",
    "    \"max_bid_price\": 100,\n",
    "    \"algorithm\": \"matd3\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_episodes\": 10,\n",
    "    \"episodes_collecting_initial_experience\": 3,\n",
    "    \"train_freq\": \"24h\",\n",
    "    \"gradient_steps\": -1,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"noise_sigma\": 0.1,\n",
    "    \"noise_scale\": 1,\n",
    "    \"noise_dt\": 1,\n",
    "    \"validation_episodes_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f9429",
   "metadata": {
    "id": "ZlRnTgCy5d9W"
   },
   "source": [
    "In order to let the simulation run with the integrated learning we need to touch up the main file that runs it in the following way.\n",
    "\n",
    "In the following cell, we let the example run in case 1 of [1], where we have one big reinforcement learning power plan exists that technically can exert my power.\n",
    "\n",
    "[1] Harder, N.; Qussous, R.; Weidlich, A. Fit for purpose: Modeling wholesale electricity markets realistically with multi-agent deep reinforcement learning. *Energy and AI* **2023**. 14. 100295. https://doi.org/10.1016/j.egyai.2023.100295.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53219009",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZlWxXxZr54WV",
    "outputId": "e30f4279-7a4e-4efc-9cfb-61416e4fe2f1"
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(\n",
    "            world,\n",
    "            inputs_path=inputs_path,\n",
    "            scenario=scenario,\n",
    "            study_case=study_case,\n",
    "        )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621dd9c",
   "metadata": {},
   "source": [
    "In comparison, the following cell executes example case 2 of [1] where the same capacity of the reinforcement power plant in case 1 is divided into five reinforcement learning power plants, which hence cannot exert market power anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_02b\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(\n",
    "            world,\n",
    "            inputs_path=inputs_path,\n",
    "            scenario=scenario,\n",
    "            study_case=study_case,\n",
    "        )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c085b",
   "metadata": {},
   "source": [
    "The following simulation represents case 3, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400495c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_02c\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning( world, inputs_path=input_path, scenario=scenario, study_case=study_case, )\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(\n",
    "            world,\n",
    "            inputs_path=inputs_path,\n",
    "            scenario=scenario,\n",
    "            study_case=study_case,\n",
    "        )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512bd0b",
   "metadata": {},
   "source": [
    "### Result Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832901f6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b425f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT ident, simulation,\n",
    "sum(round(CAST(value AS numeric), 2))  FILTER (WHERE variable = 'total_cost') as total_cost,\n",
    "sum(round(CAST(value AS numeric), 2)*1000)  FILTER (WHERE variable = 'total_volume') as total_volume,\n",
    "sum(round(CAST(value AS numeric), 2))  FILTER (WHERE variable = 'avg_price') as average_cost\n",
    "FROM kpis\n",
    "where variable in ('total_cost', 'total_volume', 'avg_price')\n",
    "and simulation in ('example_02a_base', 'example_02b_base', 'example_02c_base')\n",
    "group by simulation, ident ORDER BY simulation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "kpis = pd.read_sql(sql, engine)\n",
    "\n",
    "kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6417202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe to have sho, bo and lo case in the right order\n",
    "\n",
    "# sort kpis in the order sho, bo, lo\n",
    "\n",
    "kpis = kpis.sort_values(\n",
    "    by=\"simulation\",\n",
    "    #    key=lambda x: x.map({\"example_02a\": 1, \"example_02b\": 2, \"example_02c\": 3}),\n",
    ")\n",
    "\n",
    "\n",
    "kpis[\"total_volume\"] /= 1e9\n",
    "kpis[\"total_cost\"] /= 1e6\n",
    "savefig = partial(plt.savefig, transparent=False, bbox_inches=\"tight\")\n",
    "\n",
    "xticks = kpis[\"simulation\"].unique()\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax2 = ax.twinx()  # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "kpis.total_volume.plot(kind=\"bar\", ax=ax, width=width, position=1, color=\"royalblue\")\n",
    "kpis.total_cost.plot(kind=\"bar\", ax=ax2, width=width, position=0, color=\"green\")\n",
    "\n",
    "# set x-achxis limits\n",
    "ax.set_xlim(-0.6, len(kpis[\"simulation\"]) - 0.4)\n",
    "\n",
    "# set y-achxis limits\n",
    "ax.set_ylim(0, max(kpis.total_volume) * 1.1 + 0.1)\n",
    "ax2.set_ylim(0, max(kpis.total_cost) * 1.1 + 0.1)\n",
    "\n",
    "ax.set_ylabel(\"Total Volume (GWh)\")\n",
    "ax2.set_ylabel(\"Total Cost (M€)\")\n",
    "\n",
    "ax.set_xticklabels(xticks, rotation=45)\n",
    "ax.set_xlabel(\"Simulation\")\n",
    "\n",
    "ax.legend([\"Total Volume\"], loc=\"upper left\")\n",
    "ax2.legend([\"Total Cost\"], loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Total Volume and Total Cost for each Simulation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT\n",
    "  product_start AS \"time\",\n",
    "  price AS \"Price\",\n",
    "  simulation AS \"simulation\",\n",
    "  node\n",
    "FROM market_meta\n",
    "WHERE simulation in ('example_02a_base', 'example_02b_base', 'example_02c_base') AND market_id in ('EOM') \n",
    "GROUP BY market_id, simulation, product_start, price, node\n",
    "ORDER BY product_start, node\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql, engine)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a884224",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Convert the 'time' column to datetime\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Loop through each simulation and plot\n",
    "for simulation in df[\"simulation\"].unique():\n",
    "    subset = df[df[\"simulation\"] == simulation]\n",
    "    plt.plot(subset[\"time\"], subset[\"Price\"], label=simulation)\n",
    "\n",
    "plt.title(\"Price over Time for Different Simulations\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(title=\"Simulation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698b545",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
