{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining data for MA-DPG evaluation form example 02b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# assume module imports\n",
    "import examples.examples as examples\n",
    "from assume import World\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_file,\n",
    "    load_scenario_folder,\n",
    "    run_learning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running example 02b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"small_learning_1\"\n",
    "db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "inputs_dir = \"../inputs\"\n",
    "\n",
    "scenario = examples.available_examples[example][\"scenario\"]\n",
    "study_case = examples.available_examples[example][\"study_case\"]\n",
    "csv_path = \"outputs/\" + scenario + \"/\" + study_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run example 02b\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "load_scenario_folder(world, inputs_dir, scenario, study_case)\n",
    "run_learning(\n",
    "    world,\n",
    "    inputs_dir,\n",
    "    scenario,\n",
    "    study_case,\n",
    ")\n",
    "world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving the data from the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best actors directory\n",
    "best_actors_dir = os.path.join(\n",
    "    inputs_dir,\n",
    "    scenario,\n",
    "    \"learned_strategies\",\n",
    "    study_case,\n",
    "    \"avg_reward_eval_policies/actors/\",\n",
    ")\n",
    "actors = os.listdir(best_actors_dir)\n",
    "actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Retrieving best run actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the config file\n",
    "config_path = os.path.join(inputs_dir, scenario, \"config.yaml\")\n",
    "\n",
    "# Read the number of validation episodes from the config file\n",
    "with open(config_path) as file:\n",
    "    config = yaml.safe_load(file)[study_case]\n",
    "learning_config = config[\"learning_config\"]\n",
    "no_of_val_episodes = (\n",
    "    learning_config[\"training_episodes\"]\n",
    "    - learning_config[\"episodes_collecting_initial_experience\"]\n",
    ") // learning_config.get(\"validation_episodes_interval\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the database connection\n",
    "db = create_engine(db_uri)\n",
    "simulation = f\"{scenario}_{study_case}_eval\"\n",
    "\n",
    "# Get the average reward for each episode in order to determine the best episode.\n",
    "reward_df = pd.DataFrame(columns=[\"avg_reward\"], index=range(1, no_of_val_episodes + 1))\n",
    "for episode in range(1, no_of_val_episodes + 1):\n",
    "    query = f\"SELECT AVG(reward) as avg_reward FROM rl_params where simulation = '{simulation}_{episode}'\"\n",
    "    reward_df.at[episode, \"avg_reward\"] = pd.read_sql(query, db).values[0][0]\n",
    "reward_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the episode with the best reward to get the respective actions\n",
    "episode = reward_df[\"avg_reward\"].idxmax()\n",
    "query = f\"SELECT datetime as dt, unit, actions_0, actions_1 FROM rl_params where simulation = '{simulation}_{episode}'\"\n",
    "actions_df = pd.read_sql(query, db)\n",
    "actions_df.index = pd.to_datetime(actions_df[\"dt\"])\n",
    "actions_df.drop(columns=[\"dt\"], inplace=True)\n",
    "actions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting the demand dataframe and power plant units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp(config[\"start_date\"])\n",
    "end = pd.Timestamp(config[\"end_date\"])\n",
    "\n",
    "index = pd.date_range(\n",
    "    start=start,\n",
    "    end=end,\n",
    "    freq=config[\"time_step\"],\n",
    ")\n",
    "\n",
    "demand_df = load_file(\n",
    "    os.path.join(inputs_dir, scenario), config, file_name=\"demand_df\", index=index\n",
    ")\n",
    "demand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_units = pd.read_csv(\n",
    "    os.path.join(inputs_dir, scenario, \"powerplant_units.csv\"), index_col=0\n",
    ")\n",
    "pp_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ich brauche die Daten für den dispatch vom best run\n",
    "# heißt hier muss die normale simulation (kein eval run) noch mal laufen gelassen werden mit den best_actors von vorher, sonst speichert er die dispatch Daten nicht\n",
    "# sofern ich das richtig sehe\n",
    "\n",
    "episode = reward_df[\"avg_reward\"].idxmax()\n",
    "query = \"SELECT * FROM unit_dispatch where simulation = 'example_02b_base'\"\n",
    "dispatch_df = pd.read_sql(query, db)\n",
    "\n",
    "# drop duplicates time and unit and keep second\n",
    "# TODO: fix double logging of dispatch_df (future I guess, ich hab nen quick fix gemacht)\n",
    "dispatch_df = dispatch_df.drop_duplicates(subset=[\"time\", \"unit\"], keep=\"first\")\n",
    "dispatch_df[\"time\"] = pd.to_datetime(dispatch_df[\"time\"])\n",
    "dispatch_df = dispatch_df[dispatch_df[\"time\"] != pd.to_datetime(\"2019-03-01 00:00:00\")]\n",
    "dispatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of best dispatch run:\n",
    "class NoAliasDumper(yaml.SafeDumper):\n",
    "    def ignore_aliases(self, data):\n",
    "        return True\n",
    "\n",
    "\n",
    "# Read the existing config file\n",
    "with open(config_path) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "if \"base_dispatch\" in config:\n",
    "    del config[\"base_dispatch\"]\n",
    "# Copy the base and new base_dispatch configuration\n",
    "base_config = config[\"base\"].copy()\n",
    "base_dispatch = config[\"base\"].copy()\n",
    "base_dispatch[\"learning_config\"] = base_config[\"learning_config\"].copy()\n",
    "\n",
    "# Modify learning config parameters for base_dispatch\n",
    "base_dispatch[\"learning_config\"].update(\n",
    "    {\n",
    "        \"continue_learning\": True,\n",
    "        \"trained_policies_save_path\": \"learned_strategies/base_dispatch/last_policies\",\n",
    "        \"trained_policies_load_path\": \"learned_strategies/base_dispatch/avg_reward_eval_policies\",\n",
    "        \"training_episodes\": 0,\n",
    "        \"episodes_collecting_initial_experience\": 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update the config with both sections\n",
    "config[\"base\"] = base_config\n",
    "config[\"base_dispatch\"] = base_dispatch\n",
    "\n",
    "# Write the updated config back to file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(\n",
    "        config,\n",
    "        file,\n",
    "        Dumper=NoAliasDumper,\n",
    "        default_flow_style=False,\n",
    "        sort_keys=False,\n",
    "    )\n",
    "\n",
    "# Define paths\n",
    "base_dir = Path(os.path.join(inputs_dir, scenario, \"learned_strategies/base\"))\n",
    "dispatch_dir = Path(\n",
    "    os.path.join(inputs_dir, scenario, \"learned_strategies/base_dispatch\")\n",
    ")\n",
    "\n",
    "# Check if source directory exists\n",
    "if not base_dir.exists():\n",
    "    print(f\"Source directory {base_dir} does not exist!\")\n",
    "elif dispatch_dir.exists():\n",
    "    print(f\"Target directory {dispatch_dir} already exists!\")\n",
    "else:\n",
    "    # Create target directory if it doesn't exist\n",
    "    dispatch_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy directory\n",
    "    shutil.copytree(base_dir, dispatch_dir)\n",
    "    print(f\"Successfully copied {base_dir} to {dispatch_dir}\")\n",
    "\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "load_scenario_folder(world, inputs_dir, scenario, \"base_dispatch\")\n",
    "run_learning(\n",
    "    world,\n",
    "    inputs_dir,\n",
    "    scenario,\n",
    "    \"base_dispatch\",\n",
    ")\n",
    "\n",
    "world.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM unit_dispatch where simulation = 'base_dispatch'\"\n",
    "dispatch_df2 = pd.read_sql(query, db)\n",
    "dispatch_df2 = dispatch_df2.drop_duplicates(subset=[\"time\", \"unit\"], keep=\"first\")\n",
    "dispatch_df2[\"time\"] = pd.to_datetime(dispatch_df[\"time\"])\n",
    "dispatch_df2 = dispatch_df2[\n",
    "    dispatch_df2[\"time\"] != pd.to_datetime(\"2019-03-01 00:00:00\")\n",
    "]\n",
    "dispatch_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check are these only the accepted offers?\n",
    "# We need all, because potentially a different behavior of the agent we optimise and validate could change the market so that a former rejected offer is now accepted\n",
    "\n",
    "episode = reward_df[\"avg_reward\"].idxmax()\n",
    "query = f\"SELECT * FROM market_orders where simulation = '{scenario}_{study_case}'\"\n",
    "market_orders_df = pd.read_sql(query, db)\n",
    "\n",
    "# As I understand it, the market_orders table contains all orders, including those that were not accepted.\n",
    "# If they are not accepted, the accepted_volume column is 0.\n",
    "# When filtering for all bid_ids that end with \"_1\", and checking if the length matches the length of the dispatch_df,\n",
    "# we can see if the dispatch_df contains all accepted orders.\n",
    "# When running:\n",
    "check_only_accepted = market_orders_df[\n",
    "    market_orders_df[\"bid_id\"].str.endswith(\"_1\")\n",
    "].copy()\n",
    "print(f\"Lengths are matching: {len(check_only_accepted) == len(dispatch_df)}\")\n",
    "\n",
    "market_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Establish Sampling of days to be analysed\n",
    "\n",
    "Here we sample from the entire training data a subset of days, for which we test if the profit of all drl agents is similar to their MPEC formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_EOM</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:00:00</th>\n",
       "      <td>5180.4</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 01:00:00</th>\n",
       "      <td>4940.1</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 02:00:00</th>\n",
       "      <td>4790.7</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 03:00:00</th>\n",
       "      <td>4749.2</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 04:00:00</th>\n",
       "      <td>4771.9</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 19:00:00</th>\n",
       "      <td>5533.8</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 20:00:00</th>\n",
       "      <td>5264.4</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 21:00:00</th>\n",
       "      <td>4947.3</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 22:00:00</th>\n",
       "      <td>4786.8</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 23:00:00</th>\n",
       "      <td>4481.6</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     demand_EOM        date\n",
       "2019-03-16 00:00:00      5180.4  2019-03-16\n",
       "2019-03-16 01:00:00      4940.1  2019-03-16\n",
       "2019-03-16 02:00:00      4790.7  2019-03-16\n",
       "2019-03-16 03:00:00      4749.2  2019-03-16\n",
       "2019-03-16 04:00:00      4771.9  2019-03-16\n",
       "...                         ...         ...\n",
       "2019-03-30 19:00:00      5533.8  2019-03-30\n",
       "2019-03-30 20:00:00      5264.4  2019-03-30\n",
       "2019-03-30 21:00:00      4947.3  2019-03-30\n",
       "2019-03-30 22:00:00      4786.8  2019-03-30\n",
       "2019-03-30 23:00:00      4481.6  2019-03-30\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_seasonal_weeks(datetime_index):\n",
    "    \"\"\"\n",
    "    Sample one random complete week from each season.\n",
    "\n",
    "    Args:\n",
    "        datetime_index (pd.DatetimeIndex): DatetimeIndex of the DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DatetimeIndex: Combined index of four sampled weeks (one per season)\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Define seasons by month numbers\n",
    "    seasons = {\n",
    "        \"Spring\": [3, 4, 5],\n",
    "        \"Summer\": [6, 7, 8],\n",
    "        \"Fall\": [9, 10, 11],\n",
    "        \"Winter\": [12, 1, 2],\n",
    "    }\n",
    "\n",
    "    sampled_dates = []\n",
    "\n",
    "    for season, months in seasons.items():\n",
    "        # Get seasonal data indices\n",
    "        seasonal_idx = datetime_index[datetime_index.month.isin(months)]\n",
    "\n",
    "        # Find complete weeks within season\n",
    "        complete_weeks = []\n",
    "        for week in seasonal_idx.isocalendar().week.unique():\n",
    "            week_idx = datetime_index[datetime_index.isocalendar().week == week]\n",
    "            # Check if week is complete (168 hours) and fully within season\n",
    "            if len(week_idx) == 168 and all(\n",
    "                month in months for month in week_idx.month.unique()\n",
    "            ):\n",
    "                complete_weeks.append(week)\n",
    "\n",
    "        if complete_weeks:\n",
    "            random_week = random.choice(complete_weeks)\n",
    "            week_idx = datetime_index[datetime_index.isocalendar().week == random_week]\n",
    "            sampled_dates.extend([d.date() for d in week_idx])\n",
    "\n",
    "        print(f\"{season} complete weeks: {complete_weeks}\")\n",
    "\n",
    "    return sorted(list(set(sampled_dates)))\n",
    "\n",
    "\n",
    "sampled_indices = sample_seasonal_weeks(demand_df.index)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get sample subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_EOM</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:00:00</th>\n",
       "      <td>5180.4</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 01:00:00</th>\n",
       "      <td>4940.1</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 02:00:00</th>\n",
       "      <td>4790.7</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 03:00:00</th>\n",
       "      <td>4749.2</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 04:00:00</th>\n",
       "      <td>4771.9</td>\n",
       "      <td>2019-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 19:00:00</th>\n",
       "      <td>5533.8</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 20:00:00</th>\n",
       "      <td>5264.4</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 21:00:00</th>\n",
       "      <td>4947.3</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 22:00:00</th>\n",
       "      <td>4786.8</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30 23:00:00</th>\n",
       "      <td>4481.6</td>\n",
       "      <td>2019-03-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     demand_EOM        date\n",
       "2019-03-16 00:00:00      5180.4  2019-03-16\n",
       "2019-03-16 01:00:00      4940.1  2019-03-16\n",
       "2019-03-16 02:00:00      4790.7  2019-03-16\n",
       "2019-03-16 03:00:00      4749.2  2019-03-16\n",
       "2019-03-16 04:00:00      4771.9  2019-03-16\n",
       "...                         ...         ...\n",
       "2019-03-30 19:00:00      5533.8  2019-03-30\n",
       "2019-03-30 20:00:00      5264.4  2019-03-30\n",
       "2019-03-30 21:00:00      4947.3  2019-03-30\n",
       "2019-03-30 22:00:00      4786.8  2019-03-30\n",
       "2019-03-30 23:00:00      4481.6  2019-03-30\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_df[\"date\"] = demand_df.index.date\n",
    "sample_demand_df = demand_df.loc[demand_df[\"date\"].isin(sampled_indices)]\n",
    "rest_demand_df = demand_df.loc[~demand_df[\"date\"].isin(sampled_indices)]\n",
    "sample_demand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df[\"date\"] = actions_df.index.date\n",
    "\n",
    "sample_actions_df = actions_df.loc[actions_df[\"date\"].isin(sampled_indices)]\n",
    "rest_actions_df = actions_df.loc[~actions_df[\"date\"].isin(sampled_indices)]\n",
    "sample_actions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_df.index = pd.to_datetime(dispatch_df[\"time\"])\n",
    "dispatch_df = dispatch_df.drop(columns=[\"time\"])\n",
    "dispatch_df[\"date\"] = dispatch_df.index.date\n",
    "\n",
    "sample_dispatch_df = dispatch_df.loc[dispatch_df[\"date\"].isin(sampled_indices)]\n",
    "rest_dispatch_df = dispatch_df.loc[~dispatch_df[\"date\"].isin(sampled_indices)]\n",
    "sample_dispatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample market orders as well\n",
    "market_orders_df.index = pd.to_datetime(market_orders_df[\"start_time\"])\n",
    "market_orders_df = market_orders_df.drop(columns=[\"start_time\"])\n",
    "market_orders_df[\"date\"] = market_orders_df.index.date\n",
    "\n",
    "sample_market_orders_df = market_orders_df.loc[\n",
    "    market_orders_df[\"date\"].isin(sampled_indices)\n",
    "]\n",
    "rest_market_orders_df = market_orders_df.loc[\n",
    "    ~market_orders_df[\"date\"].isin(sampled_indices)\n",
    "]\n",
    "sample_market_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Analyse sample distribution in comparison to entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list([\"green\"] * len(rest_demand_df)) + list([\"blue\"] * len(sample_demand_df))\n",
    "\n",
    "# Scatter matrix\n",
    "fig = pd.plotting.scatter_matrix(\n",
    "    pd.concat([rest_demand_df, sample_demand_df], sort=False),\n",
    "    c=colors,\n",
    "    figsize=(7, 7),\n",
    "    range_padding=0.2,\n",
    "    hist_kwds={\"bins\": 20},  # Generic histogram configuration\n",
    "    s=30,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Customize histogram colors for each diagonal\n",
    "hist_colors = [\"green\", \"blue\"]\n",
    "for i, ax in enumerate(fig.diagonal()):\n",
    "    data_combined = pd.concat([rest_demand_df.iloc[:, i], sample_demand_df.iloc[:, i]])\n",
    "    ax.hist(\n",
    "        [rest_demand_df.iloc[:, i], sample_demand_df.iloc[:, i]],\n",
    "        bins=20,\n",
    "        color=hist_colors,\n",
    "        stacked=True,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bi-Level Optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MPEC.bilevel_opt import find_optimal_dispatch\n",
    "from MPEC.uc_problem import solve_uc_problem\n",
    "from MPEC.utils import calculate_profits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defintion for case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"test_case\"\n",
    "\n",
    "big_w = 100000  # weight for duality gap objective\n",
    "k_max = 2  # maximum multiplier for strategic bidding\n",
    "\n",
    "start = pd.to_datetime(\"2019-03-02 06:00\")\n",
    "end = pd.to_datetime(\"2019-03-02 14:00\")\n",
    "\n",
    "# gens\n",
    "gens_df = pp_units.copy()\n",
    "\n",
    "# 24 hours of demand first increasing and then decreasing\n",
    "demand_df = sample_demand_df.copy()\n",
    "demand_df.index = pd.to_datetime(demand_df.index)\n",
    "demand_df = demand_df.loc[start:end]\n",
    "# reset index to start at 0\n",
    "demand_df = demand_df.reset_index(drop=True)\n",
    "demand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data transformation for Optimisation Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform gen_df into the format that is expected by the optimization problem\n",
    "# g_max\tmc\tu_0\tg_0\tr_up\tr_down\tk_up\tk_down\n",
    "gens_df = gens_df.reset_index()\n",
    "gens_df = gens_df.rename(columns={\"max_power\": \"g_max\", \"min_power\": \"u_0\"})\n",
    "gens_df[\"r_up\"] = gens_df[\"g_max\"]  # ramping up constraints\n",
    "gens_df[\"r_down\"] = gens_df[\"g_max\"]  # ramping down constraints\n",
    "gens_df[\"k_up\"] = 0  # start up costs\n",
    "gens_df[\"k_down\"] = 0  # shut down costs\n",
    "gens_df[\"g_0\"] = 0  # start with no power output\n",
    "\n",
    "# get average mc from dispatch_df per unit name\n",
    "mc = dispatch_df.groupby(\"unit\")[\"energy_marginal_costs\"].mean()\n",
    "\n",
    "# based on name and unit column join mc into gens_df\n",
    "gens_df = gens_df.merge(mc, left_on=\"name\", right_on=\"unit\", how=\"left\")\n",
    "gens_df = gens_df.rename(columns={\"energy_marginal_costs\": \"mc\"})\n",
    "gens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate actions of RL model into k_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on both 'unit_id' and 'time' columns\n",
    "merged_df = sample_market_orders_df.merge(\n",
    "    sample_dispatch_df.reset_index(),\n",
    "    left_on=[\"unit_id\", \"start_time\"],\n",
    "    right_on=[\"unit\", \"time\"],\n",
    "    how=\"right\",\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to translate the 2 actions per unit into one k_value? Currently:\n",
    "# get max price per unit_id and date in the dataframe\n",
    "id_k = merged_df.groupby([\"unit_id\", \"time\"])[\"price\"].idxmax()\n",
    "k_df = merged_df.loc[id_k]\n",
    "k_df = k_df[k_df[\"unit_id\"].isin(gens_df[\"name\"])]\n",
    "\n",
    "mc_mapping = dict(zip(gens_df[\"name\"], gens_df[\"mc\"]))\n",
    "k_df[\"gens_df_mc\"] = k_df[\"unit_id\"].map(mc_mapping)\n",
    "\n",
    "# transformed actions into k_values, one per generator\n",
    "k_df[\"k\"] = k_df[\"price\"] / (k_df[\"energy_marginal_costs\"] * k_df[\"gens_df_mc\"])\n",
    "\n",
    "# replace inf with 0\n",
    "k_df[\"k\"] = k_df[\"k\"].replace(np.inf, 0)\n",
    "\n",
    "k_values_df = k_df.pivot(index=\"time\", columns=\"unit_id\", values=\"k\")\n",
    "# k_values_df.reset_index(inplace=True)\n",
    "k_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop demand_EOM column\n",
    "# demand_df=demand_df.drop(columns=['demand_EOM'])\n",
    "\n",
    "# sort columns to match the order of the columns in the gens_df\n",
    "k_values_df = k_values_df[gens_df[\"name\"].values]\n",
    "\n",
    "# rename columns to match index of gens_df\n",
    "k_values_df.columns = gens_df.index\n",
    "k_values_df.reset_index(inplace=True)\n",
    "k_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join demand and price bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join sample demand df and sample market orders where unit id is demand_EOM based on index\n",
    "sample_demand_df[\"price\"] = sample_market_orders_df[\n",
    "    sample_market_orders_df[\"unit_id\"] == \"demand_EOM\"\n",
    "][\"price\"]\n",
    "\n",
    "# drop date column\n",
    "sample_demand_df = sample_demand_df.drop(columns=[\"date\"])\n",
    "\n",
    "# rename index and columns\n",
    "sample_demand_df.index.name = \"datetime\"\n",
    "sample_demand_df.columns = [\"volume\", \"price\"]\n",
    "demand_df = sample_demand_df.copy()\n",
    "demand_df = demand_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_gen = 5  # generator index that is allowed to bid strategically\n",
    "print(\"We now optimize the decison for unit \", gens_df.index[opt_gen])\n",
    "\n",
    "main_df, supp_df, k_values = find_optimal_dispatch(\n",
    "    gens_df=gens_df,\n",
    "    k_values_df=k_values_df,\n",
    "    demand_df=demand_df,\n",
    "    k_max=k_max,\n",
    "    opt_gen=opt_gen,\n",
    "    big_w=big_w,\n",
    "    time_limit=3600,\n",
    "    print_results=True,\n",
    "    K=5,\n",
    "    big_M=10e6,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# calculate actual market clearing prices\n",
    "k_values_df_2 = k_values_df.copy()\n",
    "k_values_df_2[opt_gen] = k_values\n",
    "\n",
    "updated_main_df_2, updated_supp_df_2 = solve_uc_problem(\n",
    "    gens_df, demand_df, k_values_df_2\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Calculate profits\n",
    "profits_1 = calculate_profits(main_df=main_df, supp_df=supp_df, gens_df=gens_df)\n",
    "profits_2 = calculate_profits(\n",
    "    main_df=updated_main_df_2, supp_df=updated_supp_df_2, gens_df=gens_df\n",
    ")\n",
    "\n",
    "print(f\"Estimated Profits: {profits_1[opt_gen].sum():.2f}\")\n",
    "print(f\"True profits: {profits_2[opt_gen].sum():.2f}\")\n",
    "\n",
    "# in percentage\n",
    "# print(\n",
    "#    f\"Profits difference: {100 * (profits_2[opt_gen].sum() - profits_1[opt_gen].sum()) / profits_1[opt_gen].sum():.2f}%\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create index from 0 to 71\n",
    "index = range(72)\n",
    "\n",
    "# Create 5 power plant columns\n",
    "columns = [f\"pp_{i}\" for i in range(1, 6)]\n",
    "\n",
    "# Create random data between 200 and 300\n",
    "df_rl = pd.DataFrame(\n",
    "    np.random.uniform(200, 300, size=(72, 5)), index=index, columns=columns\n",
    ")\n",
    "\n",
    "df_mpec = pd.DataFrame(\n",
    "    np.random.uniform(200, 300, size=(72, 5)), index=index, columns=columns\n",
    ")\n",
    "\n",
    "print(\"RL Profits:\")\n",
    "print(df_rl.head())\n",
    "print(\"\\nMPEC Profits:\")\n",
    "print(df_mpec.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profit_comparison_plot(df_rl, df_mpec, bound=-10):\n",
    "    # Calculate percentage deviation\n",
    "    percent_deviation = ((df_rl - df_mpec) / df_mpec) * 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot(\n",
    "        [percent_deviation[col].values for col in percent_deviation.columns],\n",
    "        showmeans=False,\n",
    "        showmedians=False,\n",
    "        showextrema=False,\n",
    "    )\n",
    "\n",
    "    # Customize violin plot colors\n",
    "    for pc in parts[\"bodies\"]:\n",
    "        pc.set_facecolor(\"lightblue\")\n",
    "        pc.set_alpha(0.7)\n",
    "\n",
    "    # Add box plot inside violin plot\n",
    "    ax.boxplot(\n",
    "        [percent_deviation[col].values for col in percent_deviation.columns],\n",
    "        positions=range(1, len(percent_deviation.columns) + 1),\n",
    "        widths=0.2,\n",
    "        showfliers=True,\n",
    "        notch=True,\n",
    "    )\n",
    "\n",
    "    # Add horizontal lines and colored regions\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7, linewidth=1.5)\n",
    "    ax.axhline(y=bound, color=\"black\", linestyle=\"--\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    # Create background colors for different regions\n",
    "    plt.axhspan(\n",
    "        0,\n",
    "        max(percent_deviation.max()) + 10,\n",
    "        color=\"lightgreen\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit > MPEC profit\",\n",
    "    )\n",
    "    plt.axhspan(\n",
    "        bound,\n",
    "        0,\n",
    "        color=\"yellow\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit < MPEC profit but in bounds\",\n",
    "    )\n",
    "    plt.axhspan(\n",
    "        min(percent_deviation.min()) - 5,\n",
    "        bound,\n",
    "        color=\"lightcoral\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit < MPEC profit outside bounds\",\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel(\"Power Plant Units\")\n",
    "    ax.set_ylabel(\"Deviation (%)\\n(RL - MPEC) / MPEC\")\n",
    "    ax.set_title(\"Profit Deviation Distribution (Combined Violin and Box Plot)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Set x-ticks\n",
    "    ax.set_xticks(range(1, len(percent_deviation.columns) + 1))\n",
    "    ax.set_xticklabels(percent_deviation.columns)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Create and show the plot\n",
    "fig = create_profit_comparison_plot(df_rl, df_mpec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
