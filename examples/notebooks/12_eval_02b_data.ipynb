{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining data for MA-DPG evaluation form example 02b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# assume module imports\n",
    "import examples.examples as examples\n",
    "from assume import World\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_file,\n",
    "    load_scenario_folder,\n",
    "    run_learning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"small_learning_1\"\n",
    "db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "inputs_dir = \"examples/inputs\"\n",
    "\n",
    "scenario = examples.available_examples[example][\"scenario\"]\n",
    "study_case = examples.available_examples[example][\"study_case\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_dir := os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving the data from the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best actors directory\n",
    "best_actors_dir = os.path.join(\n",
    "    inputs_dir,\n",
    "    scenario,\n",
    "    \"learned_strategies\",\n",
    "    study_case,\n",
    "    \"avg_reward_eval_policies/actors/\",\n",
    ")\n",
    "actors = os.listdir(best_actors_dir)\n",
    "actors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Retrieving best run actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the config file\n",
    "config_path = os.path.join(inputs_dir, scenario, \"config.yaml\")\n",
    "\n",
    "# Read the number of validation episodes from the config file\n",
    "with open(config_path) as file:\n",
    "    config = yaml.safe_load(file)[study_case]\n",
    "learning_config = config[\"learning_config\"]\n",
    "no_of_val_episodes = (\n",
    "    learning_config[\"training_episodes\"]\n",
    "    - learning_config[\"episodes_collecting_initial_experience\"]\n",
    ") // learning_config.get(\"validation_episodes_interval\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the database connection\n",
    "db = create_engine(db_uri)\n",
    "simulation = f\"{scenario}_{study_case}_eval\"\n",
    "\n",
    "# Get the average reward for each episode in order to determine the best episode.\n",
    "reward_df = pd.DataFrame(columns=[\"avg_reward\"], index=range(1, no_of_val_episodes + 1))\n",
    "for episode in range(1, no_of_val_episodes + 1):\n",
    "    query = f\"SELECT AVG(reward) as avg_reward FROM rl_params where simulation = '{simulation}_{episode}'\"\n",
    "    reward_df.at[episode, \"avg_reward\"] = pd.read_sql(query, db).values[0][0]\n",
    "reward_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the episode with the best reward to get the respective actions\n",
    "episode = reward_df[\"avg_reward\"].idxmax()\n",
    "query = f\"SELECT datetime as dt, unit, actions_0, actions_1 FROM rl_params where simulation = '{simulation}_{episode}'\"\n",
    "actions_df = pd.read_sql(query, db)\n",
    "actions_df.index = pd.to_datetime(actions_df[\"dt\"])\n",
    "actions_df.drop(columns=[\"dt\"], inplace=True)\n",
    "actions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting the demand dataframe and power plant units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp(config[\"start_date\"])\n",
    "end = pd.Timestamp(config[\"end_date\"])\n",
    "\n",
    "index = pd.date_range(\n",
    "    start=start,\n",
    "    end=end,\n",
    "    freq=config[\"time_step\"],\n",
    ")\n",
    "\n",
    "demand_df = load_file(\n",
    "    os.path.join(inputs_dir, scenario), config, file_name=\"demand_df\", index=index\n",
    ")\n",
    "demand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_units = pd.read_csv(\n",
    "    os.path.join(inputs_dir, scenario, \"powerplant_units.csv\"), index_col=0\n",
    ")\n",
    "pp_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of best dispatch run:\n",
    "class NoAliasDumper(yaml.SafeDumper):\n",
    "    def ignore_aliases(self, data):\n",
    "        return True\n",
    "\n",
    "\n",
    "# Read the existing config file\n",
    "with open(config_path) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "if f\"{study_case}_dispatch\" in config:\n",
    "    del config[f\"{study_case}_dispatch\"]\n",
    "# Copy the base and new base_dispatch configuration\n",
    "base_config = config[study_case].copy()\n",
    "base_dispatch = config[study_case].copy()\n",
    "base_dispatch[\"learning_config\"] = base_config[\"learning_config\"].copy()\n",
    "\n",
    "# Modify learning config parameters for base_dispatch\n",
    "base_dispatch[\"learning_config\"].update(\n",
    "    {\n",
    "        \"continue_learning\": False,\n",
    "        \"trained_policies_save_path\": \"learned_strategies/base_dispatch/last_policies\",\n",
    "        \"trained_policies_load_path\": \"learned_strategies/base_dispatch/avg_reward_eval_policies\",\n",
    "        \"training_episodes\": 0,\n",
    "        \"episodes_collecting_initial_experience\": 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "base_dispatch.update(\n",
    "    {\n",
    "        \"learning_mode\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update the config with both sections\n",
    "config[study_case] = base_config\n",
    "config[f\"{study_case}_dispatch\"] = base_dispatch\n",
    "\n",
    "# # Write the updated config back to file\n",
    "# with open(config_path, \"w\") as file:\n",
    "#     yaml.dump(\n",
    "#         config,\n",
    "#         file,\n",
    "#         Dumper=NoAliasDumper,\n",
    "#         default_flow_style=False,\n",
    "#         sort_keys=False,\n",
    "#     )\n",
    "\n",
    "# Define paths\n",
    "base_dir = Path(os.path.join(inputs_dir, scenario, f\"learned_strategies/{study_case}\"))\n",
    "dispatch_dir = Path(\n",
    "    os.path.join(inputs_dir, scenario, f\"learned_strategies/{study_case}_dispatch\")\n",
    ")\n",
    "\n",
    "# # Check if source directory exists\n",
    "# if not base_dir.exists():\n",
    "#     print(f\"Source directory {base_dir} does not exist!\")\n",
    "# elif dispatch_dir.exists():\n",
    "#     print(f\"Target directory {dispatch_dir} already exists!\")\n",
    "# else:\n",
    "#     # Create target directory if it doesn't exist\n",
    "#     dispatch_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Copy directory\n",
    "#     shutil.copytree(base_dir, dispatch_dir)\n",
    "#     print(f\"Successfully copied {base_dir} to {dispatch_dir}\")\n",
    "\n",
    "# world = World(database_uri=db_uri)\n",
    "\n",
    "load_scenario_folder(world, inputs_dir, scenario, f\"{study_case}_dispatch\")\n",
    "\n",
    "# world.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM unit_dispatch where simulation = '{scenario}_{study_case}_dispatch'\"\n",
    "dispatch_df = pd.read_sql(query, db)\n",
    "dispatch_df = dispatch_df.drop_duplicates(subset=[\"time\", \"unit\"], keep=\"first\")\n",
    "\n",
    "dispatch_df = dispatch_df.sort_values('time')\n",
    "dispatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = reward_df[\"avg_reward\"].idxmax()\n",
    "query = f\"SELECT * FROM market_orders where simulation = '{scenario}_{study_case}'\"\n",
    "market_orders_df = pd.read_sql(query, db)\n",
    "\n",
    "market_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Establish Sampling of days to be analysed\n",
    "\n",
    "Here we sample from the entire training data a subset of days, for which we test if the profit of all drl agents is similar to their MPEC formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_seasonal_weeks(datetime_index):\n",
    "    \"\"\"\n",
    "    Sample one random complete week from each season.\n",
    "\n",
    "    Args:\n",
    "        datetime_index (pd.DatetimeIndex): DatetimeIndex of the DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DatetimeIndex: Combined index of four sampled weeks (one per season)\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Define seasons by month numbers\n",
    "    seasons = {\n",
    "        \"Spring\": [3, 4, 5],\n",
    "        \"Summer\": [6, 7, 8],\n",
    "        \"Fall\": [9, 10, 11],\n",
    "        \"Winter\": [12, 1, 2],\n",
    "    }\n",
    "\n",
    "    sampled_dates = []\n",
    "\n",
    "    for season, months in seasons.items():\n",
    "        # Get seasonal data indices\n",
    "        seasonal_idx = datetime_index[datetime_index.month.isin(months)]\n",
    "\n",
    "        # Find complete weeks within season\n",
    "        complete_weeks = []\n",
    "        for week in seasonal_idx.isocalendar().week.unique():\n",
    "            week_idx = datetime_index[datetime_index.isocalendar().week == week]\n",
    "            # Check if week is complete (168 hours) and fully within season\n",
    "            if len(week_idx) == 168 and all(\n",
    "                month in months for month in week_idx.month.unique()\n",
    "            ):\n",
    "                complete_weeks.append(week)\n",
    "\n",
    "        if complete_weeks:\n",
    "            random_week = random.choice(complete_weeks)\n",
    "            week_idx = datetime_index[datetime_index.isocalendar().week == random_week]\n",
    "            sampled_dates.extend([d.date() for d in week_idx])\n",
    "\n",
    "        print(f\"{season} complete weeks: {complete_weeks}\")\n",
    "\n",
    "    return sorted(list(set(sampled_dates)))\n",
    "\n",
    "\n",
    "sampled_indices = sample_seasonal_weeks(demand_df.index)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get sample subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_df[\"date\"] = demand_df.index.date\n",
    "sample_demand_df = demand_df.loc[demand_df[\"date\"].isin(sampled_indices)]\n",
    "rest_demand_df = demand_df.loc[~demand_df[\"date\"].isin(sampled_indices)]\n",
    "sample_demand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df[\"date\"] = actions_df.index.date\n",
    "\n",
    "sample_actions_df = actions_df.loc[actions_df[\"date\"].isin(sampled_indices)]\n",
    "rest_actions_df = actions_df.loc[~actions_df[\"date\"].isin(sampled_indices)]\n",
    "sample_actions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_df.index = pd.to_datetime(dispatch_df['time'])\n",
    "dispatch_df.drop(columns=['time'], inplace=True)\n",
    "dispatch_df[\"date\"] = dispatch_df.index.date\n",
    "\n",
    "sample_dispatch_df = dispatch_df.loc[dispatch_df[\"date\"].isin(sampled_indices)]\n",
    "rest_dispatch_df = dispatch_df.loc[~dispatch_df[\"date\"].isin(sampled_indices)]\n",
    "sample_dispatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample market orders as well\n",
    "market_orders_df.index = pd.to_datetime(market_orders_df[\"start_time\"])\n",
    "market_orders_df = market_orders_df.drop(columns=[\"start_time\"])\n",
    "market_orders_df[\"date\"] = market_orders_df.index.date\n",
    "\n",
    "sample_market_orders_df = market_orders_df.loc[\n",
    "    market_orders_df[\"date\"].isin(sampled_indices)\n",
    "]\n",
    "rest_market_orders_df = market_orders_df.loc[\n",
    "    ~market_orders_df[\"date\"].isin(sampled_indices)\n",
    "]\n",
    "sample_market_orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Analyse sample distribution in comparison to entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list([\"green\"] * len(rest_demand_df)) + list([\"blue\"] * len(sample_demand_df))\n",
    "\n",
    "# Scatter matrix\n",
    "fig = pd.plotting.scatter_matrix(\n",
    "    pd.concat([rest_demand_df, sample_demand_df], sort=False),\n",
    "    c=colors,\n",
    "    figsize=(7, 7),\n",
    "    range_padding=0.2,\n",
    "    hist_kwds={\"bins\": 20},  # Generic histogram configuration\n",
    "    s=30,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Customize histogram colors for each diagonal\n",
    "hist_colors = [\"green\", \"blue\"]\n",
    "for i, ax in enumerate(fig.diagonal()):\n",
    "    data_combined = pd.concat([rest_demand_df.iloc[:, i], sample_demand_df.iloc[:, i]])\n",
    "    ax.hist(\n",
    "        [rest_demand_df.iloc[:, i], sample_demand_df.iloc[:, i]],\n",
    "        bins=20,\n",
    "        color=hist_colors,\n",
    "        stacked=True,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bi-Level Optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.notebooks.MPEC.bilevel_opt import find_optimal_dispatch\n",
    "from examples.notebooks.MPEC.uc_problem import solve_uc_problem\n",
    "from examples.notebooks.MPEC.utils import calculate_profits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defintion for case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"test_case\"\n",
    "\n",
    "big_w = 100000  # weight for duality gap objective\n",
    "k_max = 2  # maximum multiplier for strategic bidding\n",
    "\n",
    "# gens\n",
    "gens_df = pp_units.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data transformation for Optimisation Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform gen_df into the format that is expected by the optimization problem\n",
    "# g_max\tmc\tu_0\tg_0\tr_up\tr_down\tk_up\tk_down\n",
    "gens_df = gens_df.reset_index()\n",
    "gens_df = gens_df.rename(columns={\"max_power\": \"g_max\", \"min_power\": \"u_0\"})\n",
    "gens_df[\"r_up\"] = gens_df[\"g_max\"]  # ramping up constraints\n",
    "gens_df[\"r_down\"] = gens_df[\"g_max\"]  # ramping down constraints\n",
    "gens_df[\"k_up\"] = 0  # start up costs\n",
    "gens_df[\"k_down\"] = 0  # shut down costs\n",
    "gens_df[\"g_0\"] = 0  # start with no power output\n",
    "\n",
    "# get average mc from dispatch_df per unit name\n",
    "mc = dispatch_df.groupby(\"unit\")[\"energy_marginal_costs\"].mean()\n",
    "\n",
    "# based on name and unit column join mc into gens_df\n",
    "gens_df = gens_df.merge(mc, left_on=\"name\", right_on=\"unit\", how=\"left\")\n",
    "gens_df = gens_df.rename(columns={\"energy_marginal_costs\": \"mc\"})\n",
    "gens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate actions of RL model into k_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on both 'unit_id' and 'time' columns\n",
    "merged_df = sample_market_orders_df.merge(\n",
    "    sample_dispatch_df.reset_index(),\n",
    "    left_on=[\"unit_id\", \"start_time\"],\n",
    "    right_on=[\"unit\", \"time\"],\n",
    "    how=\"right\",\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to translate the 2 actions per unit into one k_value? Currently:\n",
    "# get max price per unit_id and date in the dataframe\n",
    "id_k = merged_df.groupby([\"unit_id\", \"time\"])[\"price\"].idxmax()\n",
    "k_df = merged_df.loc[id_k]\n",
    "k_df = k_df[k_df[\"unit_id\"].isin(gens_df[\"name\"])]\n",
    "\n",
    "k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_mapping = dict(zip(gens_df[\"name\"], gens_df[\"mc\"]))\n",
    "k_df[\"gens_df_mc\"] = k_df[\"unit_id\"].map(mc_mapping)\n",
    "\n",
    "# transformed actions into k_values, one per generator\n",
    "k_df[\"k\"] = k_df[\"price\"] / k_df[\"gens_df_mc\"]\n",
    "\n",
    "# replace inf with 0\n",
    "k_df[\"k\"] = k_df[\"k\"].replace(np.inf, 0)\n",
    "\n",
    "k_values_df = k_df.pivot(index=\"time\", columns=\"unit_id\", values=\"k\")\n",
    "# k_values_df.reset_index(inplace=True)\n",
    "\n",
    "# sort columns to match the order of the columns in the gens_df\n",
    "k_values_df = k_values_df[gens_df[\"name\"].values]\n",
    "k_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join demand and price bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join sample demand df and sample market orders where unit id is demand_EOM based on index\n",
    "sample_demand_df[\"price\"] = sample_market_orders_df[\n",
    "    sample_market_orders_df[\"unit_id\"] == \"demand_EOM\"\n",
    "][\"price\"]\n",
    "\n",
    "#drop time column\n",
    "sample_demand_df = sample_demand_df.drop(columns=['date'])\n",
    "\n",
    "# rename index and columns\n",
    "sample_demand_df.index.name = \"datetime\"\n",
    "sample_demand_df.columns = [\"volume\", \"price\"]\n",
    "demand_df = sample_demand_df.copy()\n",
    "demand_df.index = pd.to_datetime(demand_df.index)\n",
    "demand_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MPEC(opt_gen, index, gens_df, demand_df, k_values_df):\n",
    "    print(\"We now optimize the decison for unit \", gens_df.index[opt_gen])\n",
    "    demand_df = demand_df.copy(deep=True).loc[index]\n",
    "    # reset index to start at 0\n",
    "    demand_df = demand_df.reset_index(drop=True)\n",
    "    demand_df\n",
    "\n",
    "    k_values_df = k_values_df.copy(deep=True).loc[index]\n",
    "    # rename columns to match index of gens_df\n",
    "    k_values_df.columns = gens_df.index\n",
    "    k_values_df.reset_index(inplace=True)\n",
    "    k_values_df\n",
    "    \n",
    "    gens_df = gens_df.copy(deep=True)\n",
    "    \n",
    "    main_df, supp_df, k_values = find_optimal_dispatch(\n",
    "        gens_df=gens_df,\n",
    "        k_values_df=k_values_df,\n",
    "        demand_df=demand_df,\n",
    "        k_max=k_max,\n",
    "        opt_gen=opt_gen,\n",
    "        big_w=big_w,\n",
    "        time_limit=3600,\n",
    "        print_results=True,\n",
    "        K=5,\n",
    "        big_M=10e6,\n",
    "    )\n",
    "\n",
    "    # %%\n",
    "    # calculate actual market clearing prices\n",
    "    k_values_df_2 = k_values_df.copy()\n",
    "    k_values_df_2[opt_gen] = k_values\n",
    "\n",
    "    updated_main_df_2, updated_supp_df_2 = solve_uc_problem(\n",
    "        gens_df, demand_df, k_values_df_2\n",
    "    )\n",
    "\n",
    "    # %%\n",
    "    # Calculate profits\n",
    "    profits_1 = calculate_profits(main_df=main_df, supp_df=supp_df, gens_df=gens_df)\n",
    "    profits_2 = calculate_profits(\n",
    "        main_df=updated_main_df_2, supp_df=updated_supp_df_2, gens_df=gens_df\n",
    "    )\n",
    "\n",
    "    return profits_1, profits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(\"2019-03-07 09:00\")\n",
    "end = pd.to_datetime(\"2019-03-07 13:00\")\n",
    "index = pd.date_range(start, end, freq=\"h\")\n",
    "\n",
    "opt_gen = 5\n",
    "\n",
    "profits_1, profits_2 = run_MPEC(opt_gen, index, gens_df, demand_df, k_values_df)\n",
    "\n",
    "print('Optimisation results:')\n",
    "print(f\"Estimated Profits: {profits_1[opt_gen].sum():.2f}\")\n",
    "print(f\"True profits: {profits_2[opt_gen].sum():.2f}\")\n",
    "\n",
    "cashflow=sample_dispatch_df[sample_dispatch_df['unit']==gens_df.loc[opt_gen]['name']].loc[start:end]['energy_cashflow']\n",
    "costs=sample_dispatch_df[sample_dispatch_df['unit']==gens_df.loc[opt_gen]['name']].loc[start:end]['total_costs']\n",
    "\n",
    "profit = (cashflow-costs).sum()\n",
    "\n",
    "print(\"\")\n",
    "print('Learning results:')\n",
    "print(f\"Profits: {profit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over different units and weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gens = sorted([int(actor.split(\"_\")[-1].split(\".\")[0]) for actor in actors], key=int)\n",
    "# Get unique year-month combinations to filter for different weeks\n",
    "unique_year_months = set((date.year, date.month) for date in sampled_indices)\n",
    "\n",
    "df_estimated = pd.DataFrame(columns = [f\"Unit_{opt_gen}\" for opt_gen in opt_gens])\n",
    "df_true = pd.DataFrame(columns = [f\"Unit_{opt_gen}\" for opt_gen in opt_gens])\n",
    "\n",
    "for i, (year, month) in enumerate(unique_year_months):\n",
    "    filtered_indices = [date for date in sampled_indices if date.year == year and date.month == month]\n",
    "    df_estimated_tmp = pd.DataFrame(columns = [f\"Unit_{opt_gen}\" for opt_gen in opt_gens])\n",
    "    df_true_tmp = pd.DataFrame(columns = [f\"Unit_{opt_gen}\" for opt_gen in opt_gens])\n",
    "    for opt_gen in opt_gens:\n",
    "        profits_1, profits_2 = run_MPEC(opt_gen, index, gens_df, demand_df, k_values_df)\n",
    "        df_estimated_tmp[f\"Unit_{opt_gen}\"] = (profits_1[opt_gen])\n",
    "        df_true_tmp[f\"Unit_{opt_gen}\"] = (profits_2[opt_gen])\n",
    "    df_estimated = pd.concat([df_estimated, df_estimated_tmp])\n",
    "    df_true = pd.concat([df_true, df_true_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create index from 0 to 71\n",
    "index = range(72)\n",
    "\n",
    "# Create 5 power plant columns\n",
    "columns = [f\"pp_{i}\" for i in range(1, 6)]\n",
    "\n",
    "# Create random data between 200 and 300\n",
    "df_rl = pd.DataFrame(\n",
    "    np.random.uniform(200, 300, size=(72, 5)), index=index, columns=columns\n",
    ")\n",
    "\n",
    "df_mpec = pd.DataFrame(\n",
    "    np.random.uniform(200, 300, size=(72, 5)), index=index, columns=columns\n",
    ")\n",
    "\n",
    "print(\"RL Profits:\")\n",
    "print(df_rl.head())\n",
    "print(\"\\nMPEC Profits:\")\n",
    "print(df_mpec.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_profit_comparison_plot(df_rl, df_mpec, bound=-10):\n",
    "    # Calculate percentage deviation\n",
    "    percent_deviation = ((df_rl - df_mpec) / df_mpec) * 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot(\n",
    "        [percent_deviation[col].values for col in percent_deviation.columns],\n",
    "        showmeans=False,\n",
    "        showmedians=False,\n",
    "        showextrema=False,\n",
    "    )\n",
    "\n",
    "    # Customize violin plot colors\n",
    "    for pc in parts[\"bodies\"]:\n",
    "        pc.set_facecolor(\"lightblue\")\n",
    "        pc.set_alpha(0.7)\n",
    "\n",
    "    # Add box plot inside violin plot\n",
    "    ax.boxplot(\n",
    "        [percent_deviation[col].values for col in percent_deviation.columns],\n",
    "        positions=range(1, len(percent_deviation.columns) + 1),\n",
    "        widths=0.2,\n",
    "        showfliers=True,\n",
    "        notch=True,\n",
    "    )\n",
    "\n",
    "    # Add horizontal lines and colored regions\n",
    "    ax.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.7, linewidth=1.5)\n",
    "    ax.axhline(y=bound, color=\"black\", linestyle=\"--\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    # Create background colors for different regions\n",
    "    plt.axhspan(\n",
    "        0,\n",
    "        max(percent_deviation.max()) + 10,\n",
    "        color=\"lightgreen\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit > MPEC profit\",\n",
    "    )\n",
    "    plt.axhspan(\n",
    "        bound,\n",
    "        0,\n",
    "        color=\"yellow\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit < MPEC profit but in bounds\",\n",
    "    )\n",
    "    plt.axhspan(\n",
    "        min(percent_deviation.min()) - 5,\n",
    "        bound,\n",
    "        color=\"lightcoral\",\n",
    "        alpha=0.3,\n",
    "        label=\"RL profit < MPEC profit outside bounds\",\n",
    "    )\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel(\"Power Plant Units\")\n",
    "    ax.set_ylabel(\"Deviation (%)\\n(RL - MPEC) / MPEC\")\n",
    "    ax.set_title(\"Profit Deviation Distribution (Combined Violin and Box Plot)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Set x-ticks\n",
    "    ax.set_xticks(range(1, len(percent_deviation.columns) + 1))\n",
    "    ax.set_xticklabels(percent_deviation.columns)\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Create and show the plot\n",
    "fig = create_profit_comparison_plot(df_rl, df_mpec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
