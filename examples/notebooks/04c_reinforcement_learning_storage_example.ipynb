{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02bad29e",
   "metadata": {},
   "source": [
    "# 4.3 Storage Units: Learning Temporal Bidding Strategies\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this tutorial, we extend the **reinforcement learning (RL)** framework to **storage units**, such as batteries, which face a unique decision structure: they must **buy energy (charge) when prices are low** and **sell it (discharge) when prices are high**, across multiple time steps.\n",
    "\n",
    "This creates a **temporal coupling** of actions and rewards. Unlike generators that can profit immediately from selling electricity and are only time-coupled due to technical constraints (ramping, shut down etc), storage units must learn to plan ahead, accepting short-term costs in anticipation of future gains. As such, both the **observation space** and **reward design** need to reflect this temporal structure.\n",
    "\n",
    "The tutorial is again designed to walk you through the essential components required to transform a conventional storage into an RL agent. Rather than concentrating on the underlying algorithmic infrastructure—such as training loops, buffers, or learning roles—this tutorial emphasizes **how to define a bidding strategy that interfaces with the ASSUME learning backend**. You will learn how to construct observation spaces, define action mappings, and design reward functions that guide agent behavior, but this time for a storage unit. If you have done the [04b_RL_example](.\\04b_reinforcement_learning_example.ipynb) some parts will be familiar, since this is designed as a stand-alone tutorial as well. Mainly step 1. **ASSUME & Learning Basics** and 2. **Get ASSUME Running** can be skipped.\n",
    "\n",
    "Each core concept is addressed in a dedicated chapter, accompanied by exercises that allow you to apply the material directly. These hands-on tasks culminate in a final integration chapter where you will run a complete simulation and train your first learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952b93e",
   "metadata": {},
   "source": [
    "**Tutorial Structure**\n",
    "\n",
    "The tutorial is divided into the following chapters:\n",
    "\n",
    "1. **Get ASSUME Running**\n",
    "\n",
    "   Instructions for installing ASSUME and preparing your environment, whether locally or in Google Colab.\n",
    "\n",
    "2. **ASSUME & Learning Basics**\n",
    "\n",
    "   A conceptual overview of RL within the ASSUME framework, including actor-critic architectures, centralized training, and multi-agent design principles.\n",
    "\n",
    "3. **Defining the Observation Space for Storages**\n",
    "\n",
    "   Explanation and coding tasks for constructing shared and individual observations used by agents to make decisions.\n",
    "\n",
    "4. **Action Selection**\n",
    "\n",
    "   How to convert actor network outputs into economically meaningful bid prices.\n",
    "\n",
    "5. **Reward Function Design**\n",
    "\n",
    "   Techniques for shaping agent behavior using profit- and regret-based reward signals. Includes a task to define your own reward logic.\n",
    "\n",
    "6. **Training Your First Learning Agent**\n",
    "\n",
    "   Integration of the previously implemented components into a complete simulation run, demonstrating end-to-end learning behavior in a market setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d221706",
   "metadata": {},
   "source": [
    "**Learning Outcomes**\n",
    "\n",
    "By completing this tutorial, you will be able to:\n",
    "\n",
    "* Implement RL-compatible bidding strategies for storages within the ASSUME framework.\n",
    "* Define observation inputs for learning agents, with an emphasis on the cost of stored energy.\n",
    "* Map actor outputs to valid market actions and manage exploration.\n",
    "* Construct reward functions that combine economic incentives for charging and discharging.\n",
    "* Train and evaluate a basic RL agent in a multi-agent electricity market simulation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cb4ab",
   "metadata": {},
   "source": [
    "## 1. Get ASSUME Running\n",
    "\n",
    "This chapter walks you through setting up the ASSUME framework in your environment and preparing the required input files. At the end, you will confirm that the installation was successful and ready for use.\n",
    "\n",
    "\n",
    "### 1.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608f337",
   "metadata": {},
   "source": [
    "#### In Google Colab\n",
    "\n",
    "Google Colab already includes most scientific computing libraries (e.g., `numpy`, `torch`). You only need to install the ASSUME core framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you are using Google Colab\n",
    "import importlib.util\n",
    "\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install assume-framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1014e2",
   "metadata": {},
   "source": [
    "> **Note**: After installation, **Colab may prompt you to restart the session** due to dependency changes.\n",
    "> To do so, click **\"Runtime\" → \"Restart session...\"** in the menu bar, then re-run the cells above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a2d43",
   "metadata": {},
   "source": [
    "#### On Your Local Machine\n",
    "\n",
    "To install ASSUME with all learning-related dependencies, run the following in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install 'assume-framework[learning]'\n",
    "```\n",
    "\n",
    "This will install the simulation framework and the packages required for RL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ca74e",
   "metadata": {},
   "source": [
    "### 1.2 Repository Setup\n",
    "\n",
    "To access predefined simulation scenarios, clone the ASSUME repository (Colab only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you are using Google Colab\n",
    "if IN_COLAB:\n",
    "    !git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6aad1",
   "metadata": {},
   "source": [
    "> Local users may skip this step if input files are already available in the project directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49910de8",
   "metadata": {},
   "source": [
    "### 1.3 Input Path Configuration\n",
    "\n",
    "We define the path to input files depending on whether you're in Colab or working locally. This variable will be used to load configuration and scenario files throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7368b9",
   "metadata": {},
   "source": [
    "### 1.4 Installation Check\n",
    "\n",
    "Use the following cell to ensure the installation was successful and that essential components are available. This test ensures that the simulation engine and RL strategy base class are accessible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6304eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from assume import World\n",
    "    from assume.strategies.learning_strategies import BaseLearningStrategy\n",
    "\n",
    "    print(\"✅ ASSUME framework is installed and functional.\")\n",
    "except ImportError as e:\n",
    "    print(\"❌ Failed to import essential components:\", e)\n",
    "    print(\n",
    "        \"Please review the installation instructions and ensure all dependencies are installed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c05cc",
   "metadata": {},
   "source": [
    "### 1.5 Limitations in Colab\n",
    "\n",
    "Colab does not support Docker, so dashboard visualizations included in some ASSUME workflows will not be available. However, simulation runs and RL training can still be fully executed.\n",
    "\n",
    "* In **Colab**: Training and basic plotting are supported.\n",
    "* In **Local environments with Docker**: Full access, including dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcee69",
   "metadata": {},
   "source": [
    "### 1.6 Core Imports\n",
    "\n",
    "In this section, we import the core modules that will be used throughout the tutorial. Each import is explained to clarify its role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import logging  # For logging messages during simulation and debugging\n",
    "from datetime import timedelta  # To handle market time resolutions (e.g., hourly steps)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific and data processing libraries\n",
    "import numpy as np  # Numerical operations and array handling\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import yaml  # Parsing YAML configuration files\n",
    "\n",
    "# Database and visualization libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ASSUME framework components\n",
    "from assume import World  # Core simulation container that manages markets and agents\n",
    "from assume.scenario.loader_csv import (  # Functions to load and execute scenarios\n",
    "    load_scenario_folder,\n",
    "    run_learning,\n",
    ")\n",
    "from assume.strategies.learning_strategies import (\n",
    "    BaseLearningStrategy,  # Abstract base for RL bidding strategies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d63d4b",
   "metadata": {},
   "source": [
    "These imports are used for:\n",
    "\n",
    "* Defining RL bidding strategies.\n",
    "* Managing input/output data.\n",
    "* Executing and analyzing simulations.\n",
    "\n",
    "\n",
    "At this point, you are ready to begin building your RL bidding agent. In the next chapter, we will define how agents perceive the market by constructing their **observation vectors**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff06560",
   "metadata": {},
   "source": [
    "## 2. ASSUME & Learning Basics\n",
    "\n",
    "### 2.1 The ASSUME Framework\n",
    "\n",
    "ASSUME is a simulation framework designed for researchers, utilities, and planners to model and understand market dynamics in electricity systems. It allows for agent-based modeling of market participants in a modular and configurable environment.\n",
    "\n",
    "The core structure of the framework consists of:\n",
    "\n",
    "* **Markets** (on the left of the architecture diagram): Where electricity products are traded.\n",
    "* **Market Participants / Units** (on the right): Each agent represents a physical or virtual unit bidding into the market.\n",
    "* **Orders**: The main communication channel between units and markets.\n",
    "* **Learning Agents**: Highlighted in yellow in the architecture, these are agents using RL strategies.\n",
    "\n",
    "\n",
    "> The image below illustrates the high-level architecture of ASSUME. Focus on the yellow components—these are the parts involved in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "image_path = \"assume-repo/docs/source/img/architecture.svg\"\n",
    "alt_image_path = \"../../docs/source/img/architecture.svg\"\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    display(SVG(image_path))\n",
    "elif os.path.exists(alt_image_path):\n",
    "    display(SVG(alt_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6441c01",
   "metadata": {},
   "source": [
    "### 2.2 Introduction to Learning in ASSUME\n",
    "\n",
    "The current implementation of RL in ASSUME models electricity markets as **partially observable Markov games**, allowing multiple agents to operate under individual observations and reward structures.\n",
    "\n",
    "If you are unfamiliar with RL, refer to the following links for background material:\n",
    "\n",
    "* [Reinforcement Learning Overview](https://assume.readthedocs.io/en/latest/learning.html)\n",
    "* [Reinforcement Learning Algorithms](https://assume.readthedocs.io/en/latest/learning_algorithm.html)\n",
    "\n",
    "**Central Concepts:**\n",
    "\n",
    "* **Policy**: The strategy used by an agent to select actions based on observations.\n",
    "* **Actor-Critic Architecture**: A method where the \"actor\" chooses actions and the \"critic\" evaluates them.\n",
    "* **Learning Strategy**: Defines how a unit transforms observations into bids using a trainable model.\n",
    "* **Step Functions**: The typical RL cycle of Observe → Act → Reward → Update is split across several methods in ASSUME, as described in Section 3.\n",
    "\n",
    "### 2.3 Single-Agent RL\n",
    "\n",
    "In a single-agent setup, the agent attempts to maximize its reward over time by learning from interaction with the environment. It does so by making multiple steps in the environment. In RL, each interaction step includes:\n",
    "\n",
    "1. **Observation** of the current state.\n",
    "2. **Action** selection based on policy.\n",
    "3. **Reward** from the environment.\n",
    "4. **Policy Update** to improve behavior.\n",
    "\n",
    "In ASSUME, this step cycle is modularized:\n",
    "\n",
    "| RL Step | Implemented via                                            | Description                                  |\n",
    "| ------- | ---------------------------------------------------------- | -------------------------------------------- |\n",
    "| Step 1  | `create_observation()` and `get_individual_observations()` | Constructs the observation vector.           |\n",
    "| Step 2  | `calculate_bids()` and `get_actions()`                     | Maps observations to bid prices.             |\n",
    "| Step 3  | `calculate_reward()`                                       | Computes the reward signal.                  |\n",
    "| Step 4  | Handled by the learning role                               | Updates model and manages the replay buffer. |\n",
    "\n",
    "\n",
    "**Actor-Critic Structure:**\n",
    "To increase learning stability actor-critic methods are commonly used. They divide the tasks in the following way:\n",
    "\n",
    "* **Actor**: Learns a deterministic policy for choosing actions. Uses policy gradient methods to maximize expected reward.\n",
    "* **Critic**: Learns a value function using Temporal Difference (TD) learning. Provides feedback to the actor based on action quality.\n",
    "\n",
    "### 2.4 Multi-Agent RL\n",
    "\n",
    "Real-world electricity markets involve multiple agents acting simultaneously, which introduces interdependencies and non-stationarity. As a result, multi-agent learning requires additional considerations.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "* Actions by one agent influence the environment experienced by others.\n",
    "* The state transitions and rewards become non-stationary.\n",
    "\n",
    "**Solution: Centralized Training with Decentralized Execution (CTDE)**\n",
    "\n",
    "To address these challenges, ASSUME employs the **Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (MATD3)** framework with CTDE:\n",
    "\n",
    "* **Centralized Training**: A critic with access to all agents' states and actions is used during training to stabilize learning. Note the critic is only there to update the actor network, so it is only necessary while training. \n",
    "* **Decentralized Execution**: During simulation, the actual actor of each agent relies only on its own observations and learned policy.\n",
    "\n",
    "Each agent trains two critic networks to mitigate overestimation bias, uses target noise for robustness, and relies on deterministic policy gradients for the actor update.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e18a81",
   "metadata": {},
   "source": [
    "## 3. Defining the Observation Space for Storages\n",
    "\n",
    "In this chapter, you will define what information your RL agent perceives about the environment and itself at each decision point. This is a critical component of the agent’s behavior, as the observation vector forms the basis for all future actions and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab1bb3",
   "metadata": {},
   "source": [
    "| RL Step | Implemented via                                            | Description                                  |\n",
    "| ------- | ---------------------------------------------------------- | -------------------------------------------- |\n",
    "| **Step 1**  | `create_observation()` and `get_individual_observations()` | Constructs the observation vector.           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86b3b",
   "metadata": {},
   "source": [
    "### 3.2 Observation Structure in ASSUME\n",
    "\n",
    "Observations are composed of two parts:\n",
    "\n",
    "**1. Global Observations**\n",
    "\n",
    "These are shared across all agents and constructed by the base class method `create_observation()`. They include:\n",
    "\n",
    "* **Forecasted residual load** over the foresight horizon.\n",
    "* **Forecasted market price** over the foresight horizon.\n",
    "* **Historical market price** over a specified window.\n",
    "\n",
    "> These are normalized by maximum demand and maximum bid price for stability.\n",
    "> These values are generated by a forecasting role and made available to all agents before each market cycle.\n",
    "\n",
    "For this tutorial **you do not need to modify this part.** However, if you want to equip new units types with learning or expand the simulation by new concepts, additional global information might be needed. \n",
    "\n",
    "**2. Individual Observations**\n",
    "\n",
    "These are unit-specific and must be implemented by you. The purpose is to provide the agent with private, operational information that may help improve bidding decisions. Each agent appends this information to the end of the shared observation vector.\n",
    "\n",
    "This is done via the method `get_individual_observations(unit, start)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8380f",
   "metadata": {},
   "source": [
    "### 3.3 Exercise 1: Choose a Suitable Foresight for Storage Agents\n",
    "\n",
    "To enable learning for storage units, we define a custom strategy class that extends `BaseLearningStrategy`. This class specifies key dimensions such as the size of the observation and action spaces. One crucial parameter you need to define is the **foresight**—how many future time steps the agent considers when making decisions.\n",
    "\n",
    "Unlike dispatchable power plants, storage units face **temporally coupled decisions**: they must charge at one point in time and discharge at another, often hours later. This delay between cost and profit means that storage agents require a **longer foresight** than units that act on short-term signals.\n",
    "\n",
    "Define the foresight of your agent by updating the `self.foresight` attribute inside the constructor:\n",
    "\n",
    "\n",
    "> **Hint:** Typical power plants operate well with a foresight of 12 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageRLStrategy(BaseLearningStrategy):\n",
    "    \"\"\"\n",
    "    A simple reinforcement learning bidding strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        obs_dim = kwargs.pop(\"obs_dim\", 74)  # Forecasts + history + individual values\n",
    "        act_dim = kwargs.pop(\"act_dim\", 1)  # One action: bid price\n",
    "        unique_obs_dim = kwargs.pop(\"unique_obs_dim\", 2)  # Number of individual obs\n",
    "\n",
    "        super().__init__(\n",
    "            obs_dim=obs_dim,\n",
    "            act_dim=act_dim,\n",
    "            unique_obs_dim=unique_obs_dim,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Forecast horizon (in timesteps) used for market and residual load forecasts\n",
    "        self.foresight = None  # Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947e174",
   "metadata": {},
   "source": [
    "**Solution Excercise 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Solution Exercise 1\n",
    "class StorageRLStrategy(BaseLearningStrategy):\n",
    "    \"\"\"\n",
    "    A simple reinforcement learning bidding strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        obs_dim = kwargs.pop(\"obs_dim\", 74)  # Forecasts + history + individual values\n",
    "        act_dim = kwargs.pop(\"act_dim\", 1)  # One action: bid price\n",
    "        unique_obs_dim = kwargs.pop(\"unique_obs_dim\", 2)  # Number of individual obs\n",
    "\n",
    "        super().__init__(\n",
    "            obs_dim=obs_dim,\n",
    "            act_dim=act_dim,\n",
    "            unique_obs_dim=unique_obs_dim,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Forecast horizon (in timesteps) used for market and residual load forecasts\n",
    "        self.foresight = 24  # Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bcb3c",
   "metadata": {},
   "source": [
    "For storages, we recommend a foresight of **24 hours**, which aligns with standard industry practice and allows for daily charge/discharge cycles. Note that longer foresight increases the size of the observation space, as **each forecasted time series (e.g., price, residual load)** is extended accordingly. If you're designing seasonal storage agents (e.g., hydrogen or pumped hydro), you may consider even longer horizons—but beware the combinatorial explosion of the input space.\n",
    "\n",
    "With this foresight range the global observations are defined in the function `create_observation`of the base class. We focus here on the individual observations in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc511c",
   "metadata": {},
   "source": [
    "### 3.4 Exercise 2: Define Individual Observations\n",
    "\n",
    "The storage agent receives the standard set of **global observations**, including price and load forecasts over a 24-hour foresight window. However, two **individual features** are added to reflect its internal state:\n",
    "\n",
    "- `State of Charge (SoC)`: How full the battery is, scaled between 0 and 1.\n",
    "- `Energy Cost`\n",
    "\n",
    "These individual features are returned by the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def get_individual_observations(self, unit, start, end):\n",
    "        \"\"\"\n",
    "        Define custom unit-specific observations for the RL agent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        unit : SupportsMinMax\n",
    "            The unit representing the power plant.\n",
    "        start : datetime.datetime\n",
    "            Start time of the market product.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Normalized 1D array of individual observations.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the current soc and energy cost value\n",
    "        soc_scaled = unit.outputs[\"soc\"].at[start] / unit.max_soc\n",
    "        energy_cost_scaled = unit.outputs[\"energy_cost\"].at[start] / self.max_bid_price\n",
    "\n",
    "        individual_observations = np.array([soc_scaled, energy_cost_scaled])\n",
    "\n",
    "        return individual_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536464e8",
   "metadata": {},
   "source": [
    "This method must return a NumPy array of length `unique_obs_dim`. \n",
    "\n",
    "When a storage unit charges or discharges, the **cost of its stored energy** must be updated to reflect the new energy mix. This cost is defined as the **volume-weighted average procurement cost** and plays a key role when deciding whether selling is profitable.\n",
    "\n",
    "\n",
    "The update depends on the **type of action**:\n",
    "\n",
    "- When **charging**, the cost of stored energy is updated.\n",
    "- If **discharging** or **inactive**, the cost remains unchanged.\n",
    "\n",
    "\n",
    "The energy cost update depends on:\n",
    "\n",
    "- `accepted_volume`: How much energy was bought (negative) or sold (positive).\n",
    "- `accepted_price` and `marginal_cost`: Cost components when buying.\n",
    "- `duration_hours`: How long the bid covers.\n",
    "- `current_soc` and `next_soc`: Storage level before and after the bid.\n",
    "\n",
    "\n",
    "Implement the following logic in the `update_energy_cost` funtion:\n",
    "\n",
    "```python\n",
    "if next_soc < 1:\n",
    "    cost_next = 0\n",
    "elif accepted_volume < 0:  # Charging\n",
    "    cost_next = (old_cost * current_soc - (price + marginal_cost) * volume * duration) / next_soc\n",
    "elif accepted_volume > 0:  # Discharging\n",
    "    cost_next = (old_cost * (current_soc - volume * duration)) / next_soc\n",
    "else:  # No accepted action\n",
    "    cost_next = old_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def update_cost_stored_energy(\n",
    "        self,\n",
    "        unit,\n",
    "        start,\n",
    "        next_time,\n",
    "        current_soc,\n",
    "        next_soc,\n",
    "        accepted_volume,\n",
    "        accepted_price,\n",
    "        marginal_cost,\n",
    "        duration_hours,\n",
    "        max_bid_price,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the cost of stored energy based on accepted market actions.\n",
    "        \"\"\"\n",
    "        # TODO: Replace this with your own logic\n",
    "        if next_soc < 1:\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time] = (\n",
    "                None  # Your implementation here\n",
    "            )\n",
    "\n",
    "        elif accepted_volume < 0:  # Charging\n",
    "            cost = None  # Your implementation here\n",
    "\n",
    "        else:  # No action\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time] = (\n",
    "                None  # Your implementation here\n",
    "            )\n",
    "\n",
    "        unit.outputs[\"cost_stored_energy\"].at[next_time] = np.clip(\n",
    "            cost, -max_bid_price, max_bid_price\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebab7a",
   "metadata": {},
   "source": [
    "> **Why do we clip the energy cost?**  \n",
    "> In rare cases, especially during initial learning or under extreme prices, the calculated energy cost can get very high.  \n",
    "> Clipping the value ensures numerical stability for the observation space and keeps the input to the neural network within a realistic and learnable range (between `-max_bid_price` and `+max_bid_price`), which is also the bound we chose for scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac196c1b",
   "metadata": {},
   "source": [
    "**Solution Excercise 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Solution Excercise 2\n",
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def update_cost_stored_energy(\n",
    "        self,\n",
    "        unit,\n",
    "        start,\n",
    "        next_time,\n",
    "        current_soc,\n",
    "        next_soc,\n",
    "        accepted_volume,\n",
    "        accepted_price,\n",
    "        marginal_cost,\n",
    "        duration_hours,\n",
    "        max_bid_price,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the cost of stored energy based on accepted market actions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate and clip the energy cost for the start time\n",
    "        # cost_stored_energy = average volume weighted procurement costs of the currently stored energy\n",
    "        if next_soc < 1:\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time] = 0\n",
    "        elif accepted_volume < 0:\n",
    "            # increase costs of current SoC by price for buying energy\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time] = (\n",
    "                unit.outputs[\"cost_stored_energy\"].at[start] * current_soc\n",
    "                - (accepted_price + marginal_cost) * accepted_volume * duration_hours\n",
    "            ) / next_soc\n",
    "        else:\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time] = unit.outputs[\n",
    "                \"cost_stored_energy\"\n",
    "            ].at[start]\n",
    "\n",
    "        unit.outputs[\"cost_stored_energy\"].at[next_time] = np.clip(\n",
    "            unit.outputs[\"cost_stored_energy\"].at[next_time],\n",
    "            -max_bid_price,\n",
    "            max_bid_price,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b440f6d",
   "metadata": {},
   "source": [
    "> **Note for advanced users:**  \n",
    "> The environment for storage units is **not fully Markovian**. Future rewards depend on past actions — particularly the prices at which energy was charged.  \n",
    "> To mitigate this partial observability, we **augment the observation space** with the **average cost of stored energy**. This acts as a memory proxy, helping the agent assess whether selling at a given price is profitable.  \n",
    "> This approach is a form of *state augmentation*, commonly used in reinforcement learning to approximate Markovian behaviour in **partially observable environments (POMDPs)**.\n",
    "\n",
    "\n",
    "### 3.5 Summary\n",
    "\n",
    "* Observations in ASSUME combine **shared global forecasts** and **custom individual data**.\n",
    "* The base class handles forecasted residual load and price, as well as historical price signals.\n",
    "* For storage units, individual observations include the **state of charge** and the **cost of stored energy**, which reflects past purchase prices and is updated over time.\n",
    "* You implemented the logic for updating this cost after market actions—this is crucial for enabling the agent to assess profitability when selling energy.\n",
    "* These observations directly affect agent behaviour and learning convergence—thoughtful design matters.\n",
    "\n",
    "In the next chapter, you will define **how the agent selects actions** based on its observations, and how **exploration** is introduced during initial training to populate the learning buffer.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6afe4",
   "metadata": {},
   "source": [
    "## 4. Action Selection and Exploration\n",
    "\n",
    "Once an observation is formed, the next step is for the agent to decide how to act. In this context, the **action** determines the **bid** consisting of a price-volume pair that the agent submits to the electricity market.\n",
    "\n",
    "\n",
    "This chapter focuses on how actions are derived from the agent’s policy and how exploration is handled—especially during the early training phase when experience is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bf5a3",
   "metadata": {},
   "source": [
    "### 4.1 Action Selection in RL\n",
    "\n",
    "In RL, the **policy** defines the agent’s behavior: it maps observations to actions. In the actor-critic architecture used by ASSUME, this policy is represented by the **actor neural network**.\n",
    "\n",
    "However, to enable **exploration**, especially in the early stages of training, agents must not always follow the policy exactly. They need to try out a variety of actions—even suboptimal ones—to collect diverse experiences and learn effectively.\n",
    "\n",
    "This is done by **adding noise** to the actions suggested by the policy network.\n",
    "\n",
    "> Note: The implementation of noise we present here is specific to the used algorithm **MADDPG**. Other Algorithms such as the PPO will use a different mechanism for exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec3eae",
   "metadata": {},
   "source": [
    "### 4.2 Understanding `get_actions()`\n",
    "\n",
    "The method `get_actions(next_observation)` in `BaseLearningStrategy` defines how actions are computed in different modes of operation.\n",
    "\n",
    "Here is a simplified overview of the logic:\n",
    "\n",
    "```python\n",
    "def get_actions(self, next_observation):\n",
    "    if self.learning_mode and not self.evaluation_mode:\n",
    "        if self.collect_initial_experience_mode:\n",
    "            # Initial exploration: use pure noise as action\n",
    "            noise = self.action_noise.noise(...)\n",
    "            curr_action = noise\n",
    "        else:\n",
    "            # Regular exploration: add noise to policy output\n",
    "            curr_action = self.actor(next_observation).detach()\n",
    "            noise = self.action_noise.noise(...)\n",
    "            curr_action += noise\n",
    "    else:\n",
    "        # Evaluation or deterministic policy use\n",
    "        curr_action = self.actor(next_observation).detach()\n",
    "        noise = zeros_like(curr_action)\n",
    "\n",
    "    return curr_action, noise\n",
    "```\n",
    "\n",
    "**Modes of Operation:**\n",
    "\n",
    "* `learning_mode`: Indicates that the agent is being trained.\n",
    "* `evaluation_mode`: Disables noise; used to assess performance of a learned policy.\n",
    "* `collect_initial_experience_mode`: Special sub-phase during early episodes of training where we rely heavily on **randomized exploration** to populate the replay buffer with diverse samples.\n",
    "\n",
    "The output of `get_actions` is then transformed into the actual bids by the `calculate_bids`function which we will look at in the next chapter. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b671f",
   "metadata": {},
   "source": [
    "## 5. From Observation to Action to Bids\n",
    "\n",
    "In the previous chapters, we explored how an agent perceives its environment through observations and how it selects actions using its policy, optionally enriched with exploration noise. In this short chapter, we show how these two steps come together inside the `calculate_bids()` method.\n",
    "\n",
    "There is **no task** in this chapter we just walk you through the function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9005c",
   "metadata": {},
   "source": [
    "### 5.1 The Role of `calculate_bids()`\n",
    "\n",
    "The `calculate_bids()` method defines how a market participant formulates its bid at each market interval. It brings together two crucial operations:\n",
    "\n",
    "1. **Generating Observations**:\n",
    "   Calls `create_observation()` to construct the full input vector (including both global and individual components).\n",
    "\n",
    "2. **Choosing an Action**:\n",
    "   Passes the observation to `get_actions()`, which invokes the actor network (and optionally adds noise) to return an action vector.\n",
    "\n",
    "This forms the agent’s internal decision pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11607d",
   "metadata": {},
   "source": [
    "### 5.2 Action Normalization and Scaling\n",
    "\n",
    "The actor network produces a **single continuous output** between \\(-1\\) and \\(+1\\), which we interpret in two ways:\n",
    "\n",
    "- The **sign** of the output determines the **bid direction**:\n",
    "  - `action < 0`: The agent wants to **buy** energy (i.e. charge the battery).\n",
    "  - `action ≥ 0`: The agent wants to **sell** energy (i.e. discharge the battery).\n",
    "  \n",
    "- The **magnitude** (absolute value) of the output determines the **bid price**, scaled by the agent’s `max_bid_price`:\n",
    "  \n",
    "    $ \\text{bid\\_price} = |\\text{action}| \\times \\text{max\\_bid\\_price} $\n",
    "\n",
    "For example:  \n",
    "If the network outputs `-0.7`, this translates to a **buy bid** with price `0.7 × max_bid_price`.  \n",
    "If it outputs `+0.4`, this becomes a **sell bid** at `0.4 × max_bid_price`.\n",
    "\n",
    "\n",
    "By modifying `max_bid_price` in the learning config, you directly influence the economic \"aggressiveness\" of the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a9025",
   "metadata": {},
   "source": [
    "### 5.3 Bid Structure\n",
    "\n",
    "Each bid submitted to the market follows a defined structure, encapsulated as a dictionary:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"start_time\": start,\n",
    "    \"end_time\": end,\n",
    "    \"price\": bid_price,\n",
    "    \"volume\": max_power,\n",
    "    \"node\": unit.node,\n",
    "}\n",
    "```\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "* **price**: Determined from the scaled output of the policy.\n",
    "* **volume**: Set to the full technical charging of discharging power of the sotrage considering the current State of Charge.\n",
    "* **node**: Locational identifier (used for zonal/nodal pricing and congestion modeling).\n",
    "\n",
    "Note that `max_power` is **positive**, as this strategy models a generator offering energy. For a **consumer or demand bid**, the volume would be **negative** to reflect load withdrawal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f2f1e",
   "metadata": {},
   "source": [
    "### 5.4 Why We Store Everything in `unit.outputs`\n",
    "\n",
    "The outputs of the bidding process are stored in two places:\n",
    "\n",
    "* `unit.outputs[\"rl_observations\"]` and `[\"rl_actions\"]`:\n",
    "  Stored as lists to be written into the replay buffer for learning.\n",
    "\n",
    "* `unit.outputs[\"actions\"]` and `[\"exploration_noise\"]`:\n",
    "  Stored as `pandas.Series` for compatibility with the unit’s internal logging and database structure.\n",
    "\n",
    "This dual storage ensures that both the simulation engine and the learning backend have access to the relevant data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838b5df",
   "metadata": {},
   "source": [
    "### 5.5 Controlling Action Dimensions\n",
    "\n",
    "By changing the `act_dim` in the strategy constructor, you can control the number of outputs returned by the actor network:\n",
    "\n",
    "```python\n",
    "act_dim = kwargs.pop(\"act_dim\", 1)\n",
    "```\n",
    "\n",
    "This allows for richer bidding logic. For instance:\n",
    "\n",
    "* 1 action: Bid price for total capacity.\n",
    "* 2 actions: Bid prices and bid volume.\n",
    "* 3 actions: Add directionality or reserve offers.\n",
    "\n",
    "However, it is important to note that **RL performance deteriorates with high-dimensional action spaces**, especially in continuous domains.\n",
    "\n",
    "If you decide to increase `act_dim`, ensure that your `calculate_bids()` method is updated accordingly to interpret and transform all action elements correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6cd5",
   "metadata": {},
   "source": [
    "### 5.6 Full Code Implementation\n",
    "\n",
    "Here is the complete `calculate_bids()` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def calculate_bids(\n",
    "        self,\n",
    "        unit,\n",
    "        market_config,\n",
    "        product_tuples,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates market bids based on the unit's current state and observations.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        unit : SupportsMinMaxCharge\n",
    "            The storage unit with information on charging/discharging capacity.\n",
    "        market_config : MarketConfig\n",
    "            Configuration of the energy market.\n",
    "        product_tuples : list[Product]\n",
    "            List of market products to bid on, each containing start and end times.\n",
    "        **kwargs : Additional keyword arguments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Orderbook\n",
    "            Structured bids including price, volume, and bid direction.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Observations are used to calculate bid actions, which are then scaled and processed\n",
    "        into bids for submission in the market.\n",
    "        \"\"\"\n",
    "\n",
    "        start = product_tuples[0][0]\n",
    "        end_all = product_tuples[-1][1]\n",
    "        # =============================================================================\n",
    "        # 1. Get the observations, which are the basis of the action decision\n",
    "        # =============================================================================\n",
    "        next_observation = self.create_observation(\n",
    "            unit=unit,\n",
    "            market_id=market_config.market_id,\n",
    "            start=start,\n",
    "            end=end_all,\n",
    "        )\n",
    "\n",
    "        # =============================================================================\n",
    "        # 2. Get the actions, based on the observations\n",
    "        # =============================================================================\n",
    "        actions, noise = self.get_actions(next_observation)\n",
    "\n",
    "        # =============================================================================\n",
    "        # 3. Transform actions into bids\n",
    "        # =============================================================================\n",
    "        # the absolute value of the action determines the bid price\n",
    "        bid_price = abs(actions[0]) * self.max_bid_price\n",
    "        # the sign of the action determines the bid direction\n",
    "        if actions[0] < 0:\n",
    "            bid_direction = \"buy\"\n",
    "        elif actions[0] >= 0:\n",
    "            bid_direction = \"sell\"\n",
    "\n",
    "        # these are function from the technical representation of storages\n",
    "        _, max_discharge = unit.calculate_min_max_discharge(start, end_all)\n",
    "        _, max_charge = unit.calculate_min_max_charge(start, end_all)\n",
    "\n",
    "        bid_quantity_supply = max_discharge[0]\n",
    "        bid_quantity_demand = max_charge[0]\n",
    "\n",
    "        bids = []\n",
    "\n",
    "        if bid_direction == \"sell\":\n",
    "            bids.append(\n",
    "                {\n",
    "                    \"start_time\": start,\n",
    "                    \"end_time\": end_all,\n",
    "                    \"only_hours\": None,\n",
    "                    \"price\": bid_price,\n",
    "                    \"volume\": bid_quantity_supply,\n",
    "                    \"node\": unit.node,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif bid_direction == \"buy\":\n",
    "            bids.append(\n",
    "                {\n",
    "                    \"start_time\": start,\n",
    "                    \"end_time\": end_all,\n",
    "                    \"only_hours\": None,\n",
    "                    \"price\": bid_price,\n",
    "                    \"volume\": bid_quantity_demand,  # negative value for demand\n",
    "                    \"node\": unit.node,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        unit.outputs[\"rl_observations\"].append(next_observation)\n",
    "        unit.outputs[\"rl_actions\"].append(actions)\n",
    "\n",
    "        # store results in unit outputs as series to be written to the database by the unit operator\n",
    "        unit.outputs[\"actions\"].at[start] = actions\n",
    "        unit.outputs[\"exploration_noise\"].at[start] = noise\n",
    "\n",
    "        return bids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9def27c",
   "metadata": {},
   "source": [
    "In the next chapter, we will define how to compute the **reward** associated with each bid outcome, which completes the agent’s learning cycle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462de3c",
   "metadata": {},
   "source": [
    "## 6. Reward Function Design\n",
    "\n",
    "The reward function is the **central learning signal** in any RL environment. It defines the objective the agent is trying to maximize and serves as the only feedback mechanism from the environment to the agent.\n",
    "\n",
    "Designing the reward function is a delicate balance between:\n",
    "\n",
    "* Capturing **realistic economic goals** (e.g., profit maximization),\n",
    "* Enabling **learning stability and convergence**, and\n",
    "* Leaving room for the agent to **discover unexpected, valid strategies**.\n",
    "\n",
    "It’s tempting to hard-code your preferred behavior into the reward function. However, this often leads to agents that are overly adapted to a specific scenario and perform poorly in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2ad62",
   "metadata": {},
   "source": [
    "### 6.1 When Is the Reward Computed?\n",
    "\n",
    "In ASSUME, the reward is computed **after the market clears**, in the `calculate_reward()` method. At this point, the agent receives information about:\n",
    "\n",
    "* Which portion of its bid was accepted,\n",
    "* at what price,\n",
    "* and what operational costs it incurred, if any.\n",
    "\n",
    "This allows us to calculate realized **profit**, which is the most direct economic reward signal.\n",
    "\n",
    "### 6.2 RL Theory for Temporally Distributed Reward\n",
    "\n",
    "In RL, the agent’s goal is to **maximise the expected sum of discounted future rewards**:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]\n",
    "$$\n",
    "\n",
    "The discount factor $\\gamma$ (typically between 0.95 and 0.99) controls how much future rewards are valued compared to immediate ones.\n",
    "\n",
    "For **storage units**, this matters a lot: charging leads to **short-term losses**, while discharging later yields profits. The agent must therefore learn to **delay gratification** and value **future gains**.\n",
    "\n",
    "> Choosing a **high discount factor** (e.g. $\\gamma = 0.999$) is essential so the agent connects today’s cost with tomorrow’s profit.\n",
    "\n",
    "If $\\gamma$ is too low, the agent may avoid charging altogether, failing to discover arbitrage opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a279e",
   "metadata": {},
   "source": [
    "### 6.3 Exercise 3: Implement Profit-Based Reward for Storage Units\n",
    "\n",
    "Your next task is to implement a **profit-based reward** for a storage unit. \n",
    "\n",
    "Unlike generators, storage units may buy energy (at a cost) and sell it later (for a profit). So, their reward must reflect **charging costs and discharging revenue**, depending on the action taken.\n",
    "\n",
    "Use the following formula:\n",
    "\n",
    "$$\n",
    "\\pi_{i,t} = P^\\text{conf}_{i,t} \\cdot (M_t - mc_{i,t}) \\cdot dt\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $P^\\text{conf}$: Confirmed volume (positive for discharge / sell, negative for charge / buy),\n",
    "* $M_t$: Accepted market price,\n",
    "* $mc_{i,t}$: Marginal cost of charging or discharging, if any,\n",
    "* $dt$: Duration in hours.\n",
    "\n",
    "\n",
    "\n",
    "You can access the required quantities using:\n",
    "\n",
    "```python\n",
    "accepted_volume = order[\"accepted_volume\"]\n",
    "accepted_price = order[\"accepted_price\"]\n",
    "duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "marginal_cost = unit.calculate_marginal_cost(\n",
    "    start, unit.outputs[marketconfig.product_type].at[start]\n",
    ")\n",
    "marginal_cost += unit.get_starting_costs(int(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009dc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def calculate_reward(self, unit, marketconfig, orderbook):\n",
    "        \"\"\"\n",
    "        Reward function: implement profit-based reward for storage agents.\n",
    "        \"\"\"\n",
    "\n",
    "        order = orderbook[0]\n",
    "        start = order[\"start_time\"]\n",
    "        end = order[\"end_time\"]\n",
    "        end_excl = end - unit.index.freq\n",
    "        next_time = start + unit.index.freq\n",
    "        duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "        # === Access values ===\n",
    "        accepted_price = None  # YOUR CODE\n",
    "        accepted_volume = None  # YOUR CODE\n",
    "\n",
    "        marginal_cost = unit.calculate_marginal_cost(\n",
    "            start, unit.outputs[marketconfig.product_type].at[start]\n",
    "        )\n",
    "        marginal_cost += unit.get_starting_costs(int(duration))\n",
    "\n",
    "        # === Compute profit ===\n",
    "        order_profit = None  # YOUR CODE\n",
    "        order_cost = None  # YOUR CODE\n",
    "        profit = None  # YOUR CODE\n",
    "\n",
    "        # === Scale reward ===\n",
    "        scaling = 1 / (self.max_bid_price * unit.max_power_discharge)\n",
    "        reward = scaling * profit\n",
    "\n",
    "        # === Update stored energy cost ===\n",
    "        self.update_cost_stored_energy(\n",
    "            unit=unit,\n",
    "            start=start,\n",
    "            next_time=next_time,\n",
    "            current_soc=unit.outputs[\"soc\"].at[start],\n",
    "            next_soc=unit.outputs[\"soc\"].at[next_time],\n",
    "            accepted_volume=accepted_volume,\n",
    "            accepted_price=accepted_price,\n",
    "            marginal_cost=marginal_cost,\n",
    "            duration_hours=duration,\n",
    "            max_bid_price=self.max_bid_price,\n",
    "        )\n",
    "\n",
    "        # === Store results ===\n",
    "        unit.outputs[\"profit\"].loc[start:end_excl] += profit\n",
    "        unit.outputs[\"reward\"].loc[start:end_excl] = reward\n",
    "        unit.outputs[\"total_costs\"].loc[start:end_excl] = order_cost\n",
    "        unit.outputs[\"rl_rewards\"].append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569f490",
   "metadata": {},
   "source": [
    "### 6.4 Why Just the Profit as Feedback?\n",
    "\n",
    "In contrast to other agent types, storage units use **only realised profit** as a reward signal — without including opportunity cost or regret terms.\n",
    "\n",
    "For storage, defining missed opportunities is difficult:\n",
    "\n",
    "- Profit depends on **temporal strategies** (charge now, discharge later).\n",
    "- Simple heuristics often provide **misleading incentives**.\n",
    "- Unlike generators, there’s no clear rule like “produce if price > cost” that works somewhat reliably.\n",
    "\n",
    "In theory, we could compute opportunity costs by comparing the agent’s profit to a **hindsight optimal schedule**.\n",
    "\n",
    "But this is:\n",
    "- **Computationally infeasible** at every step,\n",
    "- **Non-scalable** in multi-agent settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f8b90",
   "metadata": {},
   "source": [
    "### 6.6 Reward Scaling and Learning Stability\n",
    "\n",
    "Scaling the reward to a **narrow and consistent range** is crucial for stable RL. This is particularly important in continuous-action settings like bidding, where one overly large reward spike can skew the policy updates significantly.\n",
    "\n",
    "**1. Why scale?**\n",
    "\n",
    "* Stabilizes gradients during actor-critic training.\n",
    "* Makes different time steps comparable in magnitude.\n",
    "* Prevents the agent from overfitting to rare but extreme events.\n",
    "\n",
    "**2. What can go wrong?**\n",
    "\n",
    "If your scaling factor is too small:\n",
    "\n",
    "* Rewards become indistinguishable from noise.\n",
    "\n",
    "If your scaling factor is too large:\n",
    "\n",
    "* A single high-reward event (e.g., bidding into a rare price spike) can **dominate learning**,\n",
    "  making the agent try to reproduce that event rather than learn a general policy.\n",
    "\n",
    "> **Tip**: Use conservative scaling based on maximum realistic bid × capacity:\n",
    "\n",
    "```python\n",
    "scaling = 1 / (self.max_bid_price * unit.max_power_discharge)\n",
    "```\n",
    "\n",
    "**3. Recommended Practice**\n",
    "\n",
    "Before committing to training:\n",
    "\n",
    "* **Plot the distribution of rewards** across time steps for a few sample runs.\n",
    "* Check for outliers, saturation, or skewness.\n",
    "* If needed, adjust `scaling` or cap outliers in reward postprocessing.\n",
    "\n",
    "This diagnostic step can save hours of failed training runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664884a",
   "metadata": {},
   "source": [
    "**Solution Exercise 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d52545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Solution Exercise 3: Implement Reward Function\n",
    "\n",
    "# we define the class again and inherit from the initial class just to add the additional method to the original class\n",
    "# this is a workaround to have different methods of the class in different cells\n",
    "# which is good for the purpose of this tutorial\n",
    "# however, you should have all functions in a single class when using this example in .py files\n",
    "\n",
    "\n",
    "class StorageRLStrategy(StorageRLStrategy):\n",
    "    def calculate_reward(self, unit, marketconfig, orderbook):\n",
    "        \"\"\"\n",
    "        Reward function: implement profit-based reward for storage agents.\n",
    "        \"\"\"\n",
    "\n",
    "        order = orderbook[0]\n",
    "        start = order[\"start_time\"]\n",
    "        end = order[\"end_time\"]\n",
    "        end_excl = end - unit.index.freq\n",
    "        next_time = start + unit.index.freq\n",
    "        duration = (end - start) / timedelta(hours=1)\n",
    "\n",
    "        # === Access values ===\n",
    "        accepted_price = order[\"accepted_price\"]\n",
    "        accepted_volume = order[\"accepted_volume\"]\n",
    "\n",
    "        marginal_cost = unit.calculate_marginal_cost(\n",
    "            start, unit.outputs[marketconfig.product_type].at[start]\n",
    "        )\n",
    "        marginal_cost += unit.get_starting_costs(int(duration))\n",
    "\n",
    "        # === Compute profit ===\n",
    "        order_profit = accepted_price * accepted_volume * duration\n",
    "        order_cost = abs(marginal_cost * accepted_volume * duration)\n",
    "        profit = order_profit - order_cost\n",
    "\n",
    "        # === Scale reward ===\n",
    "        scaling = 1 / (self.max_bid_price * unit.max_power_discharge)\n",
    "        reward = scaling * profit\n",
    "\n",
    "        # === Update stored energy cost ===\n",
    "        self.update_cost_stored_energy(\n",
    "            unit=unit,\n",
    "            start=start,\n",
    "            next_time=next_time,\n",
    "            current_soc=unit.outputs[\"soc\"].at[start],\n",
    "            next_soc=unit.outputs[\"soc\"].at[next_time],\n",
    "            accepted_volume=accepted_volume,\n",
    "            accepted_price=accepted_price,\n",
    "            marginal_cost=marginal_cost,\n",
    "            duration_hours=duration,\n",
    "            max_bid_price=self.max_bid_price,\n",
    "        )\n",
    "\n",
    "        # === Store results ===\n",
    "        unit.outputs[\"profit\"].loc[start:end_excl] += profit\n",
    "        unit.outputs[\"reward\"].loc[start:end_excl] = reward\n",
    "        unit.outputs[\"total_costs\"].loc[start:end_excl] = order_cost\n",
    "        unit.outputs[\"rl_rewards\"].append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04b334",
   "metadata": {},
   "source": [
    "### 6.7 Summary\n",
    "\n",
    "* The reward function is the core signal guiding agent learning—design it carefully.\n",
    "* For storage units, use **realised profit** as the primary reward.\n",
    "* Avoid **opportunity cost terms** unless you can compute them reliably—storage bidding is temporally coupled and hard to benchmark heuristically.\n",
    "* Always **normalize** your reward to maintain training stability.\n",
    "* Analyze your reward distribution empirically before training large-scale agents.\n",
    "\n",
    "In the next chapter, we will bring together all the components—observation, action, and reward—and simulate a full training run using your custom learning strategy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f28147",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluating Your First Learning Agent\n",
    "\n",
    "You have now implemented all essential components of a learning bidding strategy in ASSUME:\n",
    "\n",
    "* Observations\n",
    "* Actions and exploration\n",
    "* Reward function\n",
    "\n",
    "In this chapter, you will connect your strategy to a simulation scenario, configure the learning algorithm, and evaluate the agent’s training progress.\n",
    "\n",
    "\n",
    "### 7.1 Load and Inspect the Learning Configuration\n",
    "\n",
    "Each simulation scenario in ASSUME has an associated YAML configuration file. This file contains the **learning configuration**, which determines how the RL algorithm is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"base\"\n",
    "\n",
    "# Read the YAML file\n",
    "with open(f\"{inputs_path}/example_02e/config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Print the learning config\n",
    "print(f\"Learning config for scenario '{scenario}':\")\n",
    "display(config[scenario][\"learning_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85338",
   "metadata": {},
   "source": [
    "**Explanation of Learning Configuration Parameters**\n",
    "\n",
    "| Parameter                                     | Description                                                                           |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
    "| **continue\\_learning**                        | If `True`, resumes training from saved policy checkpoints.                            |\n",
    "| **trained\\_policies\\_save\\_path**             | File path where trained policies will be saved.                                       |\n",
    "| **trained\\_policies\\_load\\_path**             | Path to pre-trained policies to load.                                                 |\n",
    "| **max\\_bid\\_price**                           | Used to scale action outputs to economic bid prices.                                  |\n",
    "| **algorithm**                                 | Learning algorithm used (e.g., `matd3` for multi-agent TD3).                          |\n",
    "| **learning\\_rate**                            | Step size for policy and critic updates.                                              |\n",
    "| **training\\_episodes**                        | Number of simulation episodes (repetitions of the time horizon) used for training.    |\n",
    "| **episodes\\_collecting\\_initial\\_experience** | Number of episodes during which agents collect experience using guided exploration.   |\n",
    "| **train\\_freq**                               | Time between training updates, e.g., `'100h'` means update every 100 simulated hours. |\n",
    "| **gradient\\_steps**                           | Number of gradient descent steps per update.                                          |\n",
    "| **batch\\_size**                               | Size of experience batch used for training.                                           |\n",
    "| **gamma**                                     | Discount factor for future rewards ($0 < \\gamma \\leq 1$).                             |\n",
    "| **device**                                    | `\"cpu\"` or `\"cuda\"` depending on hardware.                                            |\n",
    "| **action\\_noise\\_schedule**                   | How the action noise evolves over time (`linear`, `constant`, etc.).                  |\n",
    "| **noise\\_sigma**                              | Standard deviation of exploration noise.                                              |\n",
    "| **noise\\_scale**                              | Global multiplier for noise.                                                          |\n",
    "| **noise\\_dt**                                 | Discretization interval for noise time series.                                        |\n",
    "| **validation\\_episodes\\_interval**            | How often (in episodes) to evaluate the current policy without exploration.           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c181a",
   "metadata": {},
   "source": [
    "### 7.2 Run the Simulation and Train the two Agents\n",
    "\n",
    "The simulation environment and learning strategy are connected and executed as follows:\n",
    "\n",
    "> **Hint**: In Google Colab, long-running training sessions may occasionally **crash or disconnect** if the output console is flooded — for example, by verbose progress bars or print statements.  \n",
    "> To prevent this, you can suppress output during training using the following approach.\n",
    "\n",
    "1. **Import the required tools:**\n",
    "\n",
    "    ```python\n",
    "    from contextlib import redirect_stdout, redirect_stderr\n",
    "    import os\n",
    "    ```\n",
    "\n",
    "2. **Wrap the training phase with output redirection.**  \n",
    "   Insert the following lines **just before Step 4: _Run the training phase_**:\n",
    "\n",
    "    ```python\n",
    "    # Suppress output for the entire training process\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stdout(devnull), redirect_stderr(devnull):\n",
    "            # Your training function call goes here\n",
    "            train_agents(...)\n",
    "    ```\n",
    "\n",
    "> ✅ This redirects all `stdout` and `stderr` to `/dev/null`, preventing Colab from being overwhelmed by output and improving session stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4251a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "np.random.seed(42)  # Set a random seed for reproducibility\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///../local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_02e\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # 1. Create simulation world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # 2. Register your learning strategy\n",
    "    world.bidding_strategies[\"storage_learning\"] = StorageRLStrategy\n",
    "\n",
    "    # 3. Load scenario and case\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # 4. Run the training phase\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(world)\n",
    "\n",
    "    # 5. Execute final evaluation run (no exploration)\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcb328",
   "metadata": {},
   "source": [
    "This script will:\n",
    "\n",
    "* Train the agent using your defined strategy.\n",
    "* Periodically evaluate the agent using a noise-free policy.\n",
    "* Save training data into the database for post-analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc3c74",
   "metadata": {},
   "source": [
    "### **7.3 Analyze Learning Performance**\n",
    "\n",
    "Once training is complete, we can evaluate the learning progress of your RL agent using data from the simulation database. ASSUME stores detailed training metrics in the `rl_params` table, which includes rewards for each time step, grouped by episode, unit, and whether the agent was in evaluation mode.\n",
    "\n",
    "In this case, we are interested in the performance of a specific storage: **`Storage 1`**, within the simulation **`example_02e_base`**.\n",
    "\n",
    "We’ll extract the recorded rewards for this unit, group them by episode, and plot the average reward over time for both training and evaluation phases.\n",
    "\n",
    "> Instead of accessing the training evaluation via the database we also feature a tensorboard integration, which can be accessed in the console `tensorboard --logdir tensorboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the simulation database\n",
    "engine = create_engine(\"sqlite:///../local_db/assume_db.db\")\n",
    "\n",
    "# Query rewards for specific simulation and unit\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    datetime,\n",
    "    unit,\n",
    "    reward,\n",
    "    simulation,\n",
    "    evaluation_mode,\n",
    "    episode\n",
    "FROM rl_params\n",
    "WHERE simulation = 'example_02e_base'\n",
    "ORDER BY datetime\n",
    "\"\"\"\n",
    "\n",
    "# Load query results\n",
    "rewards_df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Rename column for consistency\n",
    "rewards_df.rename(columns={\"evaluation_mode\": \"evaluation\"}, inplace=True)\n",
    "\n",
    "# --- Separate plots for training and evaluation ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# Plot training rewards (evaluation == 0)\n",
    "train_df = rewards_df[rewards_df[\"evaluation\"] == 0]\n",
    "train_grouped = train_df.groupby(\"episode\")[\"reward\"].mean()\n",
    "\n",
    "axes[0].plot(train_grouped.index, train_grouped.values, \"s-\", color=\"tab:blue\")\n",
    "axes[0].set_title(\"Training Reward per Episode\")\n",
    "axes[0].set_ylabel(\"Average Reward\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot evaluation rewards (evaluation == 1)\n",
    "eval_df = rewards_df[rewards_df[\"evaluation\"] == 1]\n",
    "eval_grouped = eval_df.groupby(\"episode\")[\"reward\"].mean()\n",
    "\n",
    "axes[1].plot(eval_grouped.index, eval_grouped.values, \"s-\", color=\"tab:green\")\n",
    "axes[1].set_title(\"Evaluation Reward per Episode\")\n",
    "axes[1].set_xlabel(\"Episode\")\n",
    "axes[1].set_ylabel(\"Average Reward\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ab867",
   "metadata": {},
   "source": [
    "**What This Shows**\n",
    "\n",
    "* **Training curve**: Captures learning progress with exploration noise.\n",
    "* **Evaluation curve**: Tracks the performance of the evaluation/validation run without noise, which is performed every `validation_episodes_interval` steps, as defined in the `learning_config`. \n",
    "\n",
    "### 7.4 Exercise 4: Get a feeling for the Reward \n",
    "\n",
    "**Why is the reward so small?**\n",
    "To better understand why the observed reward is so small, revisit the definition of the reward function. Pay particular attention to the aggregation function used in the plotting cell above. What exactly is being plotted?\n",
    "\n",
    "Hint: Think about whether the plotted value reflects individual time steps or aggregated performance, and how inactive hours might affect the result.\n",
    "\n",
    "\n",
    "**What happens to the individual units?**\n",
    "The plot shows the **average reward** across both learning storage units. What would you expect the reward curve of a single storage unit to look like? Will it follow a similar trend, or could there be differences? Consider what impact one unit being more active or successful than the other might have on the average.\n",
    "\n",
    "Take a moment to think through this before checking the explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e33d58",
   "metadata": {},
   "source": [
    "**Solution Excerise 4**\n",
    "The reward is based on the **scaled profit** of each storage unit:\n",
    "\n",
    "```python\n",
    "scaling = 1 / (self.max_bid_price * unit.max_power_discharge)\n",
    "reward = scaling * profit\n",
    "```\n",
    "\n",
    "This scaling ensures that the reward stays within a small, consistent numerical range. Additionally, what is plotted is the average reward per time step across an entire episode — typically defined as one month, i.e., all hours within that month.\n",
    "\n",
    "In many of these hours, the agent does not act (i.e., the profit is zero), which pulls the average down. If you change the aggregation function from `mean` to `sum`, you'll notice that the overall reward becomes larger, and you’ll see how the zero-reward hours affect the average.\n",
    "\n",
    "By understanding this, you can better interpret agent performance and evaluate whether learning is actually occurring — even if the average reward appears small.\n",
    "\n",
    "**Let's look at individual units**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the simulation database\n",
    "engine = create_engine(\"sqlite:///../local_db/assume_db.db\")\n",
    "\n",
    "# Query rewards for specific simulation and unit\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    datetime,\n",
    "    unit,\n",
    "    reward,\n",
    "    simulation,\n",
    "    evaluation_mode,\n",
    "    episode\n",
    "FROM rl_params\n",
    "WHERE simulation = 'example_02e_base'\n",
    "ORDER BY datetime\n",
    "\"\"\"\n",
    "\n",
    "# Load query results\n",
    "rewards_df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Rename column for consistency\n",
    "rewards_df.rename(columns={\"evaluation_mode\": \"evaluation\"}, inplace=True)\n",
    "\n",
    "# Get unique units\n",
    "units = rewards_df[\"unit\"].unique()\n",
    "\n",
    "# --- Separate plots for training and evaluation ---\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=False)\n",
    "\n",
    "# Plot training rewards (evaluation == 0) - one line per unit\n",
    "train_df = rewards_df[rewards_df[\"evaluation\"] == 0]\n",
    "for unit in units:\n",
    "    unit_data = train_df[train_df[\"unit\"] == unit]\n",
    "    train_grouped = unit_data.groupby(\"episode\")[\"reward\"].mean()\n",
    "    axes[0].plot(\n",
    "        train_grouped.index, train_grouped.values, \"o-\", label=f\"{unit}\", alpha=0.7\n",
    "    )\n",
    "\n",
    "axes[0].set_title(\"Training Reward per Episode (by Unit)\")\n",
    "axes[0].set_ylabel(\"Average Reward\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot evaluation rewards (evaluation == 1) - one line per unit\n",
    "eval_df = rewards_df[rewards_df[\"evaluation\"] == 1]\n",
    "for unit in units:\n",
    "    unit_data = eval_df[eval_df[\"unit\"] == unit]\n",
    "    eval_grouped = unit_data.groupby(\"episode\")[\"reward\"].mean()\n",
    "    axes[1].plot(\n",
    "        eval_grouped.index, eval_grouped.values, \"s-\", label=f\"{unit}\", alpha=0.7\n",
    "    )\n",
    "\n",
    "axes[1].set_title(\"Evaluation Reward per Episode (by Unit)\")\n",
    "axes[1].set_xlabel(\"Episode\")\n",
    "axes[1].set_ylabel(\"Average Reward\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c289b",
   "metadata": {},
   "source": [
    "As seen in the plots above, one storage unit (Storage 2) learns a profitable strategy, while the other (Storage 1) shows little to no improvement over time. This divergence is due to differences in initialisation and learning dynamics.\n",
    "\n",
    "One key factor is that Storage 2 quickly learns that its actions can **influence the market price** — it becomes a price setter in certain hours. This feedback between its bidding strategy and the resulting price allows it to understand the reward signal more clearly and improve faster. In contrast, Storage 1 rarely becomes price-setting and thus finds it harder to link its actions to outcomes. Without this feedback loop, learning is significantly slower or even stagnant. Here we can see a slight increase in the evaluation rewrad ar the end, that indicates storage 1 might recover.\n",
    "\n",
    "To mitigate this, we often use a **warm start** strategy in practice: agents are initialised with policies that have already learned basic behavioural patterns, such as first charge and then discharge or how to bid in a stationary environment. This helps agents reach the price-setting regime more quickly and facilitates meaningful learning, especially in multi-agent setups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf740050",
   "metadata": {},
   "source": [
    "### 7.4 Summary\n",
    "\n",
    "* You have now run your **first complete training loop** in ASSUME.\n",
    "* The learning configuration defines all key training parameters—review them carefully.\n",
    "* After training, rewards from `rl_params` allow you to inspect and validate agent behavior.\n",
    "* The separation of **training** and **evaluation** rewards is key to understanding generalization.\n",
    "* **Agent performance may vary significantly** due to initialisation and the ability to influence market prices.\n",
    "* To support learning, agents are often **warm-started** with strategies that already capture basic bidding logic.\n",
    "\n",
    "In the next chapter, you may proceed to analyze simulation outcomes in greater detail (e.g., market prices, total costs, capacity dispatch), or compare different agent configurations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510ddc5",
   "metadata": {},
   "source": [
    "## 8. Analyzing Bidding Behavior\n",
    "\n",
    "Now that your agent has completed training, we shift our focus to a critical and more insightful question:\n",
    "\n",
    "> **What did the agent actually learn?**\n",
    "\n",
    "This chapter analyzes the **actual bids submitted by the agent** and evaluates whether the agent developed a **reasonable bidding behavior**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2e0c2",
   "metadata": {},
   "source": [
    "### 8.2. Extract and Plot the Agent's Bids\n",
    "\n",
    "We will extract the bids submitted by `Storage 2` from the `market_orders` table and plot them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "engine = create_engine(\"sqlite:///../local_db/assume_db.db\")\n",
    "\n",
    "# Query bids from pp_6 in simulation example_02a_base and market EOM\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    start_time AS time,\n",
    "    accepted_price,\n",
    "    unit_id,\n",
    "    simulation,\n",
    "    accepted_volume\n",
    "FROM market_orders\n",
    "WHERE simulation = 'example_02e_base'\n",
    "  AND unit_id = 'Storage 2'\n",
    "  AND market_id = 'EOM'\n",
    "ORDER BY start_time\n",
    "\"\"\"\n",
    "\n",
    "bids_df = pd.read_sql(sql, engine)\n",
    "bids_df[\"time\"] = pd.to_datetime(bids_df[\"time\"])\n",
    "\n",
    "buy_bids = bids_df[bids_df[\"accepted_volume\"] < 0].copy()\n",
    "sell_bids = bids_df[bids_df[\"accepted_volume\"] > 0].copy()\n",
    "\n",
    "# plot sell and buy bids\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(\n",
    "    sell_bids[\"time\"],\n",
    "    sell_bids[\"accepted_price\"],\n",
    "    \"o\",\n",
    "    label=\"Sell Bids\",\n",
    "    color=\"tab:orange\",\n",
    ")\n",
    "plt.plot(\n",
    "    buy_bids[\"time\"],\n",
    "    buy_bids[\"accepted_price\"],\n",
    "    \"o\",\n",
    "    label=\"Buy Bids\",\n",
    "    color=\"tab:blue\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"Storage Bidding Behavior: Charging vs Discharging\")\n",
    "plt.xlabel(\"Time\")\n",
    "# just plot one day\n",
    "plt.xlim(bids_df[\"time\"].max() - pd.Timedelta(days=1), bids_df[\"time\"].max())\n",
    "plt.ylabel(\"Accepted Market Price (€/MWh)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fdcd2",
   "metadata": {},
   "source": [
    "### 8.3. What Does This Show?\n",
    "\n",
    "The plot above shows the **accepted market prices** for storage bids over time, separated into **buy bids** (charging) and **sell bids** (discharging). Each point represents a successful bid that was accepted by the market at a specific time.\n",
    "\n",
    "- **Blue dots** indicate charging actions (buy bids), where the storage unit purchases electricity at lower prices.\n",
    "- **Orange dots** represent discharging actions (sell bids), where electricity is sold back to the market at higher prices.\n",
    "\n",
    "From the visual distribution, we can observe a typical storage behaviour:\n",
    "- Charging occurs during **low-price hours**, typically at night or early morning.\n",
    "- Discharging is concentrated in **higher-price hours**, typically in the afternoon or evening.\n",
    "\n",
    "This indicates that the agent has learned a basic arbitrage strategy — **charge low, discharge high** — which aligns with economic incentives. The spread between buy and sell prices is essential for generating profit and, consequently, positive rewards.\n",
    "\n",
    "\n",
    "While this gives insight into the **agent’s strategy and confidence**, it’s important to compare this behavior against a **benchmark** to judge its effectiveness.\n",
    "\n",
    "- For a **single storage unit**, a common benchmark is a **perfect-foresight optimization**, which computes the best possible charging and discharging schedule based on known future prices. This shows how close the RL agent gets to the theoretical optimum.\n",
    "\n",
    "- However, if you have **multiple storage agents**, their actions can **influence market prices**. In this case, the environment becomes **strategic and interdependent**, and a simple optimization no longer reflects the true benchmark.\n",
    "\n",
    "- For such settings, the appropriate comparison is a **Mathematical Program with Equilibrium Constraints (MPEC)** or other game-theoretic models. These account for the fact that **agents anticipate their own market impact**, and provide a consistent equilibrium benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d45dc",
   "metadata": {},
   "source": [
    "### 8.4. Summary\n",
    "\n",
    "- You analyzed the learned bidding strategy of the storage unit. \n",
    "- Investigating how RL design choices impact market modeling results is crutial.\n",
    "- Don't forget to benchmark your results to other models, e.g. optimization or game-theoretic models.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b67630",
   "metadata": {},
   "source": [
    "## 9. Summary and Outlook\n",
    "\n",
    "### 9.1 What You Built\n",
    "\n",
    "Over the course of this tutorial, you developed a complete **RL bidding strategy** for a storage unit in the **ASSUME** framework. You constructed and trained a learning agent that can:\n",
    "\n",
    "* Observe market signals and internal state-of-charge dynamics.\n",
    "* Decide when to charge or discharge based on learned economic strategies.\n",
    "* Receive profit-based reward signals and adapt behavior over time.\n",
    "* React to changing market conditions with temporally coordinated actions.\n",
    "\n",
    "\n",
    "### 9.2 What You Learned\n",
    "\n",
    "Throughout the tutorial, you explored the **end-to-end learning pipeline** for storage units in a realistic market setting:\n",
    "\n",
    "* How to construct **observation spaces** that reflect temporal coupling and internal energy cost.\n",
    "* How to define **bid direction and price** using a compact action space.\n",
    "* Why **realized profit** is used as the reward signal, and why opportunity cost is avoided for storage.\n",
    "* How to scale rewards and update the **cost of stored energy** after market interactions.\n",
    "* How to train and evaluate a storage agent’s behavior in **multi-agent DRL simulations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc118c",
   "metadata": {},
   "source": [
    "### 9.3 What You Can Try Next\n",
    "\n",
    "Your implementation is modular and extensible. Here are several directions you can explore on your own:\n",
    "\n",
    "1. **Adjust Learning Parameters**\n",
    "\n",
    "Experiment with:\n",
    "\n",
    "* `learning_rate`, `gamma`, `noise_sigma`, `episodes_collecting_initial_experience`\n",
    "* `validation_episodes_interval`, `train_freq`, or `gradient_steps`\n",
    "\n",
    "Observe how these changes affect convergence, stability, and bidding behavior.\n",
    "\n",
    "2. **Try Different Scenarios**\n",
    "\n",
    "* Adjust scenario inputs of example `02e`:\n",
    "\n",
    "  * Remove the second storage unit from the `storage_units.csv` file.\n",
    "  * Add many learning agents, simulating a highly competitive environment.\n",
    "\n",
    "* Compare bidding behavior and reward dynamics across settings.\n",
    "\n",
    "3. **Dive into other tutorials**\n",
    "\n",
    "* If you are interested in the general algorithm behind the MADDPG and how it is integrated into ASSUME look into [04a_RL_algorithm_example](.\\04a_reinforcement_learning_algorithm_example.ipynb) \n",
    "* In the small example we could see what the a good bidding behavior of the agent might be and, hence, can judge learning easily, but what if we model many agents in new simulations? We provide explainable RL mechanisms in another tutorial for you to dive into [09_example_Sim_and_xRL](.\\09_example_Sim_and_xRL.ipynb) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
