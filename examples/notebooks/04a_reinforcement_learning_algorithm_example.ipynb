{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dfcafd",
   "metadata": {
    "id": "4JeBorbE6FYr"
   },
   "source": [
    "# 4.1 Reinforcement Learning Algorithm Tutorial\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "This comprehensive tutorial introduces users to the **MATD3 implementation** in ASSUME and demonstrates how we leverage **reinforcement learning (RL)** within the framework. \n",
    "\n",
    "The primary goal is to ensure participants understand the essential steps required to equip ASSUME with a RL algorithm. This tutorial delves one level deeper than the basic RL application example in the user guide, providing detailed insights into the algorithm's implementation.\n",
    "\n",
    "**Note:** This tutorial focuses on understanding the implementation rather than hands-on coding tasks. The knowledge presented here is **optional** if you plan to use the pre-configured algorithms in ASSUME.\n",
    "\n",
    "\n",
    "\n",
    "**Tutorial Structure**\n",
    "\n",
    "1. **From Single Simulation to Learning Episodes**\n",
    "   - Understanding the transition from one simulation year to multiple learning episodes\n",
    "   - How RL changes the simulation flow\n",
    "\n",
    "2. **The Role of Learning Roles**\n",
    "   - Understanding the purpose and functionality of learning roles\n",
    "   - Key responsibilities and interactions\n",
    "\n",
    "3. **MATD3 Algorithm Deep Dive**\n",
    "   - Detailed examination of the Multi-Agent Twin Delayed Deep Deterministic Policy Gradient algorithm\n",
    "   - Implementation specifics and key characteristics\n",
    "\n",
    "\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- **RL Background:** For a comprehensive refresher on reinforcement learning concepts, visit our [documentation](https://assume.readthedocs.io/en/latest/)\n",
    "- **Implementation Details:** The algorithm explained here is designed as a plug-and-play solution within the ASSUME framework\n",
    "\n",
    "\n",
    "\n",
    "> **Important:** This tutorial provides an overview and explanation of reinforcement learning implementation for those interested in modifying the underlying learning algorithm. It does not include practical coding exercises.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b738b3",
   "metadata": {},
   "source": [
    "## 0. Get ASSUME Running\n",
    "\n",
    "### 0.1 Installation Process\n",
    "\n",
    "We'll start by installing ASSUME in this environment. For Google Colab, we install the ASSUME core package via pip. \n",
    "\n",
    "For comprehensive installation instructions, please refer to our [official documentation](https://assume.readthedocs.io/en/latest/installation.html).\n",
    "\n",
    "> **Note:** If we're working in Colab, creating a virtual environment (venv) is not necessary as the environment is already isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2b8fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0DaRwFA7VgW",
    "outputId": "5655adad-5b7a-4fe3-9067-6b502a06136b"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check whether notebook is run in google colab\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install assume-framework[learning]\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1d3b",
   "metadata": {
    "id": "IIw_QIE3pY34"
   },
   "source": [
    " **Dashboard Limitations in Colab**\n",
    "> **⚠️ Important:** In Google Colab, we cannot access Docker-dependent functionalities, which means:\n",
    "> - Pre-defined dashboards are not available\n",
    "> - For full dashboard access, please install Docker and ASSUME on your local machine\n",
    "\n",
    "### 0.2 Repository Setup\n",
    "\n",
    "To access predefined simulation scenarios, clone the ASSUME repository (Colab only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e77f71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hB0uDisSsg",
    "outputId": "1241881f-e090-4f26-9b02-560adfcb3a3e"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf23b1c",
   "metadata": {
    "id": "Fg7DyNjLuvSb"
   },
   "source": [
    "### First Simulation Run\n",
    "\n",
    "Now you can run your **first simulation** in ASSUME! The following code will:\n",
    "\n",
    "1. Navigate to the ASSUME directory\n",
    "2. Start the simulation using example `example_01b`\n",
    "3. Use a local SQLite database\n",
    "\n",
    "#### Command Line Equivalent\n",
    "When running locally, you can execute this command in your shell:\n",
    "```bash\n",
    "assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de097384",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eVM60Qx8SC0",
    "outputId": "20434515-6e65-4d34-d44d-8c4529a46ece"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !cd assume-repo && assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc1c12",
   "metadata": {},
   "source": [
    "### 0.3 Input Path Configuration\n",
    "\n",
    "We define the path to input files depending on whether you're in Colab or working locally. This variable will be used to load configuration and scenario files throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9899ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63deda6",
   "metadata": {
    "id": "bj2C4ElILNNv"
   },
   "source": [
    "## 1. From Single Simulation to Learning Episodes\n",
    "\n",
    "In traditional simulations **without reinforcement learning**, we execute the time horizon only **once**. However, RL introduces a fundamental change:\n",
    "\n",
    "- **Learning Requirement:** RL agents must learn strategies through repeated interactions\n",
    "- **Experience Gathering:** Agents need to encounter the same situations multiple times\n",
    "\n",
    "\n",
    "To enable this learning process, we implement a `run_learning()` function that:\n",
    "\n",
    "1. Repeats the entire simulation horizon multiple times\n",
    "2. Activates when learning mode is enabled in the configuration\n",
    "3. Manages the learning process across episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9569b2d",
   "metadata": {
    "id": "zMyZhaNM7NRP"
   },
   "source": [
    "### Required Imports\n",
    "\n",
    "First, let's import all necessary libraries and modules for our RL implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade14744",
   "metadata": {
    "id": "xUsbeZdPJ_2Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.reinforcement_learning.algorithms.base_algorithm import RLAlgorithm\n",
    "from assume.reinforcement_learning.algorithms.matd3 import TD3\n",
    "from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "from assume.reinforcement_learning.learning_role import Learning\n",
    "from assume.reinforcement_learning.learning_utils import polyak_update\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_scenario_folder,\n",
    "    setup_world,\n",
    ")\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2dae5",
   "metadata": {},
   "source": [
    "This flowchart provides an overview of the key stages involved in the run_learning function, which trains Deep Reinforcement Learning (DRL) agents within a simulated market environment. The process is divided into five main steps:\n",
    "\n",
    "1. **Initialization of the Learning Process**: The function begins by setting up the environment, initializing policies, and configuring necessary settings such as logging and buffer allocation. It ensures that no existing policies are overwritten without confirmation.\n",
    "\n",
    "2. **Training Loop**: This is the outer loop where multiple training episodes are executed. For each episode, the world simulation is completely re-initialized and reset after execution, meaning the simulation environment is essentially killed after each episode. Crucially, all necessary information that must persist across episodes—such as collected experience stored in the buffer—is maintained in the inter-episodic data. This data is key to ensuring the continuity of the learning process as it allows the DRL agents to build knowledge over time.\n",
    "\n",
    "3. **Evaluation Loop**: Nested within the training loop, the evaluation loop periodically assesses the performance of the learned policies. Based on average rewards, the best-performing policies are saved, and the function determines if further training is necessary.\n",
    "\n",
    "4. **Terminate Learning**: At the end of the training phase, the function saves the final version of the learned policies, ensuring that the results are stored for future use.\n",
    "\n",
    "5. **Final Evaluation Run**: A final evaluation run is conducted using the best policies from the training phase, providing a benchmark for overall performance.\n",
    "\n",
    "The flowchart visually represents the interaction between the training and evaluation loops, highlighting the progression through these key stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"../../docs/source/img/Assume_run_learning_loop.png\", width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94517a3e",
   "metadata": {
    "id": "UXYSesx4Ifp5"
   },
   "outputs": [],
   "source": [
    "def run_learning(\n",
    "    world: World,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"assume.scenario.loader_csv\").setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    world.learning_role.rl_algorithm.initialize_policy()\n",
    "\n",
    "    # check if we already stored policies for this simulation\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir() and not world.learning_config[\"continue_learning\"]:\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learned strategies? (y/N) \"\n",
    "        )\n",
    "        if accept.lower().startswith(\"y\"):\n",
    "            # remove existing policies\n",
    "            if os.path.exists(save_path):\n",
    "                shutil.rmtree(save_path, ignore_errors=True)\n",
    "\n",
    "        else:\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\n",
    "                \"Simulation aborted by user not to overwrite existing learned strategies. You can use 'simulation_id' parameter in the config to start a new simulation.\"\n",
    "            )\n",
    "\n",
    "    # also remove tensorboard logs\n",
    "    tensorboard_path = f\"tensorboard/{world.scenario_data['simulation_id']}\"\n",
    "    if os.path.exists(tensorboard_path):\n",
    "        shutil.rmtree(tensorboard_path, ignore_errors=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "    }\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initialized learning role the needed information across episodes\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                episode=episode,\n",
    "            )\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                evaluation_mode=True,\n",
    "                episode=episode,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward(episode=eval_episode)\n",
    "\n",
    "            if len(total_rewards) == 0:\n",
    "                raise AssumeException(\"No rewards were collected during evaluation run\")\n",
    "\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # save the policies after each episode in case the simulation is stopped or crashes\n",
    "        if (\n",
    "            episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "    # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # Set 'trained_policies_load_path' to None in order to load the most recent policies,\n",
    "    # especially if previous strategies were loaded from an external source.\n",
    "    # This is useful when continuing from a previous learning session.\n",
    "    world.scenario_data[\"config\"][\"learning_config\"][\"trained_policies_load_path\"] = (\n",
    "        f\"{world.learning_role.trained_policies_save_path}/avg_reward_eval_policies\"\n",
    "    )\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        terminate_learning=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894edbd9",
   "metadata": {
    "id": "8UM1QPZrIdqK"
   },
   "source": [
    "## 2. What Role has a Learning Role\n",
    "\n",
    "The LearningRole class in learning_role.py is a central component of the reinforcement learning framework. It manages configurations, device settings, early stopping of the learning process, and initializes various RL strategies, the algorithm and buffers. This class ensures that the RL agent can be trained or evaluated effectively, leveraging the available hardware and adhering to the specified configurations. The parameters of the learning process are also described in the read-the-docs under learning_algorithms.\n",
    "\n",
    "### 2.1 Learning Data Management\n",
    "\n",
    "One key feature of the LearningRole class is its ability to load and manage the inter episodic data. This involves storing experiences and the training progress and retrieving this data to train the RL agent. By efficiently handling episodic data, the LearningRole class enables the agent to learn from past experiences and improve its performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    \"\"\"\n",
    "    This class manages the learning process of reinforcement learning agents, including initializing key components such as\n",
    "    neural networks, replay buffer, and learning hyperparameters. It handles both training and evaluation modes based on\n",
    "    the provided learning configuration.\n",
    "\n",
    "    Args:\n",
    "        simulation_start (datetime.datetime): The start of the simulation.\n",
    "        simulation_end (datetime.datetime): The end of the simulation.\n",
    "        learning_config (LearningConfig): The configuration for the learning process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def load_inter_episodic_data(self, inter_episodic_data):\n",
    "        \"\"\"\n",
    "        Load the inter-episodic data from the dict stored across simulation runs.\n",
    "\n",
    "        Args:\n",
    "            inter_episodic_data (dict): The inter-episodic data to be loaded.\n",
    "\n",
    "        \"\"\"\n",
    "        self.episodes_done = inter_episodic_data[\"episodes_done\"]\n",
    "        self.eval_episodes_done = inter_episodic_data[\"eval_episodes_done\"]\n",
    "        self.max_eval = inter_episodic_data[\"max_eval\"]\n",
    "        self.rl_eval = inter_episodic_data[\"all_eval\"]\n",
    "        self.avg_rewards = inter_episodic_data[\"avg_all_eval\"]\n",
    "        self.buffer = inter_episodic_data[\"buffer\"]\n",
    "\n",
    "        # if enough initial experience was collected according to specifications in learning config\n",
    "        # turn off initial exploration and go into full learning mode\n",
    "        if self.episodes_done >= self.episodes_collecting_initial_experience:\n",
    "            self.turn_off_initial_exploration()\n",
    "\n",
    "        self.set_noise_scale(inter_episodic_data[\"noise_scale\"])\n",
    "\n",
    "        self.initialize_policy(inter_episodic_data[\"actors_and_critics\"])\n",
    "\n",
    "    def get_inter_episodic_data(self):\n",
    "        \"\"\"\n",
    "        Dump the inter-episodic data to a dict for storing across simulation runs.\n",
    "\n",
    "        Returns:\n",
    "            dict: The inter-episodic data to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"episodes_done\": self.episodes_done,\n",
    "            \"eval_episodes_done\": self.eval_episodes_done,\n",
    "            \"max_eval\": self.max_eval,\n",
    "            \"all_eval\": self.rl_eval,\n",
    "            \"avg_all_eval\": self.avg_rewards,\n",
    "            \"buffer\": self.buffer,\n",
    "            \"actors_and_critics\": self.rl_algorithm.extract_policy(),\n",
    "            \"noise_scale\": self.get_noise_scale(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de9ea4",
   "metadata": {},
   "source": [
    "The metrics in `inter_episodic_data` are stored for the following reasons:\n",
    "\n",
    "- `episodes_done` and `eval_episodes_done`: **Monitoring Progress**  \n",
    "  Keeping track of the number of episodes completed.\n",
    "\n",
    "- `max_eval`, `all_eval`, `avg_all_eval`: **Evaluating Performance**  \n",
    "  Storing evaluation scores and average rewards to assess the agent's performance across episodes.\n",
    "\n",
    "- `buffer`: **Experience Replay**  \n",
    "  Using a replay buffer to learn from past experiences and improve data efficiency.\n",
    "\n",
    "- `noise_scale`: **Policy Exploration**  \n",
    "  The noise is used to include exploration in the policy. It may be decreased across episode numbers, and we store the current noise value to continue the decrease across future episodes.\n",
    "\n",
    "- `actors_and_critics`: **Policy Initialization**  \n",
    "  Initializing the policy with actors and critics (`self.initialize_policy()`) ensures that the agent starts with the pre-defined strategy from the previous episode and can improve upon it through learning.\n",
    "\n",
    "\n",
    "### 2.2 Learning Algorithm\n",
    "\n",
    "If learning is used, then the learning role initializes a learning algorithm which is the heart of the learning progress. Currently, only the MATD3 is implemented, but we are working on different PPO implementations as well. If you would like to add an algorithm it would be integrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632844c2",
   "metadata": {
    "id": "0ww-L9fABnw3"
   },
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    def create_learning_algorithm(self, algorithm: RLAlgorithm):\n",
    "        \"\"\"\n",
    "        Create and initialize the reinforcement learning algorithm.\n",
    "\n",
    "        This method creates and initializes the reinforcement learning algorithm based on the specified algorithm name. The algorithm\n",
    "        is associated with the learning role and configured with relevant hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            algorithm (RLAlgorithm): The name of the reinforcement learning algorithm.\n",
    "        \"\"\"\n",
    "        if algorithm == \"matd3\":\n",
    "            self.rl_algorithm = TD3(\n",
    "                learning_role=self,\n",
    "                learning_rate=self.learning_rate,\n",
    "                episodes_collecting_initial_experience=self.episodes_collecting_initial_experience,\n",
    "                gradient_steps=self.gradient_steps,\n",
    "                batch_size=self.batch_size,\n",
    "                gamma=self.gamma,\n",
    "                actor_architecture=self.actor_architecture,\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f\"Learning algorithm {algorithm} not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f6366",
   "metadata": {},
   "source": [
    "## 3. Learning Algorithm Flow in ASSUME\n",
    "\n",
    "The following graph illustrates the structure and flow of the learning algorithm within the reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"../../docs/source/img/TD3_algorithm.jpeg\", width=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f544e80",
   "metadata": {},
   "source": [
    "Within the algorithm, we distinguish three different steps that are translated into ASSUME in the following way:\n",
    "\n",
    "1. **Initialization**: This is the first step where all necessary components such as the actors, critics, and buffer are set up.\n",
    "\n",
    "2. **Experience Collection**: The second step, represented in the flowchart above within the loop, involves the collection of experience. This includes choosing an action, observing a reward, and storing the transition tuple in the buffer.\n",
    "\n",
    "3. **Policy Update**: The third step is the actual policy update, which is also performed within the loop, allowing the agent to improve its performance over time.\n",
    "\n",
    "\n",
    "### 3.1 Initialization\n",
    "\n",
    "The initialization of the actors, critics, and the buffer is handled via the `learning_role` and the `inter_episodic_data`, as described earlier. The `create_learning_algorithm` function triggers their initialization in `initialize_policy`. At the beginning of the training process, they are initialized with new random settings. In subsequent episodes, they are initialized with pre-learned data, ensuring that previous learning is retained and built upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def initialize_policy(self, actors_and_critics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Create actor and critic networks for reinforcement learning.\n",
    "\n",
    "        If `actors_and_critics` is None, this method creates new actor and critic networks.\n",
    "        If `actors_and_critics` is provided, it assigns existing networks to the respective attributes.\n",
    "\n",
    "        Args:\n",
    "            actors_and_critics (dict): The actor and critic networks to be assigned.\n",
    "\n",
    "        \"\"\"\n",
    "        if actors_and_critics is None:\n",
    "            self.create_actors()\n",
    "            self.create_critics()\n",
    "\n",
    "        else:\n",
    "            for u_id, strategy in self.learning_role.rl_strats.items():\n",
    "                strategy.actor = actors_and_critics[\"actors\"][u_id]\n",
    "                strategy.actor_target = actors_and_critics[\"actor_targets\"][u_id]\n",
    "\n",
    "                strategy.critics = actors_and_critics[\"critics\"][u_id]\n",
    "                strategy.target_critics = actors_and_critics[\"target_critics\"][u_id]\n",
    "\n",
    "            self.obs_dim = actors_and_critics[\"obs_dim\"]\n",
    "            self.act_dim = actors_and_critics[\"act_dim\"]\n",
    "            self.unique_obs_dim = actors_and_critics[\"unique_obs_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b64d11",
   "metadata": {},
   "source": [
    "Please also note that we make a distinction in the handling of the critics and target critics compared to the actors and target actors. You can observe this in the `initialize_policy` function. For the critics, they are assigned to the `learning_role` as there are centralized critics used for all the different actors. In contrast, the actors are assigned to specific unit strategies. Each learning unit, such as a power plant, has one learning strategy and therefore an individual actor, while the critics remain centralized.\n",
    "\n",
    "This distinction leads to the case where, even if learning is not active, we still need the actors to perform the entire simulation using pre-trained policies. This is essential, for example, when running simulations with previously learned policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88b3d9",
   "metadata": {},
   "source": [
    "### 3.2 Experience Collection\n",
    "\n",
    "Within the loop, the selection of an action with exploration noise, as well as the observation of a new reward and state, and the storing of this tuple in the buffer, are all handled within the bidding strategy. \n",
    "\n",
    "This specific process is covered in more detail in another tutorial. For more details, refer to [tutorial 04b](04b_reinforcement_learning_example.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df537a91",
   "metadata": {},
   "source": [
    "### 3.3 Policy Update \n",
    "\n",
    "The core of the algorithm, which comprises all other steps is embodied by the `assume.reinforcement_learning.algorithms.matd3.TD3.update_policy` function in the learning algorithms. Here, the critic and the actor are updated according to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        Update the policy of the reinforcement learning agent using the Twin Delayed Deep Deterministic Policy Gradients (TD3) algorithm.\n",
    "\n",
    "        Notes:\n",
    "            This function performs the policy update step, which involves updating the actor (policy) and critic (Q-function) networks\n",
    "            using TD3 algorithm. It iterates over the specified number of gradient steps and performs the following steps for each\n",
    "            learning strategy:\n",
    "\n",
    "            1. Sample a batch of transitions from the replay buffer.\n",
    "            2. Calculate the next actions with added noise using the actor target network.\n",
    "            3. Compute the target Q-values based on the next states, rewards, and the target critic network.\n",
    "            4. Compute the critic loss as the mean squared error between current Q-values and target Q-values.\n",
    "            5. Optimize the critic network by performing a gradient descent step.\n",
    "            6. Optionally, update the actor network if the specified policy delay is reached.\n",
    "            7. Apply Polyak averaging to update target networks.\n",
    "\n",
    "            This function implements the TD3 algorithm's key step for policy improvement and exploration.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(\"Updating Policy\")\n",
    "\n",
    "        n_rl_agents = len(self.learning_role.rl_strats)\n",
    "\n",
    "        for _ in range(self.gradient_steps):\n",
    "            self.n_updates += 1\n",
    "\n",
    "            transitions = self.learning_role.buffer.sample(self.batch_size)\n",
    "            states, actions, next_states, rewards = (\n",
    "                transitions.observations,\n",
    "                transitions.actions,\n",
    "                transitions.next_observations,\n",
    "                transitions.rewards,\n",
    "            )\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Select action according to policy and add clipped noise\n",
    "                # Select action according to policy and add clipped noise\n",
    "                noise = th.randn_like(actions) * self.target_policy_noise\n",
    "                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n",
    "\n",
    "                next_actions = th.stack(\n",
    "                    [\n",
    "                        (\n",
    "                            strategy.actor_target(next_states[:, i, :]) + noise[:, i, :]\n",
    "                        ).clamp(-1, 1)\n",
    "                        for i, strategy in enumerate(\n",
    "                            self.learning_role.rl_strats.values()\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                next_actions = next_actions.transpose(0, 1).contiguous()\n",
    "                next_actions = next_actions.view(-1, n_rl_agents * self.act_dim)\n",
    "\n",
    "            all_actions = actions.view(self.batch_size, -1)\n",
    "\n",
    "            # Precompute unique observation parts for all agents\n",
    "            unique_obs_from_others = states[\n",
    "                :, :, self.obs_dim - self.unique_obs_dim :\n",
    "            ].reshape(self.batch_size, n_rl_agents, -1)\n",
    "            next_unique_obs_from_others = next_states[\n",
    "                :, :, self.obs_dim - self.unique_obs_dim :\n",
    "            ].reshape(self.batch_size, n_rl_agents, -1)\n",
    "\n",
    "            # Loop over all agents and update their actor and critic networks\n",
    "            for i, strategy in enumerate(self.learning_role.rl_strats.values()):\n",
    "                actor = strategy.actor\n",
    "                critic = strategy.critics\n",
    "                critic_target = strategy.target_critics\n",
    "\n",
    "                # Efficiently extract unique observations from all other agents\n",
    "                other_unique_obs = th.cat(\n",
    "                    (unique_obs_from_others[:, :i], unique_obs_from_others[:, i + 1 :]),\n",
    "                    dim=1,\n",
    "                )\n",
    "                other_next_unique_obs = th.cat(\n",
    "                    (\n",
    "                        next_unique_obs_from_others[:, :i],\n",
    "                        next_unique_obs_from_others[:, i + 1 :],\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Construct final state representations\n",
    "                all_states = th.cat(\n",
    "                    (\n",
    "                        states[:, i, :].reshape(self.batch_size, -1),\n",
    "                        other_unique_obs.reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                all_next_states = th.cat(\n",
    "                    (\n",
    "                        next_states[:, i, :].reshape(self.batch_size, -1),\n",
    "                        other_next_unique_obs.reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                with th.no_grad():\n",
    "                    # Compute the next Q-values: min over all critics targets\n",
    "                    next_q_values = th.cat(\n",
    "                        critic_target(all_next_states, next_actions), dim=1\n",
    "                    )\n",
    "                    next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
    "                    target_Q_values = (\n",
    "                        rewards[:, i].unsqueeze(1) + self.gamma * next_q_values\n",
    "                    )\n",
    "\n",
    "                # Get current Q-values estimates for each critic network\n",
    "                current_Q_values = critic(all_states, all_actions)\n",
    "\n",
    "                # Compute critic loss\n",
    "                critic_loss = sum(\n",
    "                    F.mse_loss(current_q, target_Q_values)\n",
    "                    for current_q in current_Q_values\n",
    "                )\n",
    "\n",
    "                # Optimize the critics\n",
    "                critic.optimizer.zero_grad(set_to_none=True)\n",
    "                critic_loss.backward()\n",
    "                # Clip the gradients to avoid exploding gradients and stabilize training\n",
    "                th.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)\n",
    "                critic.optimizer.step()\n",
    "\n",
    "                # Delayed policy updates\n",
    "                if self.n_updates % self.policy_delay == 0:\n",
    "                    # Compute actor loss\n",
    "                    state_i = states[:, i, :]\n",
    "                    action_i = actor(state_i)\n",
    "\n",
    "                    all_actions_clone = actions.clone().detach()\n",
    "                    all_actions_clone[:, i, :] = action_i\n",
    "\n",
    "                    # calculate actor loss\n",
    "                    actor_loss = -critic.q1_forward(\n",
    "                        all_states, all_actions_clone.view(self.batch_size, -1)\n",
    "                    ).mean()\n",
    "\n",
    "                    actor.optimizer.zero_grad(set_to_none=True)\n",
    "                    actor_loss.backward()\n",
    "                    # Clip the gradients to avoid exploding gradients and stabilize training\n",
    "                    th.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)\n",
    "                    actor.optimizer.step()\n",
    "\n",
    "            # Perform batch-wise Polyak update at the end (instead of inside the loop)\n",
    "            if self.n_updates % self.policy_delay == 0:\n",
    "                all_critic_params = []\n",
    "                all_target_critic_params = []\n",
    "\n",
    "                all_actor_params = []\n",
    "                all_target_actor_params = []\n",
    "\n",
    "                for strategy in self.learning_role.rl_strats.values():\n",
    "                    all_critic_params.extend(strategy.critics.parameters())\n",
    "                    all_target_critic_params.extend(\n",
    "                        strategy.target_critics.parameters()\n",
    "                    )\n",
    "\n",
    "                    all_actor_params.extend(strategy.actor.parameters())\n",
    "                    all_target_actor_params.extend(strategy.actor_target.parameters())\n",
    "\n",
    "                # Perform batch-wise Polyak update (NO LOOPS)\n",
    "                polyak_update(all_critic_params, all_target_critic_params, self.tau)\n",
    "                polyak_update(all_actor_params, all_target_actor_params, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa903760",
   "metadata": {},
   "source": [
    "The other functions within the reinforcement learning algorithm are primarily there to store, update, and save the new policies. These functions either write the updated policies to a designated location or save them into the `inter_episodic_data`.\n",
    "\n",
    "If you would like to make a change to this algorithm, the most likely modification would be to the `update_policy` function, as it plays a central role in the learning process. The other functions would only need adjustments if the different algorithm features vary like the target critics or critic architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e10cf",
   "metadata": {
    "id": "L3flH5iY4x7Z"
   },
   "source": [
    "### 3.5 Start the Simulation\n",
    "\n",
    "We are almost done with all the changes to actually be able to make ASSUME learn here in this notebook. If you would rather like to load our pretrained strategies, we need a function for loading parameters, which can be found below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d0fa7",
   "metadata": {
    "id": "cTlqMouufKyo"
   },
   "source": [
    "To control the learning process, the config file determines the parameters of the learning algorithm. As we want to temper with these values in the notebook we will overwrite the learning config in the next cell and then load it into our world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb09b5b",
   "metadata": {
    "id": "moZ_UD7FfkOh"
   },
   "outputs": [],
   "source": [
    "learning_config = {\n",
    "    \"continue_learning\": False,\n",
    "    \"trained_policies_save_path\": None,\n",
    "    \"max_bid_price\": 100,\n",
    "    \"algorithm\": \"matd3\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_episodes\": 100,\n",
    "    \"episodes_collecting_initial_experience\": 5,\n",
    "    \"train_freq\": \"24h\",\n",
    "    \"gradient_steps\": 24,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"noise_sigma\": 0.1,\n",
    "    \"noise_scale\": 1,\n",
    "    \"noise_dt\": 1,\n",
    "    \"validation_episodes_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff2f6a",
   "metadata": {
    "id": "iPz8v4N5hpfr"
   },
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(f\"{inputs_path}/example_02a/config.yaml\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# store our modifications to the config file\n",
    "data[\"base\"][\"learning_mode\"] = True\n",
    "data[\"base\"][\"learning_config\"] = learning_config\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(f\"{inputs_path}/example_02a/config.yaml\", \"w\") as file:\n",
    "    yaml.safe_dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea575f",
   "metadata": {
    "id": "ZlRnTgCy5d9W"
   },
   "source": [
    "In order to let the simulation run with the integrated learning we need to touch up the main file that runs it in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea4585",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZlWxXxZr54WV",
    "lines_to_next_cell": 0,
    "outputId": "e30f4279-7a4e-4efc-9cfb-61416e4fe2f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   7%|▋         | 7/100 [00:48<07:28,  4.83s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from assume.strategies.learning_strategies import RLStrategy\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"./outputs\"\n",
    "os.makedirs(\"./local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Available examples:\n",
    "    - local_db: without database and grafana\n",
    "    - timescale: with database and grafana (note: you need docker installed)\n",
    "    \"\"\"\n",
    "    data_format = \"local_db\"  # \"local_db\" or \"timescale\"\n",
    "\n",
    "    if data_format == \"local_db\":\n",
    "        db_uri = \"sqlite:///./local_db/assume_db.db\"\n",
    "    elif data_format == \"timescale\":\n",
    "        db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "\n",
    "    input_path = inputs_path\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=input_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning(world)\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(world)\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0ef8a",
   "metadata": {},
   "source": [
    "## **Summary and Outlook**\n",
    "\n",
    "Well done! You have now completed your first RL simulation with ASSUME. This tutorial provided the necessary background of the RL structure and implementation for you to modify the learning algorithm.\n",
    "\n",
    "The results of the simulation are stored in the local_db as well as in .csv-files in the [output folder](\\outputs). Feel free to check the outcomes after the simulation has finished.\n",
    "\n",
    "If you'd like to understand how the actual bids of market participants as DRL learning agents are derived, the following tutorial will dive deep into the design of adaptive bidding strategies. \n",
    "\n",
    "**Next up:** [4.2 Designing Adaptive Bidding Strategies in ASSUME using Reinforcement Learning](04b_reinforcement_learning_example.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
