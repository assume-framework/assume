{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dfcafd",
   "metadata": {
    "id": "4JeBorbE6FYr"
   },
   "source": [
    "# 4.1 RL Algorithm tutorial\n",
    "\n",
    "This tutorial will introduce users into the MATD3 implementation in ASSUME and hence how we use reinforcement learning (RL). The main objective of this tutorial is to ensure participants grasp the steps required to equip ASSUME with a RL algorithm. It therefore starts one level deeper than the RL_application example and the knowledge from this tutorial is not required if the already pre-configured algorithm in Assume is to be used. The algorithm explained here is usable as a plug and play solution in the framework. The following coding snippets will highlight the key in the algorithm class and will explain the interactions with the learning role and other classes along the way. \n",
    "\n",
    "The outline of this tutorial is as follows. We will start with an introduction to the changed simulation flow when we use reinforcement learning (1. From one simulation year to learning episodes). If you need a refresher on RL in general, please visit our readthedocs (https://assume.readthedocs.io/en/latest/). Afterwards, we dive into the tasks and reason behind a learning role (2. What role has a learning role) and then dive into the characteristics of the algorithm (3. The MATD3).\n",
    "\n",
    "**Please Note:** The tutorial does not cover coding tasks. It simply provides an overview and explanation of the implementation of reinforcement learning and the flow for those who would like to modify the underlying learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b738b3",
   "metadata": {},
   "source": [
    "## 0. Install Assume\n",
    "\n",
    "First we need to install Assume in this Colab. Here we just install the ASSUME core package via pip. In general the instructions for an installation can be found here: https://assume.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2b8fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0DaRwFA7VgW",
    "outputId": "5655adad-5b7a-4fe3-9067-6b502a06136b"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check whether notebook is run in google colab\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install assume-framework[learning]\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1d3b",
   "metadata": {
    "id": "IIw_QIE3pY34"
   },
   "source": [
    "And easy like this we have ASSUME installed. Now we can let it run. Please note though that we cannot use the functionalities tied to docker and, hence, cannot access the predefined dashboards in colab. For this please install docker and ASSUME on your personal machine.\n",
    "\n",
    "Further we would like to access the predefined scenarios in ASSUME which are stored on the git repository. Hence, we clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e77f71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5hB0uDisSsg",
    "outputId": "1241881f-e090-4f26-9b02-560adfcb3a3e"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf23b1c",
   "metadata": {
    "id": "Fg7DyNjLuvSb"
   },
   "source": [
    "**Let the magic happen.** Now you can run your first ever simulation in ASSUME. The following code navigates to the respective assume folder and starts the simulation example example_01b using the local database here in colab.\n",
    "\n",
    "When running locally, you can also just run `assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\"` in a shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de097384",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eVM60Qx8SC0",
    "outputId": "20434515-6e65-4d34-d44d-8c4529a46ece"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !cd assume-repo && assume -s example_01b -db \"sqlite:///./examples/local_db/assume_db_example_01b.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc1c12",
   "metadata": {},
   "source": [
    "**Select input files path**:\n",
    "\n",
    "We also need to differentiate between the input file paths when using this tutorial in Google Colab and a local environment. The code snippets will include both options for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9899ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63deda6",
   "metadata": {
    "id": "bj2C4ElILNNv"
   },
   "source": [
    "## 1. From one simulation year to learning episodes\n",
    "\n",
    "In a normal simulation without reinforcement learning, we only run the time horizon of the simulation once. For RL the agents need to learn their strategy based on interactions. For that to work, a RL agent has to see a situation, aka a simulation hour, multiple times, and hence we need to run the entire simulation horizon multiple times as well.   \n",
    "\n",
    "To enable this we define a run learning function that will be called if the simulation is started and we defined in our config that we want to activate learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9569b2d",
   "metadata": {
    "id": "zMyZhaNM7NRP"
   },
   "source": [
    "**But first some imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade14744",
   "metadata": {
    "id": "xUsbeZdPJ_2Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from assume.common.exceptions import AssumeException\n",
    "from assume.reinforcement_learning.algorithms.base_algorithm import RLAlgorithm\n",
    "from assume.reinforcement_learning.algorithms.matd3 import TD3\n",
    "from assume.reinforcement_learning.buffer import ReplayBuffer\n",
    "from assume.reinforcement_learning.learning_role import Learning\n",
    "from assume.reinforcement_learning.learning_utils import polyak_update\n",
    "from assume.scenario.loader_csv import (\n",
    "    load_scenario_folder,\n",
    "    setup_world,\n",
    ")\n",
    "from assume.world import World\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2dae5",
   "metadata": {},
   "source": [
    "This flowchart provides an overview of the key stages involved in the run_learning function, which trains Deep Reinforcement Learning (DRL) agents within a simulated market environment. The process is divided into five main steps:\n",
    "\n",
    "**Initialization of the Learning Process**: The function begins by setting up the environment, initializing policies, and configuring necessary settings such as logging and buffer allocation. It ensures that no existing policies are overwritten without confirmation.\n",
    "\n",
    "**Training Loop**: This is the outer loop where multiple training episodes are executed. For each episode, the world simulation is completely re-initialized and reset after execution, meaning the simulation environment is essentially killed after each episode. Crucially, all necessary information that must persist across episodes—such as collected experience stored in the buffer—is maintained in the inter-episodic data. This data is key to ensuring the continuity of the learning process as it allows the DRL agents to build knowledge over time.\n",
    "\n",
    "**Evaluation Loop**: Nested within the training loop, the evaluation loop periodically assesses the performance of the learned policies. Based on average rewards, the best-performing policies are saved, and the function determines if further training is necessary.\n",
    "\n",
    "**Terminate Learning**: At the end of the training phase, the function saves the final version of the learned policies, ensuring that the results are stored for future use.\n",
    "\n",
    "**Final Evaluation Run**: A final evaluation run is conducted using the best policies from the training phase, providing a benchmark for overall performance.\n",
    "\n",
    "The flowchart visually represents the interaction between the training and evaluation loops, highlighting the progression through these key stages.\n",
    "\n",
    "<img src=\"../../docs/source/img/Assume_run_learning_loop.png\" alt=\"Learning Process Flowchart\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94517a3e",
   "metadata": {
    "id": "UXYSesx4Ifp5"
   },
   "outputs": [],
   "source": [
    "def run_learning(\n",
    "    world: World,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Learning (DRL) agents to act in a simulated market environment.\n",
    "\n",
    "    This function runs multiple episodes of simulation to train DRL agents, performs evaluation, and saves the best runs. It maintains the buffer and learned agents in memory to avoid resetting them with each new run.\n",
    "\n",
    "    Args:\n",
    "        world (World): An instance of the World class representing the simulation environment.\n",
    "        inputs_path (str): The path to the folder containing input files necessary for the simulation.\n",
    "        scenario (str): The name of the scenario for the simulation.\n",
    "        study_case (str): The specific study case for the simulation.\n",
    "\n",
    "    Note:\n",
    "        - The function uses a ReplayBuffer to store experiences for training the DRL agents.\n",
    "        - It iterates through training episodes, updating the agents and evaluating their performance at regular intervals.\n",
    "        - Initial exploration is active at the beginning and is disabled after a certain number of episodes to improve the performance of DRL algorithms.\n",
    "        - Upon completion of training, the function performs an evaluation run using the best policy learned during training.\n",
    "        - The best policies are chosen based on the average reward obtained during the evaluation runs, and they are saved for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    if not verbose:\n",
    "        logger.setLevel(logging.WARNING)\n",
    "\n",
    "    # remove csv path so that nothing is written while learning\n",
    "    temp_csv_path = world.export_csv_path\n",
    "    world.export_csv_path = \"\"\n",
    "\n",
    "    # initialize policies already here to set the obs_dim and act_dim in the learning role\n",
    "    world.learning_role.rl_algorithm.initialize_policy()\n",
    "\n",
    "    # check if we already stored policies for this simulation\n",
    "    save_path = world.learning_config[\"trained_policies_save_path\"]\n",
    "\n",
    "    if Path(save_path).is_dir() and not world.learning_config[\"continue_learning\"]:\n",
    "        # we are in learning mode and about to train new policies, which might overwrite existing ones\n",
    "        accept = input(\n",
    "            f\"{save_path=} exists - should we overwrite current learned strategies? (y/N) \"\n",
    "        )\n",
    "        if accept.lower().startswith(\"y\"):\n",
    "            # remove existing policies\n",
    "            if os.path.exists(save_path):\n",
    "                shutil.rmtree(save_path, ignore_errors=True)\n",
    "\n",
    "        else:\n",
    "            # stop here - do not start learning or save anything\n",
    "            raise AssumeException(\n",
    "                \"Simulation aborted by user not to overwrite existing learned strategies. You can use 'simulation_id' parameter in the config to start a new simulation.\"\n",
    "            )\n",
    "\n",
    "    # also remove tensorboard logs\n",
    "    tensorboard_path = f\"tensorboard/{world.scenario_data['simulation_id']}\"\n",
    "    if os.path.exists(tensorboard_path):\n",
    "        shutil.rmtree(tensorboard_path, ignore_errors=True)\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # Information that needs to be stored across episodes, aka one simulation run\n",
    "    inter_episodic_data = {\n",
    "        \"buffer\": ReplayBuffer(\n",
    "            buffer_size=int(world.learning_config.get(\"replay_buffer_size\", 5e5)),\n",
    "            obs_dim=world.learning_role.rl_algorithm.obs_dim,\n",
    "            act_dim=world.learning_role.rl_algorithm.act_dim,\n",
    "            n_rl_units=len(world.learning_role.rl_strats),\n",
    "            device=world.learning_role.device,\n",
    "            float_type=world.learning_role.float_type,\n",
    "        ),\n",
    "        \"actors_and_critics\": None,\n",
    "        \"max_eval\": defaultdict(lambda: -1e9),\n",
    "        \"all_eval\": defaultdict(list),\n",
    "        \"avg_all_eval\": [],\n",
    "        \"episodes_done\": 0,\n",
    "        \"eval_episodes_done\": 0,\n",
    "    }\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "    # -----------------------------------------\n",
    "\n",
    "    validation_interval = min(\n",
    "        world.learning_role.training_episodes,\n",
    "        world.learning_config.get(\"validation_episodes_interval\", 5),\n",
    "    )\n",
    "\n",
    "    eval_episode = 1\n",
    "\n",
    "    for episode in tqdm(\n",
    "        range(1, world.learning_role.training_episodes + 1),\n",
    "        desc=\"Training Episodes\",\n",
    "    ):\n",
    "        # -----------------------------------------\n",
    "        # Give the newly initialized learning role the needed information across episodes\n",
    "        if episode != 1:\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                episode=episode,\n",
    "            )\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "        world.run()\n",
    "\n",
    "        world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Store updated information across episodes\n",
    "        inter_episodic_data = world.learning_role.get_inter_episodic_data()\n",
    "        inter_episodic_data[\"episodes_done\"] = episode\n",
    "\n",
    "        # evaluation run:\n",
    "        if (\n",
    "            episode % validation_interval == 0\n",
    "            and episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.reset()\n",
    "\n",
    "            # load evaluation run\n",
    "            setup_world(\n",
    "                world=world,\n",
    "                evaluation_mode=True,\n",
    "                episode=episode,\n",
    "                eval_episode=eval_episode,\n",
    "            )\n",
    "\n",
    "            world.learning_role.load_inter_episodic_data(inter_episodic_data)\n",
    "\n",
    "            world.run()\n",
    "\n",
    "            world.learning_role.tensor_board_logger.update_tensorboard()\n",
    "\n",
    "            total_rewards = world.output_role.get_sum_reward(episode=eval_episode)\n",
    "\n",
    "            if len(total_rewards) == 0:\n",
    "                raise AssumeException(\"No rewards were collected during evaluation run\")\n",
    "\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "\n",
    "            # check reward improvement in evaluation run\n",
    "            # and store best run in eval folder\n",
    "            terminate = world.learning_role.compare_and_save_policies(\n",
    "                {\"avg_reward\": avg_reward}\n",
    "            )\n",
    "\n",
    "            inter_episodic_data[\"eval_episodes_done\"] = eval_episode\n",
    "\n",
    "            # if we have not improved in the last x evaluations, we stop loop\n",
    "            if terminate:\n",
    "                break\n",
    "\n",
    "            eval_episode += 1\n",
    "\n",
    "        world.reset()\n",
    "\n",
    "        # save the policies after each episode in case the simulation is stopped or crashes\n",
    "        if (\n",
    "            episode\n",
    "            >= world.learning_role.episodes_collecting_initial_experience\n",
    "            + validation_interval\n",
    "        ):\n",
    "            world.learning_role.rl_algorithm.save_params(\n",
    "                directory=f\"{world.learning_role.trained_policies_save_path}/last_policies\"\n",
    "            )\n",
    "\n",
    "    # container shutdown implicitly with new initialisation\n",
    "    logger.info(\"################\")\n",
    "    logger.info(\"Training finished, Start evaluation run\")\n",
    "    world.export_csv_path = temp_csv_path\n",
    "\n",
    "    world.reset()\n",
    "\n",
    "    # Set 'trained_policies_load_path' to None in order to load the most recent policies,\n",
    "    # especially if previous strategies were loaded from an external source.\n",
    "    # This is useful when continuing from a previous learning session.\n",
    "    world.scenario_data[\"config\"][\"learning_config\"][\"trained_policies_load_path\"] = (\n",
    "        f\"{world.learning_role.trained_policies_save_path}/avg_reward_eval_policies\"\n",
    "    )\n",
    "\n",
    "    # load scenario for evaluation\n",
    "    setup_world(\n",
    "        world=world,\n",
    "        terminate_learning=True,\n",
    "    )\n",
    "\n",
    "    world.learning_role.load_inter_episodic_data(inter_episodic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894edbd9",
   "metadata": {
    "id": "8UM1QPZrIdqK"
   },
   "source": [
    "## 2. What role has a learning role\n",
    "\n",
    "The LearningRole class in learning_role.py is a central component of the reinforcement learning framework. It manages configurations, device settings, early stopping of the learning process, and initializes various RL strategies, the algorithm and buffers. This class ensures that the RL agent can be trained or evaluated effectively, leveraging the available hardware and adhering to the specified configurations. The parameters of the learning process are also described in the read-the-docs under learning_algorithms.\n",
    "\n",
    "### 2.1 Learning Data Management\n",
    "\n",
    "One key feature of the LearningRole class is its ability to load and manage the inter episodic data. This involves storing experiences and the training progress and retrieving this data to train the RL agent. By efficiently handling episodic data, the LearningRole class enables the agent to learn from past experiences and improve its performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    \"\"\"\n",
    "    This class manages the learning process of reinforcement learning agents, including initializing key components such as\n",
    "    neural networks, replay buffer, and learning hyperparameters. It handles both training and evaluation modes based on\n",
    "    the provided learning configuration.\n",
    "\n",
    "    Args:\n",
    "        simulation_start (datetime.datetime): The start of the simulation.\n",
    "        simulation_end (datetime.datetime): The end of the simulation.\n",
    "        learning_config (LearningConfig): The configuration for the learning process.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def load_inter_episodic_data(self, inter_episodic_data):\n",
    "        \"\"\"\n",
    "        Load the inter-episodic data from the dict stored across simulation runs.\n",
    "\n",
    "        Args:\n",
    "            inter_episodic_data (dict): The inter-episodic data to be loaded.\n",
    "\n",
    "        \"\"\"\n",
    "        self.episodes_done = inter_episodic_data[\"episodes_done\"]\n",
    "        self.eval_episodes_done = inter_episodic_data[\"eval_episodes_done\"]\n",
    "        self.max_eval = inter_episodic_data[\"max_eval\"]\n",
    "        self.rl_eval = inter_episodic_data[\"all_eval\"]\n",
    "        self.avg_rewards = inter_episodic_data[\"avg_all_eval\"]\n",
    "        self.buffer = inter_episodic_data[\"buffer\"]\n",
    "\n",
    "        # if enough initial experience was collected according to specifications in learning config\n",
    "        # turn off initial exploration and go into full learning mode\n",
    "        if self.episodes_done >= self.episodes_collecting_initial_experience:\n",
    "            self.turn_off_initial_exploration()\n",
    "\n",
    "        self.set_noise_scale(inter_episodic_data[\"noise_scale\"])\n",
    "\n",
    "        self.initialize_policy(inter_episodic_data[\"actors_and_critics\"])\n",
    "\n",
    "    def get_inter_episodic_data(self):\n",
    "        \"\"\"\n",
    "        Dump the inter-episodic data to a dict for storing across simulation runs.\n",
    "\n",
    "        Returns:\n",
    "            dict: The inter-episodic data to be stored.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"episodes_done\": self.episodes_done,\n",
    "            \"eval_episodes_done\": self.eval_episodes_done,\n",
    "            \"max_eval\": self.max_eval,\n",
    "            \"all_eval\": self.rl_eval,\n",
    "            \"avg_all_eval\": self.avg_rewards,\n",
    "            \"buffer\": self.buffer,\n",
    "            \"actors_and_critics\": self.rl_algorithm.extract_policy(),\n",
    "            \"noise_scale\": self.get_noise_scale(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de9ea4",
   "metadata": {},
   "source": [
    "The metrics in `inter_episodic_data` are stored for the following reasons:\n",
    "\n",
    "- `episodes_done` and `eval_episodes_done`: **Monitoring Progress**  \n",
    "  Keeping track of the number of episodes completed.\n",
    "\n",
    "- `max_eval`, `all_eval`, `avg_all_eval`: **Evaluating Performance**  \n",
    "  Storing evaluation scores and average rewards to assess the agent's performance across episodes.\n",
    "\n",
    "- `buffer`: **Experience Replay**  \n",
    "  Using a replay buffer to learn from past experiences and improve data efficiency.\n",
    "\n",
    "- `noise_scale`: **Policy Exploration**  \n",
    "  The noise is used to include exploration in the policy. It is decreased across episode numbers, and we store the current noise value to continue the decrease across future episodes.\n",
    "\n",
    "- `actors_and_critics`: **Policy Initialization**  \n",
    "  Initializing the policy with actors and critics (`self.initialize_policy()`) ensures that the agent starts with the pre-defined strategy from the previous episode and can improve upon it through learning.\n",
    "\n",
    "\n",
    "### 2.2 Learning Algorithm\n",
    "\n",
    "If learning is used, then the learning role initializes a learning algorithm which is the heart of the learning progress. Currently, only the MATD3 is implemented, but we are working on different PPO implementations as well. If you would like to add an algorithm it would be integrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632844c2",
   "metadata": {
    "id": "0ww-L9fABnw3"
   },
   "outputs": [],
   "source": [
    "class Learning(Learning):\n",
    "    def create_learning_algorithm(self, algorithm: RLAlgorithm):\n",
    "        \"\"\"\n",
    "        Create and initialize the reinforcement learning algorithm.\n",
    "\n",
    "        This method creates and initializes the reinforcement learning algorithm based on the specified algorithm name. The algorithm\n",
    "        is associated with the learning role and configured with relevant hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            algorithm (RLAlgorithm): The name of the reinforcement learning algorithm.\n",
    "        \"\"\"\n",
    "        if algorithm == \"matd3\":\n",
    "            self.rl_algorithm = TD3(\n",
    "                learning_role=self,\n",
    "                learning_rate=self.learning_rate,\n",
    "                episodes_collecting_initial_experience=self.episodes_collecting_initial_experience,\n",
    "                gradient_steps=self.gradient_steps,\n",
    "                batch_size=self.batch_size,\n",
    "                gamma=self.gamma,\n",
    "                actor_architecture=self.actor_architecture,\n",
    "            )\n",
    "        else:\n",
    "            logger.error(f\"Learning algorithm {algorithm} not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1f820",
   "metadata": {},
   "source": [
    "## 3 Learning Algorithm Flow in Assume\n",
    "\n",
    "The following graph illustrates the structure and flow of the learning algorithm within the reinforcement learning framework.\n",
    "\n",
    "<img src=\"../../docs/source/img/TD3_algorithm.jpeg\" alt=\"Learning Algorithm Graph\" height=\"400\">\n",
    "\n",
    "Within the algorithm, we distinguish three different steps that are translated into ASSUME in the following way:\n",
    "\n",
    "1. **Initialization**: This is the first step where all necessary components such as the actors, critics, and buffer are set up.\n",
    "\n",
    "2. **Experience Collection**: The second step, represented in the flowchart above within the loop, involves the collection of experience. This includes choosing an action, observing a reward, and storing the transition tuple in the buffer.\n",
    "\n",
    "3. **Policy Update**: The third step is the actual policy update, which is also performed within the loop, allowing the agent to improve its performance over time.\n",
    "\n",
    "\n",
    "### 3.1 Initialization\n",
    "\n",
    "The initialization of the actors, critics, and the buffer is handled via the `learning_role` and the `inter_episodic_data`, as described earlier. The `create_learning_algorithm` function triggers their initialization in `initialize_policy`. At the beginning of the training process, they are initialized with new random settings. In subsequent episodes, they are initialized with pre-learned data, ensuring that previous learning is retained and built upon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def initialize_policy(self, actors_and_critics: dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Create actor and critic networks for reinforcement learning.\n",
    "\n",
    "        If `actors_and_critics` is None, this method creates new actor and critic networks.\n",
    "        If `actors_and_critics` is provided, it assigns existing networks to the respective attributes.\n",
    "\n",
    "        Args:\n",
    "            actors_and_critics (dict): The actor and critic networks to be assigned.\n",
    "\n",
    "        \"\"\"\n",
    "        if actors_and_critics is None:\n",
    "            self.create_actors()\n",
    "            self.create_critics()\n",
    "\n",
    "        else:\n",
    "            for u_id, strategy in self.learning_role.rl_strats.items():\n",
    "                strategy.actor = actors_and_critics[\"actors\"][u_id]\n",
    "                strategy.actor_target = actors_and_critics[\"actor_targets\"][u_id]\n",
    "\n",
    "                strategy.critics = actors_and_critics[\"critics\"][u_id]\n",
    "                strategy.target_critics = actors_and_critics[\"target_critics\"][u_id]\n",
    "\n",
    "            self.obs_dim = actors_and_critics[\"obs_dim\"]\n",
    "            self.act_dim = actors_and_critics[\"act_dim\"]\n",
    "            self.unique_obs_dim = actors_and_critics[\"unique_obs_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b64d11",
   "metadata": {},
   "source": [
    "Please also note that we make a distinction in the handling of the critics and target critics compared to the actors and target actors. You can observe this in the `initialize_policy` function. For the critics, they are assigned to the `learning_role` as there are centralized critics used for all the different actors. In contrast, the actors are assigned to specific unit strategies. Each learning unit, such as a power plant, has one learning strategy and therefore an individual actor, while the critics remain centralized.\n",
    "\n",
    "This distinction leads to the case where, even if learning is not active, we still need the actors to perform the entire simulation using pre-trained policies. This is essential, for example, when running simulations with previously learned policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88b3d9",
   "metadata": {},
   "source": [
    "### 3.2 Experience Collection\n",
    "\n",
    "Within the loop, the selection of an action with exploration noise, as well as the observation of a new reward and state, and the storing of this tuple in the buffer, are all handled within the bidding strategy. \n",
    "\n",
    "This specific process is covered in more detail in another tutorial. For more details, refer to [tutorial 04](04_reinforcement_learning_example.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df537a91",
   "metadata": {},
   "source": [
    "### 3.3 Policy Update \n",
    "\n",
    "The core of the algorithm, which comprises all other steps is embodied by the `assume.reinforcement_learning.algorithms.matd3.TD3.update_policy` function in the learning algorithms. Here, the critic and the actor are updated according to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        Update the policy of the reinforcement learning agent using the Twin Delayed Deep Deterministic Policy Gradients (TD3) algorithm.\n",
    "\n",
    "        Notes:\n",
    "            This function performs the policy update step, which involves updating the actor (policy) and critic (Q-function) networks\n",
    "            using TD3 algorithm. It iterates over the specified number of gradient steps and performs the following steps for each\n",
    "            learning strategy:\n",
    "\n",
    "            1. Sample a batch of transitions from the replay buffer.\n",
    "            2. Calculate the next actions with added noise using the actor target network.\n",
    "            3. Compute the target Q-values based on the next states, rewards, and the target critic network.\n",
    "            4. Compute the critic loss as the mean squared error between current Q-values and target Q-values.\n",
    "            5. Optimize the critic network by performing a gradient descent step.\n",
    "            6. Optionally, update the actor network if the specified policy delay is reached.\n",
    "            7. Apply Polyak averaging to update target networks.\n",
    "\n",
    "            This function implements the TD3 algorithm's key step for policy improvement and exploration.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(\"Updating Policy\")\n",
    "\n",
    "        n_rl_agents = len(self.learning_role.rl_strats)\n",
    "\n",
    "        for _ in range(self.gradient_steps):\n",
    "            self.n_updates += 1\n",
    "\n",
    "            transitions = self.learning_role.buffer.sample(self.batch_size)\n",
    "            states, actions, next_states, rewards = (\n",
    "                transitions.observations,\n",
    "                transitions.actions,\n",
    "                transitions.next_observations,\n",
    "                transitions.rewards,\n",
    "            )\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Select action according to policy and add clipped noise\n",
    "                # Select action according to policy and add clipped noise\n",
    "                noise = th.randn_like(actions) * self.target_policy_noise\n",
    "                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)\n",
    "\n",
    "                next_actions = th.stack(\n",
    "                    [\n",
    "                        (\n",
    "                            strategy.actor_target(next_states[:, i, :]) + noise[:, i, :]\n",
    "                        ).clamp(-1, 1)\n",
    "                        for i, strategy in enumerate(\n",
    "                            self.learning_role.rl_strats.values()\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                next_actions = next_actions.transpose(0, 1).contiguous()\n",
    "                next_actions = next_actions.view(-1, n_rl_agents * self.act_dim)\n",
    "\n",
    "            all_actions = actions.view(self.batch_size, -1)\n",
    "\n",
    "            # Precompute unique observation parts for all agents\n",
    "            unique_obs_from_others = states[\n",
    "                :, :, self.obs_dim - self.unique_obs_dim :\n",
    "            ].reshape(self.batch_size, n_rl_agents, -1)\n",
    "            next_unique_obs_from_others = next_states[\n",
    "                :, :, self.obs_dim - self.unique_obs_dim :\n",
    "            ].reshape(self.batch_size, n_rl_agents, -1)\n",
    "\n",
    "            # Loop over all agents and update their actor and critic networks\n",
    "            for i, strategy in enumerate(self.learning_role.rl_strats.values()):\n",
    "                actor = strategy.actor\n",
    "                critic = strategy.critics\n",
    "                critic_target = strategy.target_critics\n",
    "\n",
    "                # Efficiently extract unique observations from all other agents\n",
    "                other_unique_obs = th.cat(\n",
    "                    (unique_obs_from_others[:, :i], unique_obs_from_others[:, i + 1 :]),\n",
    "                    dim=1,\n",
    "                )\n",
    "                other_next_unique_obs = th.cat(\n",
    "                    (\n",
    "                        next_unique_obs_from_others[:, :i],\n",
    "                        next_unique_obs_from_others[:, i + 1 :],\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Construct final state representations\n",
    "                all_states = th.cat(\n",
    "                    (\n",
    "                        states[:, i, :].reshape(self.batch_size, -1),\n",
    "                        other_unique_obs.reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                all_next_states = th.cat(\n",
    "                    (\n",
    "                        next_states[:, i, :].reshape(self.batch_size, -1),\n",
    "                        other_next_unique_obs.reshape(self.batch_size, -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                with th.no_grad():\n",
    "                    # Compute the next Q-values: min over all critics targets\n",
    "                    next_q_values = th.cat(\n",
    "                        critic_target(all_next_states, next_actions), dim=1\n",
    "                    )\n",
    "                    next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
    "                    target_Q_values = (\n",
    "                        rewards[:, i].unsqueeze(1) + self.gamma * next_q_values\n",
    "                    )\n",
    "\n",
    "                # Get current Q-values estimates for each critic network\n",
    "                current_Q_values = critic(all_states, all_actions)\n",
    "\n",
    "                # Compute critic loss\n",
    "                critic_loss = sum(\n",
    "                    F.mse_loss(current_q, target_Q_values)\n",
    "                    for current_q in current_Q_values\n",
    "                )\n",
    "\n",
    "                # Optimize the critics\n",
    "                critic.optimizer.zero_grad(set_to_none=True)\n",
    "                critic_loss.backward()\n",
    "                # Clip the gradients to avoid exploding gradients and stabilize training\n",
    "                th.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)\n",
    "                critic.optimizer.step()\n",
    "\n",
    "                # Delayed policy updates\n",
    "                if self.n_updates % self.policy_delay == 0:\n",
    "                    # Compute actor loss\n",
    "                    state_i = states[:, i, :]\n",
    "                    action_i = actor(state_i)\n",
    "\n",
    "                    all_actions_clone = actions.clone().detach()\n",
    "                    all_actions_clone[:, i, :] = action_i\n",
    "\n",
    "                    # calculate actor loss\n",
    "                    actor_loss = -critic.q1_forward(\n",
    "                        all_states, all_actions_clone.view(self.batch_size, -1)\n",
    "                    ).mean()\n",
    "\n",
    "                    actor.optimizer.zero_grad(set_to_none=True)\n",
    "                    actor_loss.backward()\n",
    "                    # Clip the gradients to avoid exploding gradients and stabilize training\n",
    "                    th.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)\n",
    "                    actor.optimizer.step()\n",
    "\n",
    "            # Perform batch-wise Polyak update at the end (instead of inside the loop)\n",
    "            if self.n_updates % self.policy_delay == 0:\n",
    "                all_critic_params = []\n",
    "                all_target_critic_params = []\n",
    "\n",
    "                all_actor_params = []\n",
    "                all_target_actor_params = []\n",
    "\n",
    "                for strategy in self.learning_role.rl_strats.values():\n",
    "                    all_critic_params.extend(strategy.critics.parameters())\n",
    "                    all_target_critic_params.extend(\n",
    "                        strategy.target_critics.parameters()\n",
    "                    )\n",
    "\n",
    "                    all_actor_params.extend(strategy.actor.parameters())\n",
    "                    all_target_actor_params.extend(strategy.actor_target.parameters())\n",
    "\n",
    "                # Perform batch-wise Polyak update (NO LOOPS)\n",
    "                polyak_update(all_critic_params, all_target_critic_params, self.tau)\n",
    "                polyak_update(all_actor_params, all_target_actor_params, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa903760",
   "metadata": {},
   "source": [
    "The other functions within the reinforcement learning algorithm are primarily there to store, update, and save the new policies. These functions either write the updated policies to a designated location or save them into the `inter_episodic_data`.\n",
    "\n",
    "If you would like to make a change to this algorithm, the most likely modification would be to the `update_policy` function, as it plays a central role in the learning process. The other functions would only need adjustments if the different algorithm features vary like the target critics or critic architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e10cf",
   "metadata": {
    "id": "L3flH5iY4x7Z"
   },
   "source": [
    "### 3.5 Start the simulation\n",
    "\n",
    "We are almost done with all the changes to actually be able to make ASSUME learn here in google colab. If you would rather like to load our pretrained strategies, we need a function for loading parameters, which can be found below.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d0fa7",
   "metadata": {
    "id": "cTlqMouufKyo"
   },
   "source": [
    "To control the learning process, the config file determines the parameters of the learning algorithm. As we want to temper with these values in the notebook we will overwrite the learning config in the next cell and then load it into our world.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb09b5b",
   "metadata": {
    "id": "moZ_UD7FfkOh"
   },
   "outputs": [],
   "source": [
    "learning_config = {\n",
    "    \"continue_learning\": False,\n",
    "    \"trained_policies_save_path\": None,\n",
    "    \"max_bid_price\": 100,\n",
    "    \"algorithm\": \"matd3\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"training_episodes\": 100,\n",
    "    \"episodes_collecting_initial_experience\": 5,\n",
    "    \"train_freq\": \"24h\",\n",
    "    \"gradient_steps\": 24,\n",
    "    \"batch_size\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"noise_sigma\": 0.1,\n",
    "    \"noise_scale\": 1,\n",
    "    \"noise_dt\": 1,\n",
    "    \"validation_episodes_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff2f6a",
   "metadata": {
    "id": "iPz8v4N5hpfr"
   },
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(f\"{inputs_path}/example_02a/config.yaml\") as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "# store our modifications to the config file\n",
    "data[\"base\"][\"learning_mode\"] = True\n",
    "data[\"base\"][\"learning_config\"] = learning_config\n",
    "\n",
    "# Write the modified data back to the file\n",
    "with open(f\"{inputs_path}/example_02a/config.yaml\", \"w\") as file:\n",
    "    yaml.safe_dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea575f",
   "metadata": {
    "id": "ZlRnTgCy5d9W"
   },
   "source": [
    "In order to let the simulation run with the integrated learning we need to touch up the main file that runs it in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea4585",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZlWxXxZr54WV",
    "lines_to_next_cell": 0,
    "outputId": "e30f4279-7a4e-4efc-9cfb-61416e4fe2f1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from assume.strategies.learning_strategies import RLStrategy\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"./outputs\"\n",
    "os.makedirs(\"./local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Available examples:\n",
    "    - local_db: without database and grafana\n",
    "    - timescale: with database and grafana (note: you need docker installed)\n",
    "    \"\"\"\n",
    "    data_format = \"local_db\"  # \"local_db\" or \"timescale\"\n",
    "\n",
    "    if data_format == \"local_db\":\n",
    "        db_uri = \"sqlite:///./local_db/assume_db.db\"\n",
    "    elif data_format == \"timescale\":\n",
    "        db_uri = \"postgresql://assume:assume@localhost:5432/assume\"\n",
    "\n",
    "    input_path = inputs_path\n",
    "    scenario = \"example_02a\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # we import our defined bidding strategey class including the learning into the world bidding strategies\n",
    "    # in the example files we provided the name of the learning bidding strategeis in the input csv is  \"pp_learning\"\n",
    "    # hence we define this strategey to be one of the learning class\n",
    "    world.bidding_strategies[\"pp_learning\"] = RLStrategy\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=input_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # run learning if learning mode is enabled\n",
    "    # needed as we simulate the modelling horizon multiple times to train reinforcement learning run_learning(world)\n",
    "\n",
    "    if world.learning_config.get(\"learning_mode\", False):\n",
    "        run_learning(world)\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
