{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344c88d7",
   "metadata": {},
   "source": [
    "# 11a. Redispatch modelling in the ASSUME Framework\n",
    "\n",
    "Welcome to the ASSUME DSM Workshop!\n",
    "\n",
    "This tutorial demonstrates modelling and simulation of redispatch mechanism using **PyPSA** as a plug and play module in **ASSUME-framework**. The model will be created mainly taking grid constraints into consideration to identify grid bottlenecks with dispatches from EOM and resolve them using the redispatch algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept of Redispatch\n",
    "\n",
    "The locational mismatch in demand and generation of electricity needs transmission of electricity from low demand regions to high demand regions. The transmission capacity limits the maximum amounts of electricity which can be transmitted at any point in time. If there is no enough capacity to transmit the required amount of electricity then there is a need of ramping down of generation at the locations of low demand and ramping up of generation at the locations of higher demand. This is typically called as Redispatch. Apart from spot markets there is redispatch mechanism to regulate this grid flows to avoid congestion issues. It is operated and controlled by the System operators (SO).\n",
    "\n",
    "## Objective \n",
    "The aim of redispatch is to reduce the overall cost of Redispatch(starting up, shutting down, ramping up, ramping down).\n",
    "\n",
    "## Structure in Redispatch model\n",
    "- The redispatch has following structure:\n",
    "    1. **Ramping up of powerplants**\n",
    "    2. **Ramping down of powerplants**:\n",
    "    3. **Ramping up/down of Demand Side flexibilites**:\n",
    "\n",
    "### Key Sections\n",
    "\n",
    "- **Section 1:** 3 node example for modelling Redispatch\n",
    "- **Section 2:** 3 node example for modelling DSM Units\n",
    "- **Section 3:** Germany scale example for modelling Redispatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f37e7",
   "metadata": {},
   "source": [
    "## 0. Install Assume\n",
    "\n",
    "First we need to install Assume in this Colab. Here we just install the ASSUME core package via pip. In general the instructions for an installation can be found here: https://assume.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45435f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check whether notebook is run in google colab\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install assume-framework\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0\n",
    "if IN_COLAB:\n",
    "    # Install some additional packages for plotting\n",
    "    !pip install plotly\n",
    "    !pip install cartopy\n",
    "    !pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ebf71",
   "metadata": {},
   "source": [
    "> **Note**: After installation, **Colab may prompt you to restart the session** due to dependency changes.\n",
    "> To do so, click **\"Runtime\" â†’ \"Restart session...\"** in the menu bar, then re-run the cells above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3abff",
   "metadata": {},
   "source": [
    "Further we would like to access the predefined scenarios in ASSUME which are stored on the git repository. Hence, we clone the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f0fc5",
   "metadata": {},
   "source": [
    "## 0.1 Repository Setup\n",
    "\n",
    "To access predefined simulation scenarios, clone the ASSUME repository (Colab only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d99793",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c0196",
   "metadata": {},
   "source": [
    "> Local users may skip this step if input files are already available in the project directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4333989",
   "metadata": {},
   "source": [
    "## 0.2 Input Path Configuration\n",
    "\n",
    "We define the path to input files depending on whether you're in Colab or working locally. This variable will be used to load configuration and scenario files throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d23293",
   "metadata": {},
   "source": [
    "## 0.3 Installation Check\n",
    "\n",
    "Use the following cell to ensure the installation was successful and that essential components are available. This test ensures that the simulation engine and RL strategy base class are accessible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from assume import World\n",
    "\n",
    "    print(\"ASSUME framework is installed and functional.\")\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import essential components:\", e)\n",
    "    print(\n",
    "        \"Please review the installation instructions and ensure all dependencies are installed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac95da5",
   "metadata": {},
   "source": [
    "Colab does not support Docker, so dashboard visualizations included in some ASSUME workflows will not be available. However, simulation runs and RL training can still be fully executed.\n",
    "\n",
    "* In **Colab**: Training and basic plotting are supported.\n",
    "* In **Local environments with Docker**: Full access, including dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647cf65",
   "metadata": {},
   "source": [
    "Let's also import some basic libraries that we will use throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1371c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections.abc import Callable\n",
    "from assume.units.demand import Demand\n",
    "from assume.common.forecasts import NaiveForecast\n",
    "import pyomo as pyo\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "# Function to display DataFrame in Jupyter\n",
    "from IPython.display import display\n",
    "from assume import World\n",
    "from assume.common.base import (\n",
    "    BaseStrategy,\n",
    "    MarketConfig,\n",
    "    Orderbook,\n",
    "    Product,\n",
    "    SupportsMinMax,\n",
    ")\n",
    "from assume.strategies import NaiveDADSMStrategy\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "from assume.units.dsm_load_shift import DSMFlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabdcb8",
   "metadata": {},
   "source": [
    "### Scenario 1: Redispatch (3-node Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432260cb",
   "metadata": {},
   "source": [
    "The grid infrastructure includes mainly three components:\n",
    "\n",
    "- **Generators**: Used to produce hydrogen for steel production.\n",
    "- **Loads**: Directly reduces iron ore using hydrogen.\n",
    "- **Transmission grid**: Converts the reduced iron into steel.\n",
    "\n",
    "\n",
    "Here the components are defined with their operational constraints (such as power, efficiency, ramp rates etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b293902",
   "metadata": {},
   "source": [
    "#### **Step 1: Define the nodes/buses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define meta-data for buses/nodes \n",
    "buses_data = {\n",
    "    \"name\": [\"north\", \"east\", \"west\"],\n",
    "    \"v_nom\": [\"380\", \"380\", \"380\"],\n",
    "    \"x\": [\"9.9437675\", \"12.228830\", \"6.6495454\"],\n",
    "    \"y\": [\"53.5560129\", \"51.3418814\", \"51.238554\"],\n",
    "}\n",
    "buses = pd.DataFrame(buses_data)\n",
    "\n",
    "print(\"Buses dataframe\")\n",
    "display(buses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97b1e9",
   "metadata": {},
   "source": [
    "#### **Step 2: Define the lines(Transmission lines)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define meta-data for transmission lines\n",
    "lines_data = {\n",
    "    \"name\": [\"Line_N_W\", \"Line_N_E\", \"Line_W_E\"],\n",
    "    \"bus0\": [\"north\", \"north\", \"west\"],\n",
    "    \"bus1\": [\"west\", \"east\", \"east\"],\n",
    "    \"s_nom\": [\"380\", \"380\", \"380\"],\n",
    "    \"x\": [\"0.01\", \"0.01\", \"0.01\"],\n",
    "    \"r\": [\"0.00001\", \"0.00001\", \"0.00001\"],\n",
    "}\n",
    "lines = pd.DataFrame(lines_data)\n",
    "\n",
    "print(\"Lines dataframe\")\n",
    "display(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632172b",
   "metadata": {},
   "source": [
    "#### **Step 3a: Define the Demand Units/Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84056e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define meta-data for demand units\n",
    "demand_units_data = {\n",
    "    \"name\": [\"demand_north\", \"demand_east\", \"demand_west\"],\n",
    "    \"technology\": [\"inflex_demand\", \"inflex_demand\", \"inflex_demand\"],\n",
    "    \"bidding_EOM\": [\"naive_eom\", \"naive_eom\", \"naive_eom\"],\n",
    "    \"max_power\": [100000, 100000, 100000],  # Max capacity (could be MW)\n",
    "    \"min_power\": [0, 0, 0],  \n",
    "    \"node\": [\"north\", \"east\", \"west\"],\n",
    "    \"unit_operator\": [\"eom_de\", \"eom_de\", \"eom_de\"],\n",
    "}\n",
    "demand_units = pd.DataFrame(demand_units_data)\n",
    "\n",
    "print(\"Demand units/Agents:\")\n",
    "display(demand_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd4838",
   "metadata": {},
   "source": [
    "#### **Step 3b: Define the Demand Profile**\n",
    "\n",
    "Now, create the demand time series for each agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a32a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range(\"2023-01-01\", periods=48, freq=\"h\")\n",
    "demand_df = pd.DataFrame({\n",
    "    \"datetime\": index,\n",
    "    \"demand_north\": [10] * 48,\n",
    "    \"demand_east\": [10] * 48,\n",
    "    \"demand_west\": [40] * 48,\n",
    "}).set_index(\"datetime\")\n",
    "\n",
    "print(\"Inflexible Demand Profile (first 5 hours):\")\n",
    "display(demand_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87a5d3",
   "metadata": {},
   "source": [
    "#### **Step 4a: Define the Powerplant Units/Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define meta-data for demand units\n",
    "powerplant_units_data = {\n",
    "    \"name\": [\"Unit 1\", \"Unit 2\", \"Unit 3\"],\n",
    "    \"technology\": [\"steam turbine\", \"steam turbine\", \"steam turbine\"],\n",
    "    \"bidding_EOM\": [\"naive_eom\", \"naive_eom\", \"naive_eom\"],\n",
    "    \"bidding_redispatch\": [\"naive_redispatch\", \"naive_redispatch\", \"naive_redispatch\"],\n",
    "    \"max_power\": [31, 19, 30],  # Max capacity (could be MW)\n",
    "    \"min_power\": [0, 0, 0],\n",
    "    \"efficiency\": [1, 1, 1],\n",
    "    \"node\": [\"north\", \"east\", \"west\"],\n",
    "    \"unit_operator\": [\"Operator 1\", \"Operator 2\", \"Operator 3\"],\n",
    "    \"fuel_type\": [\"lignite\", \"hard coal\", \"natural gas\"],\n",
    "    \"additional_costs\": [0, 0, 0],\n",
    "}\n",
    "powerplant_units = pd.DataFrame(powerplant_units_data)\n",
    "\n",
    "print(\"Powerplant units/Agents:\")\n",
    "display(powerplant_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32412e0d",
   "metadata": {},
   "source": [
    "#### **Step 4b: Define the Powerplant Profile**\n",
    "Now, create the demand time series for each agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_df = pd.DataFrame({\n",
    "    \"datetime\": index,\n",
    "    \"demand_north\": [1] * 48,\n",
    "    \"demand_east\": [1] * 48,\n",
    "    \"demand_west\": [1] * 48,\n",
    "}).set_index(\"datetime\")\n",
    "\n",
    "print(\"Availability Profile (first 5 hours):\")\n",
    "display(availability_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb2384",
   "metadata": {},
   "source": [
    "#### **Step 5: Setting up Fuel prices**\n",
    "Here we define fuel prices for the power plant units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_prices_data = {\n",
    "    \"fuel\": [\"lignite\", \"hard coal\", \"natural gas\", \"CO2\"],\n",
    "    \"price\": [10,20,50,0],  # Example prices for uranium and CO2\n",
    "}\n",
    "fuel_prices = pd.DataFrame(fuel_prices_data)\n",
    "\n",
    "print(\"Fuel prices:\")\n",
    "display(fuel_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f22fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_prices_df = pd.DataFrame({\n",
    "    \"datetime\": index,\n",
    "    \"lignite\": [10] * 48,\n",
    "    \"hard coal\": [20] * 48,\n",
    "    \"natural gas\": [50] * 48,\n",
    "    \"CO2\": [0] * 48,\n",
    "}).set_index(\"datetime\")\n",
    "\n",
    "print(\"fuel prices profile(first 5 hours):\")\n",
    "display(fuel_prices_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56011739",
   "metadata": {},
   "source": [
    "#### **Step 6: Creating input Directory to save as CSV files**\n",
    "First, we need to create the directory for the input files if it does not already exist. Then, we will save the **DataFrames** as CSV files in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input directory\n",
    "input_dir = \"inputs\"\n",
    "scenario = \"tutorial_11\"\n",
    "scenario_path = os.path.join(input_dir, scenario)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(scenario_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "powerplant_units.to_csv(f\"{scenario_path}/powerplant_units.csv\", index=False)\n",
    "availability_df.to_csv(f\"{scenario_path}/availability_df.csv\", index=False)\n",
    "demand_units.to_csv(f\"{scenario_path}/demand_units.csv\", index=False)\n",
    "demand_df.to_csv(f\"{scenario_path}/demand_df.csv\")\n",
    "buses.to_csv(f\"{scenario_path}/buses.csv\", index=False)\n",
    "lines.to_csv(f\"{scenario_path}/lines.csv\", index=False)\n",
    "fuel_prices_df.to_csv(f\"{scenario_path}/fuel_prices_df.csv\", index=True, header=False)\n",
    "\n",
    "print(f\"Input CSV files have been saved to the directory: {scenario_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aff8e5",
   "metadata": {},
   "source": [
    "#### **Step 7 Creating the Configuration YAML File**\n",
    "\n",
    "For our simulation, we will define the configuration in a **YAML** format, which specifies the time range, market setup, and other parameters. This configuration will be saved as a **config.yaml** file.\n",
    "\n",
    "Below is the creation of the **configuration dictionary** and saving it to a **YAML** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fd0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base\": {\n",
    "        \"start_date\": \"2023-01-01 00:00\",\n",
    "        \"end_date\": \"2023-01-02 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 24,\n",
    "        \"markets_config\": {\n",
    "            \"EOM\": {\n",
    "                \"start_date\": \"2023-01-01 00:00\",\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [\n",
    "                    {\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"count\": 24,\n",
    "                        \"first_delivery\": \"24h\"\n",
    "                    }\n",
    "                ],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"20h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"pay_as_clear\"\n",
    "            },\n",
    "            \"redispatch\": {\n",
    "                \"start_date\": \"2023-01-01 21:00\",\n",
    "                \"operator\": \"network_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [\n",
    "                    {\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"count\": 24,\n",
    "                        \"first_delivery\": \"3h\"\n",
    "                    }\n",
    "                ],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"2h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"redispatch\",\n",
    "                \"additional_fields\": [\n",
    "                    \"node\",\n",
    "                    \"min_power\",\n",
    "                    \"max_power\"\n",
    "                ],\n",
    "                \"param_dict\": {\n",
    "                    \"network_path\": \".\",\n",
    "                    \"solver\": \"highs\",\n",
    "                    \"payment_mechanism\": \"pay_as_bid\",\n",
    "                    \"backup_marginal_cost\": 10000\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the config file\n",
    "config_path = os.path.join(scenario_path, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03609cb5",
   "metadata": {},
   "source": [
    "#### **Step 8 Running the Simulation**\n",
    "\n",
    "Now that we have prepared the input files and configuration, we can proceed to run the simulation using the **ASSUME** framework. In this step, we will load the scenario and execute the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef71017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for input and output data\n",
    "csv_path = \"outputs\"\n",
    "\n",
    "# Define the data format and database URI\n",
    "# Use \"local_db\" for SQLite database or \"timescale\" for TimescaleDB in Docker\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "# Choose the data format: either local SQLite database or TimescaleDB\n",
    "data_format = \"local_db\"  # Options: \"local_db\" or \"timescale\"\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario by providing the world instance\n",
    "# The path to the inputs folder and the scenario name (subfolder in inputs)\n",
    "# and the study case name (which config to use for the simulation)\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=input_dir,\n",
    "    scenario=scenario,  # Scenario folder for our case\n",
    "    study_case=\"base\",  # The config we defined earlier\n",
    ")\n",
    "\n",
    "# Run the simulation\n",
    "world.run()\n",
    "\n",
    "print(\"Simulation has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2720f",
   "metadata": {},
   "source": [
    "### Scenario 2: Redispatch with Industrial DSM unit (3-node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e563a9",
   "metadata": {},
   "source": [
    "#### **Step 1 Add DSM unit at node 'west'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inflexible Hydrogen Plant: Meta-Data\n",
    "hydrogen_plant_data = {\n",
    "    \"name\": [\"plant_H2_electrolyserA\"],\n",
    "    \"bidding_EOM\": [\"naive_eom\"],       # Example: simple market bidding strategy\n",
    "    \"max_power\": [5000],                # 5 MW typical electrolyser power\n",
    "    \"min_power\": [2000],                # 2 MW technical minimum\n",
    "    \"node\": [\"west\"],\n",
    "    \"bidding_redispatch\": [\"NaiveRedispatchSteelplantStrategy\"],\n",
    "    \"unit_operator\": [\"h2co_gmbh\"],\n",
    "}\n",
    "hydrogen_plant = pd.DataFrame(hydrogen_plant_data)\n",
    "\n",
    "print(\"Hydrogen Plant Meta-Data Table:\")\n",
    "display(hydrogen_plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43df1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume night operation at max, midday ramp-down, ramp-up again in evening\n",
    "demand_h2 = [5]*48\n",
    "demand_h2_df = pd.DataFrame({\n",
    "    \"datetime\": index,\n",
    "    \"plant_H2_electrolyserA\": demand_h2,\n",
    "}).set_index(\"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275c67b",
   "metadata": {},
   "source": [
    "#### Congestion Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "congestion_df = pd.DataFrame(\n",
    "    {\n",
    "        \"line_loading\": np.abs(network.lines_t.p0.values.flatten()),\n",
    "        \"line_name\": network.lines.index.repeat(len(network.snapshots)),\n",
    "        \"timestamp\": pd.Series(network.snapshots).repeat(len(network.lines)),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_nom_values = network.lines.s_nom\n",
    "congestion_df[\"s_nom\"] = congestion_df[\"line_name\"].map(s_nom_values)\n",
    "congestion_df[\"congestion_status\"] = (\n",
    "    congestion_df[\"line_loading\"] > congestion_df[\"s_nom\"]\n",
    ")\n",
    "congested_lines = congestion_df[congestion_df[\"congestion_status\"]]\n",
    "congested_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56832e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot to see the nodes and congestion in the lines\n",
    "now = network.snapshots[10]\n",
    "loading = network.lines_t.p0.loc[now] / network.lines.s_nom\n",
    "congestion_threshold = 1\n",
    "line_colors = np.where(abs(loading) > congestion_threshold, \"red\", \"blue\")\n",
    "\n",
    "# Create the figure and the axis using Cartopy's PlateCarree projection\n",
    "fig, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "# Plot the network using the built-in network plot function\n",
    "network.plot(\n",
    "    ax=ax,\n",
    "    line_colors=line_colors,\n",
    "    line_cmap=None,\n",
    "    title=\"Line Loading\",\n",
    "    bus_sizes=5e-2,  # Size of bus markers\n",
    "    bus_alpha=1,  # Transparency of bus markers\n",
    ")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317789a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db5568ca",
   "metadata": {},
   "source": [
    "### Scenario 3: Redispatch with Industrial DSM unit (3-node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63485136",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_05f\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616585a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65098144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88207a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
