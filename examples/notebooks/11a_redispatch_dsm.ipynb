{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344c88d7",
   "metadata": {
    "id": "344c88d7"
   },
   "source": [
    "# 11a. Redispatch modelling in the ASSUME Framework\n",
    "\n",
    "Welcome to the ASSUME DSM Workshop!\n",
    "\n",
    "This tutorial demonstrates modelling and simulation of redispatch mechanism using **PyPSA** as a plug and play module in **ASSUME-framework**. The model will be created mainly taking grid constraints into consideration to identify grid bottlenecks with dispatches from EOM and resolve them using the redispatch algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept of Redispatch\n",
    "\n",
    "The locational mismatch in demand and generation of electricity needs transmission of electricity from low demand regions to high demand regions. The transmission capacity limits the maximum amounts of electricity which can be transmitted at any point in time. If there is no enough capacity to transmit the required amount of electricity then there is a need of ramping down of generation at the locations of low demand and ramping up of generation at the locations of higher demand. This is typically called as Redispatch. Apart from spot markets there is redispatch mechanism to regulate this grid flows to avoid congestion issues. It is operated and controlled by the System operators (SO).\n",
    "\n",
    "## Objective\n",
    "The aim of redispatch is to reduce the overall cost of Redispatch(starting up, shutting down, ramping up, ramping down).\n",
    "\n",
    "## Structure in Redispatch model\n",
    "- The redispatch has following structure:\n",
    "    1. **Ramping up of powerplants**\n",
    "    2. **Ramping down of powerplants**:\n",
    "    3. **Ramping up/down of Demand Side flexibilites**:\n",
    "\n",
    "### Key Sections\n",
    "\n",
    "- **Section 1:** 3 node example for modelling Redispatch (Hands-on)\n",
    "- **Section 2:** 3 node example for modelling DSM Units ( Demonstration)\n",
    "- **Section 3:** Germany scale example for modelling Redispatch (Demonstration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f37e7",
   "metadata": {
    "id": "c81f37e7"
   },
   "source": [
    "## 0. Install Assume\n",
    "\n",
    "### 0.1 Repository Setup\n",
    "\n",
    "This tutorial provides insights into the newest developments of ASSUME. To access the we need to clone the specific branch on which they are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45435f4f",
   "metadata": {
    "id": "45435f4f"
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Check whether notebook is run in google colab\n",
    "IN_COLAB = importlib.util.find_spec(\"google.colab\") is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d99793",
   "metadata": {
    "id": "45d99793"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone -b Redispatch_Workshop_iP --depth=1 https://github.com/assume-framework/assume.git assume-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f0fc5",
   "metadata": {
    "id": "eb2f0fc5"
   },
   "source": [
    "### 0.2 Install Changes from pulled Branch\n",
    "\n",
    "Then we need to install Assume in this Colab. Here we just install the ASSUME core and network package via pip. In general the instructions for an installation can be found here: https://assume.readthedocs.io/en/latest/installation.html. All the required steps are executed here and since we are working in colab the generation of a venv is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w4RYG5WX02_G",
   "metadata": {
    "id": "w4RYG5WX02_G"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    ! pip install -e ./assume-repo[network]\n",
    "    # Colab currently has issues with pyomo version 6.8.2, causing the notebook to crash\n",
    "    # Installing an older version resolves this issue. This should only be considered a temporary fix.\n",
    "    !pip install pyomo==6.8.0\n",
    "\n",
    "# Install some additional packages for plotting\n",
    "!pip install plotly\n",
    "!pip install cartopy\n",
    "!pip install seaborn\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ebf71",
   "metadata": {
    "id": "027ebf71"
   },
   "source": [
    "> **Note**: After installation, **Colab may prompt you to restart the session** due to dependency changes.\n",
    "> To do so, click **\"Runtime\" â†’ \"Restart session...\"** in the menu bar, then re-run the cells above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4333989",
   "metadata": {
    "id": "a4333989"
   },
   "source": [
    "### 0.3 Input Path Configuration\n",
    "\n",
    "We define the path to input files depending on whether you're in Colab or working locally. This variable will be used to load configuration and scenario files throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e91aa",
   "metadata": {
    "id": "491e91aa"
   },
   "outputs": [],
   "source": [
    "colab_inputs_path = \"assume-repo/examples/inputs\"\n",
    "local_inputs_path = \"../inputs\"\n",
    "\n",
    "inputs_path = colab_inputs_path if IN_COLAB else local_inputs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d23293",
   "metadata": {
    "id": "a8d23293"
   },
   "source": [
    "### 0.4 Installation Check\n",
    "\n",
    "Use the following cell to ensure the installation was successful and that essential components are available. This test ensures that the simulation engine and RL strategy base class are accessible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a898d",
   "metadata": {
    "id": "057a898d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from assume import World\n",
    "\n",
    "    print(\"ASSUME framework is installed and functional.\")\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import essential components:\", e)\n",
    "    print(\n",
    "        \"Please review the installation instructions and ensure all dependencies are installed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac95da5",
   "metadata": {
    "id": "3ac95da5"
   },
   "source": [
    "Colab does not support Docker, so dashboard visualizations included in some ASSUME workflows will not be available. However, simulation runs and RL training can still be fully executed.\n",
    "\n",
    "* In **Colab**: Training and basic plotting are supported.\n",
    "* In **Local environments with Docker**: Full access, including dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b20ed",
   "metadata": {
    "id": "ae4b20ed"
   },
   "source": [
    "## Study Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647cf65",
   "metadata": {
    "id": "f647cf65"
   },
   "source": [
    "##### Let's also import some basic libraries that we will use throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1371c59",
   "metadata": {
    "id": "d1371c59"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Function to display DataFrame in Jupyter\n",
    "from IPython.display import display\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from assume import World\n",
    "from assume.scenario.loader_csv import load_scenario_folder\n",
    "# from assume.units.dsm_load_shift import DSMFlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabdcb8",
   "metadata": {
    "id": "7cabdcb8"
   },
   "source": [
    "### Scenario 1: Redispatch (3-node Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432260cb",
   "metadata": {
    "id": "432260cb"
   },
   "source": [
    "The grid infrastructure includes mainly three components:\n",
    "\n",
    "- **Generators**: Used to produce hydrogen for steel production.\n",
    "- **Loads**: Directly reduces iron ore using hydrogen.\n",
    "- **Transmission grid**: Converts the reduced iron into steel.\n",
    "\n",
    "\n",
    "Here the components are defined with their operational constraints (such as power, efficiency, ramp rates etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b293902",
   "metadata": {
    "id": "4b293902"
   },
   "source": [
    "#### **Step 1: Define the nodes/buses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3867d37",
   "metadata": {
    "id": "f3867d37"
   },
   "outputs": [],
   "source": [
    "timesteps = 100  # Number of timesteps in the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb22d6",
   "metadata": {
    "id": "86eb22d6"
   },
   "outputs": [],
   "source": [
    "# 1. Define meta-data for buses/nodes\n",
    "buses_data = {\n",
    "    \"name\": [\"north\", \"east\", \"west\"],\n",
    "    \"v_nom\": [\"380\", \"380\", \"380\"],\n",
    "    \"x\": [\"9.9437675\", \"12.228830\", \"6.6495454\"],\n",
    "    \"y\": [\"53.5560129\", \"51.3418814\", \"51.238554\"],\n",
    "}\n",
    "buses = pd.DataFrame(buses_data)\n",
    "\n",
    "print(\"Buses dataframe\")\n",
    "display(buses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97b1e9",
   "metadata": {
    "id": "4c97b1e9"
   },
   "source": [
    "#### **Step 2: Define the lines(Transmission lines)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3f1ae",
   "metadata": {
    "id": "74f3f1ae"
   },
   "outputs": [],
   "source": [
    "# 2. Define meta-data for transmission lines\n",
    "lines_data = {\n",
    "    \"name\": [\"Line_N_W\", \"Line_N_E\", \"Line_W_E\"],\n",
    "    \"bus0\": [\"north\", \"north\", \"west\"],\n",
    "    \"bus1\": [\"west\", \"east\", \"east\"],\n",
    "    \"v_nom\": [\"380\", \"380\", \"380\"],\n",
    "    \"s_nom\": [\"14\", \"14\", \"14\"],\n",
    "    \"x\": [\"0.01\", \"0.01\", \"0.01\"],\n",
    "    \"r\": [\"0.00001\", \"0.00001\", \"0.00001\"],\n",
    "}\n",
    "lines = pd.DataFrame(lines_data)\n",
    "\n",
    "print(\"Lines dataframe\")\n",
    "display(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632172b",
   "metadata": {
    "id": "1632172b"
   },
   "source": [
    "#### **Step 3a: Define the Demand Units/Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84056e1a",
   "metadata": {
    "id": "84056e1a"
   },
   "outputs": [],
   "source": [
    "# 1. Define meta-data for demand units\n",
    "demand_units_data = {\n",
    "    \"name\": [\"demand_north\", \"demand_east\", \"demand_west\"],\n",
    "    \"technology\": [\"inflex_demand\", \"inflex_demand\", \"inflex_demand\"],\n",
    "    \"bidding_EOM\": [\"naive_eom\", \"naive_eom\", \"naive_eom\"],\n",
    "    \"bidding_redispatch\": [\"naive_redispatch\", \"naive_redispatch\", \"naive_redispatch\"],\n",
    "    \"max_power\": [100000, 100000, 100000],  # Max capacity (could be MW)\n",
    "    \"min_power\": [0, 0, 0],\n",
    "    \"node\": [\"north\", \"east\", \"west\"],\n",
    "    \"unit_operator\": [\"eom_de\", \"eom_de\", \"eom_de\"],\n",
    "}\n",
    "demand_units = pd.DataFrame(demand_units_data)\n",
    "\n",
    "print(\"Demand units/Agents:\")\n",
    "display(demand_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd4838",
   "metadata": {
    "id": "adcd4838"
   },
   "source": [
    "#### **Step 3b: Define the Demand Profile**\n",
    "\n",
    "Now, create the demand time series for each agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a32a30",
   "metadata": {
    "id": "90a32a30"
   },
   "outputs": [],
   "source": [
    "index = pd.date_range(\"2023-01-01\", periods=timesteps, freq=\"h\")\n",
    "demand_df = pd.DataFrame(\n",
    "    {\n",
    "        \"datetime\": index,\n",
    "        \"demand_north\": [10] * timesteps,\n",
    "        \"demand_east\": [10] * timesteps,\n",
    "        \"demand_west\": [40] * timesteps,\n",
    "    }\n",
    ").set_index(\"datetime\")\n",
    "\n",
    "print(\"Inflexible Demand Profile (first 5 hours):\")\n",
    "display(demand_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87a5d3",
   "metadata": {
    "id": "7f87a5d3"
   },
   "source": [
    "#### **Step 4a: Define the Powerplant Units/Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d83f7",
   "metadata": {
    "id": "ad9d83f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Define meta-data for demand units\n",
    "powerplant_units_data = {\n",
    "    \"name\": [\"Unit 1\", \"Unit 2\", \"Unit 3\"],\n",
    "    \"technology\": [\"steam turbine\", \"steam turbine\", \"steam turbine\"],\n",
    "    \"bidding_EOM\": [\"naive_eom\", \"naive_eom\", \"naive_eom\"],\n",
    "    \"bidding_redispatch\": [\"naive_redispatch\", \"naive_redispatch\", \"naive_redispatch\"],\n",
    "    \"max_power\": [31, 19, 30],  # Max capacity (could be MW)\n",
    "    \"min_power\": [0, 0, 0],\n",
    "    \"efficiency\": [1, 1, 1],\n",
    "    \"node\": [\"north\", \"east\", \"west\"],\n",
    "    \"unit_operator\": [\"Operator 1\", \"Operator 2\", \"Operator 3\"],\n",
    "    \"fuel_type\": [\"lignite\", \"hard coal\", \"natural gas\"],\n",
    "    \"additional_cost\": [10, 20, 50],\n",
    "    \"emission_factor\": [0, 0, 0],  # Emission factor in kg CO2/kWh\n",
    "}\n",
    "powerplant_units = pd.DataFrame(powerplant_units_data)\n",
    "\n",
    "print(\"Powerplant units/Agents:\")\n",
    "display(powerplant_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32412e0d",
   "metadata": {
    "id": "32412e0d"
   },
   "source": [
    "#### **Step 4b: Define the Powerplant Profile**\n",
    "Now, create the demand time series for each agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4f935",
   "metadata": {
    "id": "01c4f935"
   },
   "outputs": [],
   "source": [
    "availability_df = pd.DataFrame(\n",
    "    {\n",
    "        \"datetime\": index,\n",
    "        \"Unit 1\": [1] * timesteps,\n",
    "        \"Unit 2\": [1] * timesteps,\n",
    "        \"Unit 3\": [1] * timesteps,\n",
    "    }\n",
    ").set_index(\"datetime\")\n",
    "\n",
    "print(\"Availability Profile (first 5 hours):\")\n",
    "display(availability_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb2384",
   "metadata": {
    "id": "b4cb2384"
   },
   "source": [
    "#### **Step 5: Setting up Fuel prices and forecasts of fuel prices**\n",
    "Here we define fuel prices for the power plant units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b7b0e",
   "metadata": {
    "id": "fd6b7b0e"
   },
   "outputs": [],
   "source": [
    "fuel_prices_data = {\n",
    "    \"fuel\": [\"lignite\", \"hard coal\", \"natural gas\", \"co2\"],\n",
    "    \"price\": [0, 0, 0, 0],\n",
    "}\n",
    "fuel_prices = pd.DataFrame(fuel_prices_data)\n",
    "\n",
    "print(\"Fuel prices:\")\n",
    "display(fuel_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f22fbd",
   "metadata": {
    "id": "e6f22fbd"
   },
   "outputs": [],
   "source": [
    "fuel_prices_df = pd.DataFrame(\n",
    "    {\n",
    "        \"datetime\": index,\n",
    "        \"lignite\": [0] * timesteps,\n",
    "        \"hard coal\": [0] * timesteps,\n",
    "        \"natural gas\": [0] * timesteps,\n",
    "        \"co2\": [0] * timesteps,\n",
    "    }\n",
    ").set_index(\"datetime\")\n",
    "\n",
    "print(\"fuel prices profile(first 5 hours):\")\n",
    "display(fuel_prices_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f4cc3",
   "metadata": {
    "id": "7c2f4cc3"
   },
   "outputs": [],
   "source": [
    "forecasts_df = pd.DataFrame(\n",
    "    {\n",
    "        \"datetime\": index,\n",
    "        \"co2_price\": [0] * timesteps,\n",
    "        \"electricity_price\": [0] * timesteps,\n",
    "        \"electricity_price_flex\": [0] * timesteps,\n",
    "        \"natural_gas_price\": [0] * timesteps,\n",
    "    }\n",
    ").set_index(\"datetime\")\n",
    "\n",
    "print(\"Price forecast profile(first 5 hours):\")\n",
    "display(forecasts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56011739",
   "metadata": {
    "id": "56011739"
   },
   "source": [
    "#### **Step 6: Creating input Directory to save as CSV files**\n",
    "First, we need to create the directory for the input files if it does not already exist. Then, we will save the **DataFrames** as CSV files in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99b4c9",
   "metadata": {
    "id": "6f99b4c9"
   },
   "outputs": [],
   "source": [
    "# Define the input directory\n",
    "input_dir = \"inputs\"\n",
    "scenario = \"scenario_1\"\n",
    "scenario_path = os.path.join(input_dir, scenario)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(scenario_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "powerplant_units.to_csv(f\"{scenario_path}/powerplant_units.csv\", index=False)\n",
    "availability_df.to_csv(f\"{scenario_path}/availability_df.csv\", index=True)\n",
    "demand_units.to_csv(f\"{scenario_path}/demand_units.csv\", index=False)\n",
    "demand_df.to_csv(f\"{scenario_path}/demand_df.csv\")\n",
    "buses.to_csv(f\"{scenario_path}/buses.csv\", index=False)\n",
    "lines.to_csv(f\"{scenario_path}/lines.csv\", index=False)\n",
    "fuel_prices_df.to_csv(f\"{scenario_path}/fuel_prices_df.csv\", index=True)\n",
    "forecasts_df.to_csv(f\"{scenario_path}/forecasts_df.csv\", index=True)\n",
    "\n",
    "print(f\"Input CSV files have been saved to the directory: {scenario_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aff8e5",
   "metadata": {
    "id": "b0aff8e5"
   },
   "source": [
    "#### **Step 7 Creating the Configuration YAML File**\n",
    "\n",
    "For our simulation, we will define the configuration in a **YAML** format, which specifies the time range, market setup, and other parameters. This configuration will be saved as a **config.yaml** file.\n",
    "\n",
    "Below is the creation of the **configuration dictionary** and saving it to a **YAML** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fd0d5",
   "metadata": {
    "id": "499fd0d5"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base\": {\n",
    "        \"start_date\": \"2023-01-01 00:00\",\n",
    "        \"end_date\": \"2023-01-02 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 24,\n",
    "        \"markets_config\": {\n",
    "            \"EOM\": {\n",
    "                \"start_date\": \"2023-01-01 00:00\",\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 24, \"first_delivery\": \"24h\"}],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"20h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"pay_as_clear\",\n",
    "            },\n",
    "            \"redispatch\": {\n",
    "                \"start_date\": \"2023-01-01 21:00\",\n",
    "                \"operator\": \"network_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 24, \"first_delivery\": \"3h\"}],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"2h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"redispatch\",\n",
    "                \"additional_fields\": [\"node\", \"min_power\", \"max_power\"],\n",
    "                \"param_dict\": {\n",
    "                    \"network_path\": \".\",\n",
    "                    \"solver\": \"highs\",\n",
    "                    \"payment_mechanism\": \"pay_as_bid\",\n",
    "                    \"backup_marginal_cost\": 10000,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the config file\n",
    "config_path = os.path.join(scenario_path, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03609cb5",
   "metadata": {
    "id": "03609cb5"
   },
   "source": [
    "#### **Step 8 Running the Simulation**\n",
    "\n",
    "Now that we have prepared the input files and configuration, we can proceed to run the simulation using the **ASSUME** framework. In this step, we will load the scenario and execute the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fb99e",
   "metadata": {
    "id": "e54fb99e"
   },
   "outputs": [],
   "source": [
    "# before you import assume\n",
    "import contextlib\n",
    "\n",
    "# override the suppress_output context manager\n",
    "import assume.common.utils as utils\n",
    "\n",
    "utils.suppress_output = contextlib.nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef71017",
   "metadata": {
    "id": "fef71017"
   },
   "outputs": [],
   "source": [
    "# Define paths for input and output data\n",
    "csv_path = \"outputs/scenario_1\"\n",
    "study_case = \"base\"  # The study case we defined earlier\n",
    "\n",
    "# Define the data format and database URI\n",
    "# Use \"local_db\" for SQLite database or \"timescale\" for TimescaleDB in Docker\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "# Choose the data format: either local SQLite database or TimescaleDB\n",
    "data_format = \"local_db\"  # Options: \"local_db\" or \"timescale\"\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario by providing the world instance\n",
    "# The path to the inputs folder and the scenario name (subfolder in inputs)\n",
    "# and the study case name (which config to use for the simulation)\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=input_dir,\n",
    "    scenario=scenario,  # Scenario folder for our case\n",
    "    study_case=study_case,  # The config we defined earlier\n",
    ")\n",
    "\n",
    "# Run the simulation\n",
    "world.run()\n",
    "\n",
    "print(\"Simulation has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8a836",
   "metadata": {
    "id": "3ab8a836"
   },
   "source": [
    "#### **Step 9 Visualization: Congestion in the network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8cbe1",
   "metadata": {
    "id": "30b8cbe1"
   },
   "source": [
    "##### A) Download a map of Germany to visualize congested lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e33541",
   "metadata": {
    "id": "a4e33541"
   },
   "outputs": [],
   "source": [
    "# 1. The exact ASCII URL (no ellipsis!)\n",
    "url = \"https://gisco-services.ec.europa.eu/distribution/v2/nuts/download/ref-nuts-2013-01m.shp.zip\"\n",
    "\n",
    "# 2. Define the path where the zip file will be saved\n",
    "cwd = Path.cwd()\n",
    "scenario_rel = Path(\"inputs\") / \"scenario_1\"\n",
    "zip_name = \"EU\"\n",
    "zip_path = scenario_rel / zip_name\n",
    "\n",
    "# 3. Download into that file\n",
    "urllib.request.urlretrieve(url, zip_path)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# 4) Unzip everything into that same folder\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    z.extractall(scenario_path)\n",
    "\n",
    "# 5) Read the shapefile from the zip file\n",
    "file_path = os.path.join(scenario_path, \"NUTS_RG_01M_2013_4326_LEVL_0.shp.zip\")\n",
    "eu = gpd.read_file(file_path)\n",
    "\n",
    "# 6) Filter the GeoDataFrame for Germany (country code 'DE')\n",
    "germany_nuts3 = eu[eu[\"CNTR_CODE\"] == \"DE\"]\n",
    "\n",
    "# 7). Plot the map for Germany\n",
    "germany_nuts3.plot(figsize=(5, 5), edgecolor=\"black\")\n",
    "plt.title(\"Germany\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7b6f4",
   "metadata": {
    "id": "d2b7b6f4"
   },
   "outputs": [],
   "source": [
    "# Convert buses DataFrame to a GeoDataFrame using longitude (x) and latitude (y)\n",
    "geometry = [Point(xy) for xy in zip(buses[\"x\"], buses[\"y\"])]\n",
    "buses_gdf = gpd.GeoDataFrame(buses, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Ensure the CRS matches between the NUTS3 shapefile and buses GeoDataFrame\n",
    "buses_gdf = buses_gdf.to_crs(germany_nuts3.crs)\n",
    "buses_gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044df49",
   "metadata": {
    "id": "b044df49"
   },
   "outputs": [],
   "source": [
    "# Get the bounding box of the classified buses\n",
    "bbox = buses_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Calculate the center of the bounding box\n",
    "center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "\n",
    "# Calculate the width and height of the bounding box with a margin\n",
    "width = bbox[2] - bbox[0]\n",
    "height = bbox[3] - bbox[1]\n",
    "new_bbox = box(\n",
    "    center[0] - width * 0.55,\n",
    "    center[1] - height * 0.55,\n",
    "    center[0] + width * 0.55,\n",
    "    center[1] + height * 0.55,\n",
    ")\n",
    "\n",
    "# Convert the bounding box to a GeoSeries\n",
    "new_bbox = gpd.GeoSeries([new_bbox], crs=buses_gdf.crs)\n",
    "\n",
    "# Plot the map for Germany with buses classified by NUTS3 regions\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "germany_nuts3.plot(ax=ax, edgecolor=\"black\")\n",
    "buses_gdf.plot(ax=ax, color=\"black\", label=\"Buses\", markersize=200)\n",
    "\n",
    "plt.title(\"Buses in Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3a759",
   "metadata": {
    "id": "a9a3a759"
   },
   "source": [
    "##### B) Read & preprocess the line_loading data to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf1cfc",
   "metadata": {
    "id": "dbdf1cfc"
   },
   "outputs": [],
   "source": [
    "# Read the line_loading CSV file\n",
    "line_loading = pd.read_csv(\"outputs/line_loading.csv\")\n",
    "line_loading.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1110984",
   "metadata": {
    "id": "a1110984"
   },
   "outputs": [],
   "source": [
    "# Preprocessing of buses and lines dataframe for visualization\n",
    "\n",
    "# 1. Transpose the line_loading dataframe in order to visualize it well\n",
    "ll = line_loading.melt(id_vars=\"snapshot\", var_name=\"name\", value_name=\"loading\")\n",
    "ll.head(5)\n",
    "\n",
    "# 2. Merge the line loading data with the lines DataFrame to get bus coordinates\n",
    "bus_coords = (\n",
    "    buses_gdf[[\"name\", \"geometry\"]]\n",
    "    .rename(columns={\"name\": \"bus_name\"})\n",
    "    .assign(long=lambda df: df.geometry.x, lat=lambda df: df.geometry.y)\n",
    ")\n",
    "\n",
    "# 3. Merge geocordinates from buses data in lines DataFrame\n",
    "lines_visual = (\n",
    "    lines\n",
    "    # from end\n",
    "    .merge(bus_coords[[\"bus_name\", \"long\", \"lat\"]], left_on=\"bus0\", right_on=\"bus_name\")\n",
    "    .rename(columns={\"long\": \"x0\", \"lat\": \"y0\"})\n",
    "    .drop(columns=\"bus_name\")\n",
    "    # to end\n",
    "    .merge(bus_coords[[\"bus_name\", \"long\", \"lat\"]], left_on=\"bus1\", right_on=\"bus_name\")\n",
    "    .rename(columns={\"long\": \"x1\", \"lat\": \"y1\"})\n",
    "    .drop(columns=\"bus_name\")\n",
    ")\n",
    "lines_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c87fb0",
   "metadata": {
    "id": "49c87fb0"
   },
   "outputs": [],
   "source": [
    "# Select your snapshot to visualize\n",
    "snapshot = 0\n",
    "ll_snap = ll[ll[\"snapshot\"] == snapshot].set_index(\"name\")[\"loading\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cdfa1",
   "metadata": {
    "id": "c43cdfa1"
   },
   "outputs": [],
   "source": [
    "ll_snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46cc13",
   "metadata": {
    "id": "2f46cc13"
   },
   "outputs": [],
   "source": [
    "# Plot the line_loading on Germany map\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "germany_nuts3.plot(ax=ax, edgecolor=\"#ADD8E6\", color=\"#ADD8E6\")\n",
    "buses_gdf.plot(ax=ax, color=\"black\", label=\"Buses\", markersize=200)\n",
    "\n",
    "for _, row in lines_visual.iterrows():\n",
    "    # find the one row in ll matching this snapshot & line name\n",
    "    mask = (ll[\"snapshot\"] == snapshot) & (ll[\"name\"] == row[\"name\"])\n",
    "    if mask.any():\n",
    "        load = ll.loc[mask, \"loading\"].iloc[0]\n",
    "    else:\n",
    "        load = 0.0\n",
    "\n",
    "    color = \"red\" if load > 1 else \"blue\"\n",
    "    ax.plot(\n",
    "        [row[\"x0\"], row[\"x1\"]],\n",
    "        [row[\"y0\"], row[\"y1\"]],\n",
    "        color=color,\n",
    "        linewidth=2,\n",
    "        alpha=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2366b47",
   "metadata": {
    "id": "a2366b47"
   },
   "source": [
    "#### **Step 10  Visualization: Redispatch amounts & Estimation of Redispatch cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c074db",
   "metadata": {
    "id": "33c074db"
   },
   "source": [
    "##### A) Estimation of Redispatch cost from the market_orders output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fbaee",
   "metadata": {
    "id": "3d9fbaee"
   },
   "outputs": [],
   "source": [
    "scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fd217",
   "metadata": {
    "id": "763fd217"
   },
   "outputs": [],
   "source": [
    "# Read the market order csv file\n",
    "market_orders = pd.read_csv(f\"{csv_path}/{scenario}_{study_case}/market_orders.csv\")\n",
    "market_orders.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487052d1",
   "metadata": {
    "id": "487052d1"
   },
   "outputs": [],
   "source": [
    "# fetch the market orders for the redispatch market_id only\n",
    "redispatch_orders = market_orders[market_orders[\"market_id\"] == \"redispatch\"]\n",
    "redispatch_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f8da0",
   "metadata": {
    "id": "c86f8da0"
   },
   "outputs": [],
   "source": [
    "# Redispatch cost is equal to the accepted_volume * accepted_price for a particular snapshot\n",
    "redispatch_orders.loc[:, \"redispatch_cost\"] = (\n",
    "    redispatch_orders[\"accepted_volume\"].abs() * redispatch_orders[\"accepted_price\"]\n",
    ")\n",
    "\n",
    "# Then group & sum as before\n",
    "redispatch_costs = (\n",
    "    redispatch_orders.groupby(\"start_time\")[\"redispatch_cost\"].sum().reset_index()\n",
    ")\n",
    "redispatch_costs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7091527",
   "metadata": {
    "id": "e7091527"
   },
   "source": [
    "##### B) Plot a bar graph for redispatch by powerplants for one snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8fc10",
   "metadata": {
    "id": "96b8fc10"
   },
   "outputs": [],
   "source": [
    "# Select the first available snapshot\n",
    "snapshot = market_orders[\"start_time\"].unique()[0]\n",
    "filtered = market_orders[market_orders[\"start_time\"] == snapshot]\n",
    "\n",
    "# Pivot accepted_volume by unit_id and market_id\n",
    "pivot = filtered.pivot_table(\n",
    "    index=\"unit_id\", columns=\"market_id\", values=\"accepted_volume\", aggfunc=\"sum\"\n",
    ").fillna(0)\n",
    "\n",
    "# Plot stacked bar chart with annotations\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = pivot.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "plt.axhline(0, color=\"black\")\n",
    "plt.xlabel(\"Unit ID\")\n",
    "plt.ylabel(\"Accepted Volume\")\n",
    "plt.title(f\"Final dispatches for {snapshot}\")\n",
    "plt.legend(title=\"Market ID\")\n",
    "\n",
    "# Annotate each segment with its value\n",
    "for container in bars.containers:\n",
    "    for bar in container:\n",
    "        height = bar.get_height()\n",
    "        if height != 0:\n",
    "            ax.annotate(\n",
    "                f\"{height:.1f}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12b680",
   "metadata": {
    "id": "ce12b680"
   },
   "source": [
    "##### C) Plot on a map for redispatch by locations for one snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3cfb4",
   "metadata": {
    "id": "c2c3cfb4"
   },
   "outputs": [],
   "source": [
    "# fetch the market orders for the redispatch market_id only\n",
    "redispatch_orders = market_orders[market_orders[\"market_id\"] == \"redispatch\"]\n",
    "# fetch latitudes and longitudes of the node in positive and negative redispatch orders\n",
    "redispatch_orders = redispatch_orders.merge(\n",
    "    buses_gdf[[\"name\", \"x\", \"y\"]], left_on=\"node\", right_on=\"name\", how=\"left\"\n",
    ").drop(columns=\"name\")\n",
    "redispatch_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75295b32",
   "metadata": {
    "id": "75295b32"
   },
   "outputs": [],
   "source": [
    "snapshot = redispatch_orders[\"start_time\"].unique()[0]\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c29c0e8",
   "metadata": {
    "id": "8c29c0e8"
   },
   "outputs": [],
   "source": [
    "# Separate redispatch orders by accepted_volume , positive values as positive redispatch and negative values as negative redispatch\n",
    "positive_redispatch = redispatch_orders[redispatch_orders[\"accepted_volume\"] > 0]\n",
    "# Create a GeoDataFrame for positive redispatches\n",
    "positive_gdf = gpd.GeoDataFrame(\n",
    "    positive_redispatch,\n",
    "    geometry=gpd.points_from_xy(positive_redispatch[\"x\"], positive_redispatch[\"y\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "positive_gdf_snapshot = positive_gdf[positive_gdf[\"start_time\"] == snapshot]\n",
    "positive_gdf_snapshot = positive_gdf_snapshot[\n",
    "    [\"start_time\", \"accepted_volume\", \"geometry\"]\n",
    "]\n",
    "positive_gdf_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844e9f2",
   "metadata": {
    "id": "4844e9f2"
   },
   "outputs": [],
   "source": [
    "negative_redispatch = redispatch_orders[redispatch_orders[\"accepted_volume\"] < 0]\n",
    "# Create a GeoDataFrame for positive redispatches\n",
    "negative_gdf = gpd.GeoDataFrame(\n",
    "    negative_redispatch,\n",
    "    geometry=gpd.points_from_xy(negative_redispatch[\"x\"], negative_redispatch[\"y\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "negative_gdf_snapshot = negative_gdf[negative_gdf[\"start_time\"] == snapshot]\n",
    "negative_gdf_snapshot = negative_gdf_snapshot[\n",
    "    [\"start_time\", \"accepted_volume\", \"geometry\"]\n",
    "]\n",
    "negative_gdf_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb36783",
   "metadata": {
    "id": "ceb36783"
   },
   "outputs": [],
   "source": [
    "# Get the bounding box of the classified buses\n",
    "bbox = buses_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Calculate the center of the bounding box\n",
    "center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "\n",
    "# Calculate the width and height of the bounding box with a margin\n",
    "width = bbox[2] - bbox[0]\n",
    "height = bbox[3] - bbox[1]\n",
    "new_bbox = box(\n",
    "    center[0] - width * 0.55,\n",
    "    center[1] - height * 0.55,\n",
    "    center[0] + width * 0.55,\n",
    "    center[1] + height * 0.55,\n",
    ")\n",
    "\n",
    "# Convert the bounding box to a GeoSeries\n",
    "new_bbox = gpd.GeoSeries([new_bbox], crs=buses_gdf.crs)\n",
    "\n",
    "# Plot the map for Germany with buses classified by NUTS3 regions\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "germany_nuts3.plot(ax=ax, color=\"#ADD8E6\", edgecolor=\"black\")\n",
    "\n",
    "# Plot positive redispatch with size based on accepted volume\n",
    "positive_gdf_snapshot.plot(\n",
    "    ax=ax,\n",
    "    color=\"blue\",\n",
    "    markersize=positive_gdf_snapshot[\"accepted_volume\"] * 50,\n",
    "    label=\"Positive Redispatch\",\n",
    ")\n",
    "# Plot negative redispatch with size based on accepted volume\n",
    "negative_gdf_snapshot.plot(\n",
    "    ax=ax,\n",
    "    color=\"red\",\n",
    "    markersize=-negative_gdf_snapshot[\"accepted_volume\"] * 50,\n",
    "    label=\"Negative Redispatch\",\n",
    ")\n",
    "\n",
    "# plot legend\n",
    "plt.legend(loc=\"lower right\", fontsize=\"small\", markerscale=0.5, frameon=True)\n",
    "plt.title(\"Buses in Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec17b1",
   "metadata": {
    "id": "14ec17b1"
   },
   "source": [
    "#### **Step 11: Tasks**\n",
    "\n",
    "Go to **Step 4a** and try setting up,\n",
    "\n",
    "1. Change the marginal cost of the powerplant in node **east** to **40â‚¬/MWh**\n",
    "\n",
    "2.  **wind powerplant** at **north** node with following parameters:\n",
    " -  marginal cost=**0**,\n",
    " -  fuel_type=**renewable**,\n",
    " -  technology = wind offshore,\n",
    " -  availability==1\n",
    "\n",
    "3. **wind powerplant** at **north** node with following parameters:\n",
    " - marginal cost=**-50**,\n",
    " - fuel_type=renewable,\n",
    " - technology = wind offshore,\n",
    " - availability==1\n",
    "\n",
    "**Estimate the redispatch cost and see the difference for above scenarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2720f",
   "metadata": {
    "id": "92c2720f"
   },
   "source": [
    "### Scenario 2: Redispatch with Industrial DSM unit (3-node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e563a9",
   "metadata": {
    "id": "e3e563a9"
   },
   "source": [
    "#### **Step 1 Add DSM unit at node 'west'**\n",
    "\n",
    "One can any DSM unit from the ones modelled in ASSUME already or create a customized DSM unit to add it to the the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f09d0",
   "metadata": {
    "id": "806f09d0"
   },
   "source": [
    "Here is just a demonstration of how a unit can be added at any node in the EOM and can be considered using **bidding_strategy==steelplant_strategy** in the redispatch mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9756b",
   "metadata": {
    "id": "e9a9756b"
   },
   "outputs": [],
   "source": [
    "# Industrial Heat Pump unit: Meta-Data\n",
    "\n",
    "heat_pump_data = {\n",
    "    \"name\": [\"Heat_pump_unit_1\"],\n",
    "    \"technology\": [\"Heat pump\"],\n",
    "    \"bidding_EOM\": [\"naive_da_dsm\"],  # Example: simple market bidding strategy\n",
    "    \"node\": [\"west\"],\n",
    "    \"bidding_redispatch\": [\"steelplant_strategy\"],\n",
    "    \"unit_operator\": [\"dsm_operator_1\"],\n",
    "    \"cop\": [1],  # Coefficient of performance of the heat pump\n",
    "    \"max_power\": 5,  # Fuel type for the heat pump\n",
    "    \"min_power\": 0,  # Minimum power output\n",
    "    \"ramp_up\": 5,  # Ramp up rate in MW/min\n",
    "    \"ramp_down\": 5,  # Ramp down rate in MW/min\n",
    "    \"demand\": 240,  # Fuel type for the heat pump\n",
    "    \"cost_tolerance\": 100,  # Cost tolerance for the heat pump\n",
    "}\n",
    "heat_pump = pd.DataFrame(heat_pump_data)\n",
    "\n",
    "print(\"Heat pump Meta-Data Table:\")\n",
    "display(heat_pump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d262a",
   "metadata": {
    "id": "fb2d262a"
   },
   "outputs": [],
   "source": [
    "# Define the input directory for scenario_2\n",
    "input_dir = \"inputs\"\n",
    "scenario = \"scenario_2\"\n",
    "scenario_path = os.path.join(input_dir, scenario)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(scenario_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrames to CSV files\n",
    "powerplant_units.to_csv(f\"{scenario_path}/powerplant_units.csv\", index=False)\n",
    "availability_df.to_csv(f\"{scenario_path}/availability_df.csv\", index=True)\n",
    "demand_units.to_csv(f\"{scenario_path}/demand_units.csv\", index=False)\n",
    "demand_df.to_csv(f\"{scenario_path}/demand_df.csv\")\n",
    "buses.to_csv(f\"{scenario_path}/buses.csv\", index=False)\n",
    "lines.to_csv(f\"{scenario_path}/lines.csv\", index=False)\n",
    "fuel_prices_df.to_csv(f\"{scenario_path}/fuel_prices_df.csv\", index=True)\n",
    "forecasts_df.to_csv(f\"{scenario_path}/forecasts_df.csv\", index=True)\n",
    "heat_pump.to_csv(f\"{scenario_path}/industrial_dsm_units.csv\", index=False)\n",
    "\n",
    "print(f\"Input CSV files have been saved to the directory: {scenario_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659173ff",
   "metadata": {
    "id": "659173ff"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"base\": {\n",
    "        \"start_date\": \"2023-01-01 00:00\",\n",
    "        \"end_date\": \"2023-01-02 23:00\",\n",
    "        \"time_step\": \"1h\",\n",
    "        \"save_frequency_hours\": 24,\n",
    "        \"markets_config\": {\n",
    "            \"EOM\": {\n",
    "                \"start_date\": \"2023-01-01 00:00\",\n",
    "                \"operator\": \"EOM_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 24, \"first_delivery\": \"24h\"}],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"20h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"pay_as_clear\",\n",
    "            },\n",
    "            \"redispatch\": {\n",
    "                \"start_date\": \"2023-01-01 21:00\",\n",
    "                \"operator\": \"network_operator\",\n",
    "                \"product_type\": \"energy\",\n",
    "                \"products\": [{\"duration\": \"1h\", \"count\": 24, \"first_delivery\": \"3h\"}],\n",
    "                \"opening_frequency\": \"24h\",\n",
    "                \"opening_duration\": \"2h\",\n",
    "                \"volume_unit\": \"MWh\",\n",
    "                \"maximum_bid_volume\": 100000,\n",
    "                \"maximum_bid_price\": 3000,\n",
    "                \"minimum_bid_price\": -500,\n",
    "                \"price_unit\": \"EUR/MWh\",\n",
    "                \"market_mechanism\": \"redispatch\",\n",
    "                \"additional_fields\": [\"node\", \"min_power\", \"max_power\"],\n",
    "                \"param_dict\": {\n",
    "                    \"network_path\": \".\",\n",
    "                    \"solver\": \"highs\",\n",
    "                    \"payment_mechanism\": \"pay_as_bid\",\n",
    "                    \"backup_marginal_cost\": 10000,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the path for the config file\n",
    "config_path = os.path.join(scenario_path, \"config.yaml\")\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration YAML file has been saved to '{config_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6feb392",
   "metadata": {
    "id": "e6feb392"
   },
   "outputs": [],
   "source": [
    "# Define paths for input and output data\n",
    "csv_path = \"outputs/scenario_2\"\n",
    "study_case = \"base\"  # The study case we defined earlier\n",
    "# Define the data format and database URI\n",
    "# Use \"local_db\" for SQLite database or \"timescale\" for TimescaleDB in Docker\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "# Choose the data format: either local SQLite database or TimescaleDB\n",
    "data_format = \"local_db\"  # Options: \"local_db\" or \"timescale\"\n",
    "\n",
    "# Set the database URI based on the selected data format\n",
    "if data_format == \"local_db\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"  # SQLite database\n",
    "elif data_format == \"timescale\":\n",
    "    db_uri = \"postgresql://assume:assume@localhost:5432/assume\"  # TimescaleDB\n",
    "\n",
    "# Create the World instance\n",
    "world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "# Load the scenario by providing the world instance\n",
    "# The path to the inputs folder and the scenario name (subfolder in inputs)\n",
    "# and the study case name (which config to use for the simulation)\n",
    "load_scenario_folder(\n",
    "    world,\n",
    "    inputs_path=input_dir,\n",
    "    scenario=scenario,  # Scenario folder for our case\n",
    "    study_case=study_case,  # The config we defined earlier\n",
    ")\n",
    "\n",
    "# Run the simulation\n",
    "world.run()\n",
    "\n",
    "print(\"Simulation has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0b4c7",
   "metadata": {
    "id": "4df0b4c7"
   },
   "source": [
    "#### **Step 2 Visualization: Redispatch amounts & Estimation of Redispatch cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52eb0c4",
   "metadata": {
    "id": "e52eb0c4"
   },
   "source": [
    "##### A) Estimation of Redispatch amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1add9de7",
   "metadata": {
    "id": "1add9de7"
   },
   "outputs": [],
   "source": [
    "# Read the market order csv file\n",
    "market_orders = pd.read_csv(f\"{csv_path}/{scenario}_{study_case}/market_orders.csv\")\n",
    "market_orders.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6dfbad",
   "metadata": {
    "id": "9f6dfbad"
   },
   "outputs": [],
   "source": [
    "# fetch the market orders for the redispatch market_id only\n",
    "redispatch_orders = market_orders[market_orders[\"market_id\"] == \"redispatch\"]\n",
    "redispatch_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587f446",
   "metadata": {
    "id": "7587f446"
   },
   "outputs": [],
   "source": [
    "# Redispatch cost is equal to the accepted_volume * accepted_price for a particular snapshot\n",
    "redispatch_orders.loc[:, \"redispatch_cost\"] = (\n",
    "    redispatch_orders[\"accepted_volume\"].abs() * redispatch_orders[\"accepted_price\"]\n",
    ")\n",
    "\n",
    "# Then group & sum as before\n",
    "redispatch_costs = (\n",
    "    redispatch_orders.groupby(\"start_time\")[\"redispatch_cost\"].sum().reset_index()\n",
    ")\n",
    "redispatch_costs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342f552",
   "metadata": {
    "id": "9342f552"
   },
   "source": [
    "##### B) Plot a bar graph for redispatch by powerplants for one snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfae8f4",
   "metadata": {
    "id": "1dfae8f4"
   },
   "outputs": [],
   "source": [
    "# Select the first available snapshot\n",
    "snapshot = market_orders[\"start_time\"].unique()[0]\n",
    "filtered = market_orders[market_orders[\"start_time\"] == snapshot]\n",
    "\n",
    "# Pivot accepted_volume by unit_id and market_id\n",
    "pivot = filtered.pivot_table(\n",
    "    index=\"unit_id\", columns=\"market_id\", values=\"accepted_volume\", aggfunc=\"sum\"\n",
    ").fillna(0)\n",
    "\n",
    "# Plot stacked bar chart with annotations\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = pivot.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "plt.axhline(0, color=\"black\")\n",
    "plt.xlabel(\"Unit ID\")\n",
    "plt.ylabel(\"Accepted Volume\")\n",
    "plt.title(f\"Final dispatches for {snapshot}\")\n",
    "plt.legend(title=\"Market ID\")\n",
    "\n",
    "# Annotate each segment with its value\n",
    "for container in bars.containers:\n",
    "    for bar in container:\n",
    "        height = bar.get_height()\n",
    "        if height != 0:\n",
    "            ax.annotate(\n",
    "                f\"{height:.1f}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae86b9",
   "metadata": {
    "id": "8dae86b9"
   },
   "source": [
    "##### C) Plot on a map for redispatch by locations for one snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac3117",
   "metadata": {
    "id": "dbac3117"
   },
   "outputs": [],
   "source": [
    "# fetch the market orders for the redispatch market_id only\n",
    "redispatch_orders = market_orders[market_orders[\"market_id\"] == \"redispatch\"]\n",
    "# fetch latitudes and longitudes of the node in positive and negative redispatch orders\n",
    "redispatch_orders = redispatch_orders.merge(\n",
    "    buses_gdf[[\"name\", \"x\", \"y\"]], left_on=\"node\", right_on=\"name\", how=\"left\"\n",
    ").drop(columns=\"name\")\n",
    "redispatch_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abd7b7",
   "metadata": {
    "id": "c9abd7b7"
   },
   "outputs": [],
   "source": [
    "snapshot = redispatch_orders[\"start_time\"].unique()[0]\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1fb53",
   "metadata": {
    "id": "0cf1fb53"
   },
   "outputs": [],
   "source": [
    "# Separate redispatch orders by accepted_volume , positive values as positive redispatch and negative values as negative redispatch\n",
    "positive_redispatch = redispatch_orders[redispatch_orders[\"accepted_volume\"] > 0]\n",
    "# Create a GeoDataFrame for positive redispatches\n",
    "positive_gdf = gpd.GeoDataFrame(\n",
    "    positive_redispatch,\n",
    "    geometry=gpd.points_from_xy(positive_redispatch[\"x\"], positive_redispatch[\"y\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "positive_gdf_snapshot = positive_gdf[positive_gdf[\"start_time\"] == snapshot]\n",
    "positive_gdf_snapshot = positive_gdf_snapshot[\n",
    "    [\"start_time\", \"accepted_volume\", \"geometry\"]\n",
    "]\n",
    "positive_gdf_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fc1af",
   "metadata": {
    "id": "fd3fc1af"
   },
   "outputs": [],
   "source": [
    "negative_redispatch = redispatch_orders[redispatch_orders[\"accepted_volume\"] < 0]\n",
    "# Create a GeoDataFrame for positive redispatches\n",
    "negative_gdf = gpd.GeoDataFrame(\n",
    "    negative_redispatch,\n",
    "    geometry=gpd.points_from_xy(negative_redispatch[\"x\"], negative_redispatch[\"y\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "negative_gdf_snapshot = negative_gdf[negative_gdf[\"start_time\"] == snapshot]\n",
    "negative_gdf_snapshot = negative_gdf_snapshot[\n",
    "    [\"start_time\", \"accepted_volume\", \"geometry\"]\n",
    "]\n",
    "negative_gdf_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997f031",
   "metadata": {
    "id": "d997f031"
   },
   "outputs": [],
   "source": [
    "# Get the bounding box of the classified buses\n",
    "bbox = buses_gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# Calculate the center of the bounding box\n",
    "center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "\n",
    "# Calculate the width and height of the bounding box with a margin\n",
    "width = bbox[2] - bbox[0]\n",
    "height = bbox[3] - bbox[1]\n",
    "new_bbox = box(\n",
    "    center[0] - width * 0.55,\n",
    "    center[1] - height * 0.55,\n",
    "    center[0] + width * 0.55,\n",
    "    center[1] + height * 0.55,\n",
    ")\n",
    "\n",
    "# Convert the bounding box to a GeoSeries\n",
    "new_bbox = gpd.GeoSeries([new_bbox], crs=buses_gdf.crs)\n",
    "\n",
    "# Plot the map for Germany with buses classified by NUTS3 regions\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "germany_nuts3.plot(ax=ax, color=\"#ADD8E6\", edgecolor=\"black\")\n",
    "\n",
    "# Plot positive redispatch with size based on accepted volume\n",
    "positive_gdf_snapshot.plot(\n",
    "    ax=ax,\n",
    "    color=\"blue\",\n",
    "    markersize=positive_gdf_snapshot[\"accepted_volume\"] * 50,\n",
    "    label=\"Positive Redispatch\",\n",
    ")\n",
    "# Plot negative redispatch with size based on accepted volume\n",
    "negative_gdf_snapshot.plot(\n",
    "    ax=ax,\n",
    "    color=\"red\",\n",
    "    markersize=-negative_gdf_snapshot[\"accepted_volume\"] * 50,\n",
    "    label=\"Negative Redispatch\",\n",
    ")\n",
    "\n",
    "# plot legend\n",
    "plt.legend(loc=\"lower right\", fontsize=\"small\", markerscale=0.5, frameon=True)\n",
    "plt.title(\"Buses in Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5568ca",
   "metadata": {
    "id": "db5568ca"
   },
   "source": [
    "### Scenario 3: Redispatch for whole Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041f4c3",
   "metadata": {
    "id": "d041f4c3"
   },
   "source": [
    "A) Run simulation for Germany:\n",
    "- Nodes: 835\n",
    "- Lines: 1088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63485136",
   "metadata": {
    "id": "63485136"
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "\n",
    "csv_path = \"scenario3/outputs\"\n",
    "os.makedirs(\"local_db\", exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_uri = \"sqlite:///local_db/assume_db.db\"\n",
    "\n",
    "    scenario = \"example_05f\"\n",
    "    study_case = \"base\"\n",
    "\n",
    "    # create world\n",
    "    world = World(database_uri=db_uri, export_csv_path=csv_path)\n",
    "\n",
    "    # then we load the scenario specified above from the respective input files\n",
    "    load_scenario_folder(\n",
    "        world,\n",
    "        inputs_path=inputs_path,\n",
    "        scenario=scenario,\n",
    "        study_case=study_case,\n",
    "    )\n",
    "\n",
    "    # after the learning is done we make a normal run of the simulation, which equals a test run\n",
    "    world.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c600b08",
   "metadata": {
    "id": "2c600b08"
   },
   "source": [
    "##### A) Read the line loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871025e3",
   "metadata": {
    "id": "871025e3"
   },
   "outputs": [],
   "source": [
    "# Read the line_loading CSV file\n",
    "line_loading = pd.read_csv(\"outputs/line_loading.csv\")\n",
    "line_loading.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1aa3d",
   "metadata": {
    "id": "24b1aa3d"
   },
   "outputs": [],
   "source": [
    "# Read the market order csv file\n",
    "market_orders = pd.read_csv(f\"{csv_path}/{scenario}_{study_case}/market_orders.csv\")\n",
    "market_orders.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab05d74",
   "metadata": {
    "id": "1ab05d74"
   },
   "outputs": [],
   "source": [
    "# fetch the market orders for the redispatch market_id only\n",
    "redispatch_orders = market_orders[market_orders[\"market_id\"] == \"redispatch\"]\n",
    "redispatch_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a9b0e",
   "metadata": {
    "id": "cb6a9b0e"
   },
   "outputs": [],
   "source": [
    "# Redispatch cost is equal to the accepted_volume * accepted_price for a particular snapshot\n",
    "# Compute cost using absolute accepted_volume\n",
    "redispatch_orders.loc[:, \"redispatch_cost\"] = (\n",
    "    redispatch_orders[\"accepted_volume\"].abs() * redispatch_orders[\"accepted_price\"]\n",
    ")\n",
    "\n",
    "# Then group & sum as before\n",
    "redispatch_costs = (\n",
    "    redispatch_orders.groupby(\"start_time\")[\"redispatch_cost\"].sum().reset_index()\n",
    ")\n",
    "redispatch_costs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8eafc",
   "metadata": {
    "id": "17f8eafc"
   },
   "source": [
    "##### B) Plot a bar graph for redispatch by powerplants for one snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929ac66",
   "metadata": {
    "id": "1929ac66"
   },
   "outputs": [],
   "source": [
    "# Select the first available snapshot\n",
    "snapshot = market_orders[\"start_time\"].unique()[0]\n",
    "filtered = market_orders[market_orders[\"start_time\"] == snapshot]\n",
    "\n",
    "# Pivot accepted_volume by unit_id and market_id\n",
    "pivot = filtered.pivot_table(\n",
    "    index=\"unit_id\", columns=\"market_id\", values=\"accepted_volume\", aggfunc=\"sum\"\n",
    ").fillna(0)\n",
    "\n",
    "# Plot stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "pivot.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "\n",
    "# Zero line, labels and title\n",
    "ax.axhline(0, color=\"black\")\n",
    "ax.set_xlabel(\"\")  # remove x-axis label text if desired\n",
    "ax.set_ylabel(\"Accepted Volume\")\n",
    "ax.set_title(f\"Final dispatches for {snapshot}\")\n",
    "\n",
    "# turn off the x-tick labels\n",
    "ax.set_xticklabels([])  # remove the unit names\n",
    "ax.set_ylim(-500, 1000)\n",
    "\n",
    "# (re)enable legend if you want it\n",
    "ax.legend(title=\"Market ID\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73st3XT64tWn",
   "metadata": {
    "id": "73st3XT64tWn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "assume-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
